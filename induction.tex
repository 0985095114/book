\newcommand{\zero}{\ensuremath{\mathbf{0}}\xspace}
\newcommand{\one}{\ensuremath{\mathbf{1}}\xspace}
\newcommand{\two}{\ensuremath{\mathbf{2}}\xspace}
\newcommand{\three}{\ensuremath{\mathbf{3}}\xspace}
\newcommand{\nat}{\ensuremath{\mathbf{N}}\xspace}
\newcommand{\lst}{\ensuremath{\mathbf{List}}\xspace}
\newcommand{\true}{\ensuremath{\mathbf{true}}\xspace}
\newcommand{\false}{\ensuremath{\mathbf{false}}\xspace}
\newcommand{\rec}{\ensuremath{\mathbf{rec}}\xspace}
\newcommand{\supp}{\ensuremath{\mathbf{sup}}\xspace}
\newcommand{\z}{\ensuremath{0}\xspace}
\newcommand{\wtype}[1]{\ensuremath{W}(#1),\xspace}
\newcommand{\ind}{\ensuremath{\mathbf{Ind \;}}\xspace}
%\newcommand{\inl}{\ensuremath{\mathbf{inl}}\xspace}
%\newcommand{\inr}{\ensuremath{\mathbf{inr}}\xspace}
\newcommand{\s}{\ensuremath{\mathbf{s}}\xspace}
\newcommand{\alt}{\;|\;\;}
\newcommand{\disj}[2]{#1 + #2}
\newcommand{\der}{\vdash}
\newcommand{\dbl}{\ensuremath{\mathbf{double}}}

\chapter{Induction}
\label{cha:induction}

\section{Booleans and natural numbers}\footnote{This should probably go in an earlier chapter.}

% Local Variables:
% TeX-master: "main"
% End:

An \emph{inductive type} can be intuitively understood as a type generated by a certain finite collection of constructors. To specify an inductive type formally,
we will use a schematic definition, where we list the name and type of each constructor separately, e.g.:
\[ \two : \type \defeq \ind \left\{ 
\begin{array}{ll}
\true : &\two \\
\false : &\two
\end{array}
\right. \]     
The above definition declares the type $\two$ of Booleans to be the inductive type generated by two constant constructors $\true$ and $\false$. We can similarly define the types $\zero$ (aka Empty, Void), $\one$ (aka Unit), $\three$, and so on, with 0, 1, 3, or more constructors respectively. 

Another extremely important inductive type is the type $\nat$ of natural numbers:  
\[  \nat : \type \defeq \ind \left\{ 
\begin{array}{ll}
\z : & \nat \\
\s : & \nat \to \nat
\end{array}
\right. \]
As expected, $\nat$ is generated by a constant constructor $\z$ for the natural number zero and a unary constructor $\s$ taking a natural number $n : \nat$ to its successor $\s(n) : \nat$. 

What can we do with such an inductive type? An intuitive understanding of an inductive type $A$ is that it \emph{behaves as if the only inhabitants of $A$ were the terms constructed solely by applying the constructors of $A$}; we refer to these particular terms as the \emph{canonical terms of $A$}. In an empty context,
each term of $A$ is canonical - for instance, any closed term of $\two$ is necessarily either $\true$ or $\false$. In the setting of nonempty contexts,  
interesting and often nontrivial behavior may occur, such as the one exhibited by identity types which behave much like paths in a space.

In the case of the type $\two$ of Booleans, assuming that each term is canonical simply means that each term $b : \two$ must be either $\true$ or $\false$. In particular, we have the principle of \emph{(dependent) elimination}:

\begin{itemize}
\item When proving a statement $E : \two \to \type$ about \emph{all} Booleans, it suffices to prove it for $\true$ and $\false$, i.e., give proofs
$ e_t : E(\true)$ and $e_f : E(\false)$.
\end{itemize}

Furthermore, the resulting proof $\rec_\two(E;e_t;e_f): \prd{b : \two}E(b)$ behaves as expected when applied to the constructors $\true$ and $\false$; this principle is expressed by the \emph{computation rules}:
\begin{itemize}
\item We have $\rec_\two(E;e_t;e_f;\true) \equiv e_t$.
\item We have $\rec_\two(E;e_t;e_f;\false) \equiv e_f$.
\end{itemize}

The rules for the type $\two$ of Booleans allow us to reason by \emph{case analysis}. Since neither of the two constructors takes any arguments, this is all we need for Booleans. However, for more complex types such as the type $\nat$ of natural numbers, true induction is often needed (hence the name \emph{inductive type}):

\begin{itemize}
\item When proving a statement $E : \nat \to \type$ about \emph{all} natural numbers, it suffices to prove it for $\z$ and for $\s(n)$, assuming it holds
for $n$. This entails giving the proofs $e_z : E(\z)$ and $e_s : \prd{n : \nat}{y : E(n)} E(\s(n))$.
\end{itemize}
The variable $y$ represents our inductive hypothesis. As for Booleans, we also have the associated computation rules for the function $\rec_\nat(E;e_z;e_s) : \prd{x:\nat} E(x)$:
\begin{itemize}
\item We have $\rec_\nat(E;e_z;e_s;\z) \equiv e_z$.
\item For any $n : \nat$, we have $\rec_\nat(E;e_z;e_s;\s(n)) \equiv e_s(n;\rec_\nat(E;e_z;e_s;n))$.
\end{itemize}
The dependent function $\rec_\nat(E;e_z;e_s)$ can thus be understood as being defined recursively on the argument $x : \nat$, via the recurrences $e_z$ and $e_s$: When $x$ is zero, the function simply returns $e_z$. When $x$ is the successor of another natural number $n$, the result is obtained by taking the recurrence $e_s$ and plugging in the specific predecessor $n$ and the recursive call value $\rec_\nat(E;e_z;e_s;n)$.

As an example we look at how to define a function on natural numbers which doubles its argument. We wish to apply dependent elimination with the constant type family $E \defeq \lambda(x : \nat), \nat$ since the intended type of $\dbl$ is $\nat \to \nat$. We first need to supply the value of $\dbl(\z)$, which is easy: we put $e_z \defeq \z$. Next, to compute the value of $\dbl(\s(n))$ for a natural number $n$, we first compute the value of $\dbl(n)$ and then perform the successor operation twice. This is captured by the recurrence $e_s(n,y) \defeq \s(\s(y))$; the variable $y$ stands for the result of the recursive call $\dbl(n)$. Thus, we define
\[ \dbl \defeq \rec_\nat(\lambda(x :\nat), \nat; \; \z; \;  \lambda(n: \nat) \lambda (y:\nat), \s(\s(y))) \]
This indeed has the correct computational behavior: for example, we have 
\begin{align*}
\dbl(\s(\s(\z))) & \equiv e_s(\s(\z), \dbl(\s(\z))) \\
                 & \equiv \s(\s(\dbl(\s(\z)))) \\
                 & \equiv \s(\s(e_s(\z,\dbl(\z)))) \\
                 & \equiv \s(\s(\s(\s(\dbl(\z))))) \\
                 & \equiv \s(\s(\s(\s(e_z)))) \\
                 & \equiv \s(\s(\s(\s(\z))))
\end{align*}
as desired.

There are many similar cases when one wants to use dependent elimination with a constant family term. This is referred to as \emph{simple elimination}, which corresponds to recursion - instead of proving a separate statement $P(n)$ for each natural number $n$ as done in the dependent case, we are recursively constructing a function into a simple type $C$. In this case, we omit the $\lambda$-binder for simplicity and write $\rec_\nat(C; \ldots)$ instead of $\rec_\nat(\lambda (n : \nat), C; \ldots)$

The depndent elimination principle is quite strong and allows us to prove a variety of interesting theorems. For example, by specifying the terms $e_z$ and $e_s$, we  uniquely determine how the recursor behaves on canonical terms, and thus on all natural numbers. If we now have another function which obeys the same recurrence, then our intuition suggests these two functions should be equal. It turns out this is indeed the case and we have the following \emph{uniqueness principle}:

\begin{thm}
Let $f,g : \prd{x:\nat} E(x)$ be two functions which satisfy the recurrences $e_z : E(\z)$ and $e_s : \prd{n : \nat}{y : E(n)} E(\s(n))$ up to propositional equality, i.e., such that
\begin{align*}
\id{f(\z)}{e_z} \\ 
\id{g(\z)}{e_z}
\end{align*}
and 
\begin{align*}
\prd{n : \nat} \id{f(\s(n))}{e_s(n, f(n))} \\
\prd{n : \nat} \id{g(\s(n))}{e_s(n, g(n))}
\end{align*}
Then $f$ and $g$ are equal, i.e., we have $\alpha : \id[\prd{x :\nat} E(x)]{f}{g}$. 
\end{thm}

\begin{proof}
We use dependent elimination with the type $E(x) \defeq \id{f(x)}{g(x)}$. For the base case, we have \[f(\z) = e_z = g(\z)\]
For the inductive case, assume $n : \nat$ such that $f(n) = g(n)$. Then
\[ f(\s(n)) = e_s(n, f(n)) = e_s(n, g(n)) = g(\s(n)) \]
The first and last equality follow from the assumptions on $f$ and $g$. The middle equality follows from the inductive hypothesis and the fact that application preserves equality. This gives us pointwise equality between $f$ and $g$; invoking function extensionality finishes the proof.
\end{proof}
We note that the function $f$ is only required to satisfy the recurrences \emph{up to propositional equality}. The theorem itself only asserts propositional equality between functions - indeed, it is possible to construct functions which satisfy the same recurrence but are not definitionally equal (exercise). It is also possible to have a function which satisfies more than one set of recurrences (exercise).

Similar uniqueness theorems can generally be formulated and shown for other inductive types as well. Such uniqueness results are a very useful tool and many important results about inductive types make use of them. For instance, we can show that natural numbers are completely characterized by their introduction, elimination, and computation rules:

\begin{thm}\label{thm:uniq_types}
Let $C$ be any other type satisfying the introduction, elimination, and computation rules for natural numbers, i.e., coming with terms 
\begin{align*}
c_z & : C \\
c_s & : C \to C
\end{align*}
such that for any $E : C \to \type$, $e_z : E(c_z)$, and $e_s : \prd{c : C}{y : E(c)} E(c_s(c))$ the following hold:
\begin{itemize}
\item We have a function $\rec_C(E;e_z;e_s) : \prd{c:C} E(c)$.
\item We have $\rec_C(E;e_z;e_s,c_z) \equiv e_z$.
\item For any $c : C$, we have $\rec_C(E;e_z;e_s,c_s(e)) \equiv e_s(c,\rec_C(E;e_z;e_s;c))$.
\end{itemize}
Then $C$ is equal to $\nat$, i.e., we have $\id{C}{\nat}$.
\end{thm}
\begin{proof}
TODO
\end{proof}
Analogous theorems can likewise be shown for other inductive types.

We note that the proof of \ref{thm:uniq_types} uses univalence in a crucial way in order to go from equivalence to true equality. Using identity elimination on the equality $\id{C}{\nat}$ then shows that for any property $P : \bbU \to \type$ of types, $P(\nat)$ is inhabited iff $P(C)$ is. In other words, whatever can be shown for $\nat$ can be also shown for $C$ and vice versa. This is just one of the many results illustrating the power of the univalence axiom.

We conclude this section with a categorical observation. Using the uniqueness principle, we can show that inductive types give rise to \emph{homotopy-initial algebras}, in the following sense. Taking Booleans as the simplest example, we define:

\begin{defn}
A \emph{$\two$-algebra} is a type $C$ with two terms $c_0, c_1 : C$. Thus,
\begin{align*}
\mathtt{2Alg} \defeq \sm {C : \type} C \times C
\end{align*}
\end{defn}

\begin{defn}
Fix any $\two$-algebras $(C,c_0,c_1)$ and $(D,d_0,d_1)$. A \emph{$\two$-homo-morphism} between them is a function $h : C \to D$ mapping $c_0$ to $d_0$ and
$c_1$ to $d_1$ (up to propositional equality). Thus,
\begin{align*}
\mathtt{2Hom \; (C,c_0,c_1) \; (D,d_0,d_1)} \defeq \sm {h : C \to D} \; \id{h(c_0)}{d_0} \times \id{h(c_1)}{d_1}
\end{align*}
\end{defn}

\begin{defn}
A $\two$-algebra $\chi^*$ is called \emph{homotopy-initial} if for any other $\two$-algebra $\chi$, the type of $\two$-homomorphisms from $\chi^*$ to $\chi$ is contractible. Thus,
\begin{align*}
\mathtt{is\_hinitial \; \chi^* \defeq \fall{\chi : 2Alg} \; is\_contr \; (2Hom \; \chi^*\; \chi)}
\end{align*}
\end{defn}

As expected, homotopy-initial algebras are unique:
\begin{thm}
Any two homotopy-initial $\two$-algebras $\chi^*_1$ and $\chi^*_2$ are equal, i.e., we have $\id[\mathtt{2Alg}]{\chi_1^*}{\chi_2^*}$.
\end{thm}


We now have the following theorem; the proof is only sketched as it involves a large amount of technical detail.
\begin{thm}
The $\two$-algebra $(\two, \true, \false)$ is homotopy initial.
\end{thm}
\begin{proof}
(Rough sketch) Fix an arbitrary $\two$-algebra $(C,c_0,c_1)$. Using elimination on $\two$ it is easy to construct a $\two$-homomorphism into $C$. This will be our
center of contraction. To show that any other homomorphism is equal to the chosen one, we appeal to the uniqueness theorem for $\two$.
\end{proof}

\section{Disjoint unions and dependent sums}
A \emph{disjoint union} of two types $A$ and $B$ behaves much like a conjunction: we can choose to supply either a term of type $A$ or a term of type $B$. Schematically, we declare this as follows:

\begin{align*}
  \disj{A}{B} \defeq \; & \inl : A \to (\disj{A}{B}) \\
         \alt & \inr : B \to (\disj{A}{B})
\end{align*}

In this definition, $A$ and $B$ act as \emph{parameters}, i.e., they denote arbitrary types. In other words, the type constructor $+$ is really a function taking the types $A$ and $B$ as an argument and returning a new inductive type $\disj{A}{B}$. Although omitted from the above schema for clarity, the constructors $\inl$ and $\inr$ likewise take the types $A$ and $B$ as the first 2 arguments, followed by a term of type $A$ (for \inl) or a term of type $B$ (for \inr).

Elimination for a disjoint union amounts to case analysis:
\begin{itemize}
\item When proving a statement $E : (\disj{A}{B}) \to \type$ about \emph{all} terms of the disjoint union $\disj{A}{B}$, it suffices to prove it for $\inl(a)$ and $\inr(b)$, i.e., give proofs $e_l : \prd{a:A} E(\inl(a))$ and $e_r : \prd{b:B} E(\inr(b))$.
\end{itemize}
The associated computation rules for the function $\rec_{\disj{A}{B}}(E;e_l;e_r) : \prd{x:\disj{A}{B}} E(x)$ are as expected:
\begin{itemize}
\item For each $a : A$, the proof $\rec_{\disj{A}{B}}(E;e_l;e_r;\inl(a)) : E(\inl(a))$ is identical to $e_l(a)$.
\item For each $b : B$, the proof $\rec_{\disj{A}{B}}(E;e_l;e_r;\inr(b)) : E(\inr(b))$ is identical to $e_r(b)$.
\end{itemize}

A \emph{product} of two types $A$ and $B$ (denoted by $A \times B$) is the type of pairs $(a,b)$, where $a : A$ and $b : B$. A \emph{dependent sum} is a generalization of this concept, where we allow the type $B$ to depend on $A$. A simple example is the type of pairs $(n,(c_1,\ldots,c_n))$, where the first component is a natural number $n : \nat$ and the second component is a vector $(c_1,\ldots,c_n) : \mathbf{Vec}_C(n)$ of length $n$ over another type $C : \type$.
Given $A : \type$ and $B : A \to \type$, the dependent sum of $A$ and $B$ is given schematically as
\begin{align*}
  \sm{a:A} B(a) \defeq \; & \mathbf{pair} : \prd{a:A}{b:B(a)} \sm{a:A} B(a)
\end{align*}
Thus, the parameterized inductive type $\sm{a:A} B(a)$ has a single constructor $\mathbf{pair}$. Its first two arguments (not shown) are the parameters $A$ and $B$; the remaining two arguments are the respective components $a : A$ and $b : B(a)$. We will often denote $\mathbf{pair}(a,b)$ simply by $(a,b)$.

The elimination and computation rules are very simple:

\begin{itemize}
\item When proving a statement $E : \big(\sm{a:A} B(a)\big) \to \type$ about \emph{all} terms of the dependent sum $\sm{a:A} B(a)$, it suffices to prove it for a pair $(a,b)$, i.e., give a proof $e : \prd{a:A}{b:B(a)} E((a,b))$.
\end{itemize}

\begin{itemize}
\item For any terms $a : A$ and $b : B(a)$, the proof $\rec_{\sm{a:A} B(a)}(E;e;(a,b)) : E((a,b))$ is identical to $e(a,b)$.
\end{itemize}
Using the elimination operator, it is very easy to construct the well-know projection functions $\pi_1$ and $\pi_2$, extracting the first resp. the second component of a pair.

\section{W-types}
Martin-L{\"o}f's W-types, also known as the types of well-founded trees, are a generalization of such types as natural numbers, lists, and binary trees. A particular W-type is specified by giving two parameters $A : \type$ and $B : A \to \type$, written $\wtype{a:A} B(a)$.

The type $A$ represents the type of \emph{labels} for $\wtype{a :A} B(a)$, which function as constructors (however, we reserve that word for use in schematic definitions of inductive types). For instance, when defining natural numbers as a W-type the type $A$ would be the type $\two$ inhabited by the two terms $\true$ and $\false$, since there are precisely two ways how to obtain a natural number - either it will be zero or a successor of another natural number. 

The dependent type family $B : A \to \type$ is used to record the arity of labels: a label $a : A$ will take $B(a)$-many inductive arguments. These arguments are represented as a function $f : B(a) \to \wtype{a :A} B(a)$, with the understanding that for any $b : B(a)$, $f(b)$ is the $b$-th argument to the label $a$. The W-type $\wtype{a :A} B(a)$ can thus be thought of as the type of well-founded trees, where nodes are labeled by terms of $A$ and each node labeled by $a : A$ has $B(a)$ branches.

In the case of natural numbers, the label $\true $ has arity 0, since it constructs the constant zero; the label $\false$ has arity 1, since it constructs the successor of its argument. We can capture this by using simple elimination on $\two$ to define a function $\rec_\two(\bbU;\zero;\one)$ into a universe of types; this function returns the empty type $\zero$ for $\true$ and the unit type $\one$ for $\false$. We can thus define
\[ \nat \defeq \wtype{b:\two} \rec_\two(\bbU;\zero;\one) \]
Similarly, we can define the type of lists over $A$ as a W-type with $\disj{\one}{A}$ many labels: one nullary label for the empty list, plus one unary label for each $a : A$, corresponding to appending the term $a$ to the head of a list:

\[ \lst \; A \defeq \wtype{x:\disj{\one}{A}} \rec_{\disj{\one}{A}}(\bbU; \; \zero; \; \lambda{(a:A)}, \one) \]

Schematically, we define a W-type as follows:
\begin{align*}
  \wtype{x:A} B(x) \defeq \; \supp : & \; \prd{a:A}{f : B(a) \to \wtype{a:A} B(a)} \\ & \; \wtype{x:A} B(x)         
\end{align*}
The constructor $\supp$ (short for supremum) takes a label $a : A$ and a function $f : B(a) \to \wtype{a:A} B(a)$ representing the arguments to $a$, and constructs a new term of $\wtype{x:A} B(x)$. Using our previous encoding of natural numbers as W-types, we can for instance define
\begin{align*}
0 \defeq \supp(\true, \; \lambda(x : \zero), \rec_\zero(\nat;x))
\end{align*}
In other words, to construct $\z$ we use the label $\true$. Now $\rec_\two(\bbU;\zero;\one; \true)$ evaluates to $\zero$, as it should since $\true$ is a nullary label. Thus, we need to construct a function $f : \zero \to \nat$, which represents the (zero) arguments supplied to $\true$. This is of course trivial, using simple elimination on $\zero$. Similarly, we can define
\begin{align*}
1 \defeq \supp(\false, \; \lambda(x : \one), 0) \\
2 \defeq \supp(\false, \; \lambda(x : \one), 1)
\end{align*}
and so on.

%\subsection{Elimination Rule}
%We have the following elimination rule for W types, referred to as dependent elimination:
%\begin{mathpar}
%\inferrule{\entails{\Gamma,\term{w}{\wtype{x}{A}{B}}}{\type{C}} \\
%           \entails{\Gamma,\term{x}{A},\term{f}{B \to \wtype{x}{A}{B}},\term{g}{\pitype{b}{B}{\subst{C}{w}{(\app{f}{b})}}}}{\term{H}{\subst{C}{w}{\supp{x}{f}}}}} 
%          {\entails{\Gamma,\term{w}{\wtype{x}{A}{B}}}{\term{\wrec{C}{H}{w}}{C}}}(\WDE)
%\end{mathpar}
%The above rule is the infinitary version of the elimination rule for inductively defined datatypes. If we view the dependent type $C$ as a predicate on $\wtype{x}{A}{B}$, the rule says that in order to show that $C$ holds for all  terms of $\wtype{x}{A}{B}$, it suffices to show that $C$ holds for all terms of $\wtype{x}{A}{B}$ having the form $\supp{x}{f}$ for some $x$ and $f$, thus capturing the notion that $\wtype{x}{A}{B}$ contains no other terms besides those created though one of its constructors. Moreover, when showing that $C$ holds for a term of the form $\supp{x}{f}$, we are allowed to use the induction hypothesis that $C$ holds for all arguments of the constructor $x$, capturing the notion of well-foundedness. In other words, $\wtype{x}{A}{B}$ can be seen as the type of trees of finite depth whose nodes are labeled by terms of $A$ and each node labeled by $a$ has precisely $|\subst{B}{x}{a}|$ children. The elimination rule then allows us to reason by induction on the depth of such a tree. In the case of natural numbers this reduces to ordinary induction.
%
%As an example we consider the function \emph{double} on natural numbers. To define the function recursively we must construct a suitable term 
%\[\entails{\term{x}{\two},\term{f}{B \to \nat},\term{g}{B \to \nat}}{\term{H}{\nat}}\]
%where $B$ denotes the type $\ifte{x}{\zero}{\one}$. The variable $x$ represents an arbitrary constructor, $f$ represents the predecessor (if applicable), and $g$ represents the double of the predecessor. The term $H$ itself then stands for the double of the natural number represented by $x$ and $f$. To construct $H$, we
%first define an auxiliary term $V$ as the case analysis
%\begin{align*}
%& \ifte{x \\ & \; }{\lam{f'}{\zero \to \nat}{\lam{g'}{\zero \to \nat}{0}} \\ & \;}
%  {\lam{f'}{\one \to \nat}{\lam{g'}{\one \to \nat}{\supp{\false}{\lam{-}{\one}{\supp{\false}{\lam{-}{\one}{(\app{g'}{\langle \rangle})}}}}}}}
%\end{align*}
%We then put
%\begin{align*}
%H & \stackrel{\text{def}}{=} V f \; g \\
%\textit{double} & \stackrel{\text{def}}{=} \lam{n}{\nat}{\wrec{\nat}{H}{n}}
%\end{align*}
%The variables $f'$ and $g'$ in the case analysis can be thought of as representing the functions $f$ and $g$ after the type refinement has been performed, i.e. after replacing the type $B$ by either $\zero$ or $\one$, depending on the value of $x$. If $x$ evaluates to $\true$, we are in the branch for $n = 0$ and we return 0. If $x$ evaluates to $\false$, we are in the branch for $n = s(n')$; the expression $g' \; \langle \rangle$ then gives us the result of the recursive call on the predecessor $n'$. The constructor $\false$ is then applied twice, corresponding to a double application of the successor function. This gives us the double of $n$.
% 
%\subsection{Computation Rule}
%We have the following computation rule for W types:
%\begin{align*}
%\inferrule{\entails{\Gamma, \term{w}{\wtype{x}{A}{B}}}{\type{C}} \\
%           \entails{\Gamma,\term{x}{A},\term{f}{B \to \wtype{x}{A}{B}},\term{g}{\pitype{b}{B}{\subst{C}{w}{(\app{f}{b})}}}}
%                    {\term{H}{\subst{C}{w}{\supp{x}{f}}}} \\%\\\\
%          u(w) \stackrel{\text{def}}{=} \wrec{C}{H}{w}}
%          {\entails{\Gamma, \term{x}{A}, \term{f}{B \to \wtype{x}{A}{B}}}
%                   {\term{u(\supp{x}{f}) = \subst{H}{g}{\big(\lam{b}{B}{u(\app{f}{b})}\big)}}{\subst{C}{w}{\supp{x}{f}}}}}(\WDB)      
%\end{align*}
%The rule is a form of $\beta$-reduction and states that the recursor obtained from the elimination rule behaves as specified by $H$. By substitution, we obtain the equality 
%\[\wrec{C}{H}{\supp{a}{h}} = H[a/x, h/f, \big(\lam{b}{\subst{B}{x}{a}}{\app{h}{b}}\big) / g]\]
%for any two terms $a$ and $h$ of suitable types. In the case of the function \textit{double} defined in the previous subsection, it is easy to see that by the computation rule we have $\textit{double} \; 0 = 0$ and $\textit{double} \; 1 = 2$, as expected.
%
%\section{Identity types}