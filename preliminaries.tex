\chapter{Type theory}
\label{cha:typetheory}

\section{Type theory versus set theory}
\label{sec:types-vs-sets}

Type theory in general, and homotopy type theory in particular, is (among other things) a foundational language for mathematics, i.e., an alternative to Zermelo-Fraenkel set theory.
However, it behaves differently from the latter in several important ways, which can take some getting used to.
Explaining these differences carefully requires us to be a little bit more formal here than we will be in the rest of the book.
As stated in the introduction, our goal is to write type theory \emph{informally}; but for a mathematician accustomed to set theory, a little bit more precision at the beginning can help avoid some common misconceptions and mistakes.

First of all, we note that a set-theoretic foundation has two ``layers'': first there is the deductive system of first-order logic, and then formulated inside this system there are the axioms of a particular theory (such as ZFC).
Thus, set theory is not only about sets, but rather about the interplay between sets (the objects of the second layer) and propositions (the objects of the first layer).

By contrast, type theory is its own deductive system: it need not be formulated inside any superstructure such as first-order logic.
In particular, rather than having two basic notions (sets and propositions), type theory has one basic notion, namely \emph{types}.
Propositions (statements which we can prove, disprove, assume, negate, and so on\footnote{Confusingly, it is also a common practice (dating back to Euclid) to use the word ``proposition'' synonomously with ``theorem''.
  We will confine ourselves to the logician's usage, according to which a \emph{proposition} is a statement \emph{susceptible to} proof, whereas a \emph{theorem} (or ``lemma'' or ``corollary'') is such a statement that \emph{has been} proven.
Thus ``$0=1$'' and its negation ``$\neg(0=1)$'' are both propositions, but only the latter is a theorem.}) are identified with particular types, via the correspondence shown in Table~\ref{tab:pov} on page~\pageref{tab:pov}.
Thus, the mathematical activity of \emph{proving a theorem} is identified with a special case of the mathematical activity of \emph{constructing an object}---in this case, an inhabitant of a type that represents a proposition.

This leads us to another difference between type theory and set theory, but to explain it we must say a little about deductive systems in general.
Informally, a deductive system is a collection of rules for deriving things called \emph{judgments}.
If we think of a deductive system as a formal game, then the judgments are the ``positions'' in the game which we reach by following the game rules.
We can also think of a deductive system as a sort of algebraic theory, in which case the judgments are the elements (like the elements of a group) and the deductive rules are the operations (like the group multiplication).
From a logical point of view, the judgments can be considered to be the ``external'' statements, living in the metatheory, as opposed to the ``internal'' statements of the theory itself.

In the deductive system of first-order logic (on which set theory is based), there is only one judgment: that a given proposition has a proof.
A rule of first-order logic such as ``from $A$ and $B$ infer $A\wedge B$'' is actually a rule of ``proof construction'' which says that given the judgments ``$A$ has a proof'' and ``$B$ has a proof'', we may deduce that ``$A\wedge B$ has a proof''.
Note that the judgment ``$A$ has a proof'' exists at a different level from the \emph{proposition} $A$ itself, which is an internal statement of the theory.
% In particular, we cannot manipulate it to construct propositions such as ``if $A$ has a proof, then $B$ does not have a proof''---unless we are using our set-theoretic foundation as a meta-theory with which to talk about some other axiomatic system.

The basic judgment of type theory, analogous to ``$A$ has a proof'', is written ``$a:A$'' and pronounced as ``the term $a$ has type $A$'', or more loosely ``$a$ is an element of $A$'' (or, in homotopy type theory, a ``point'' of $A$).
When $A$ is a type representing a proposition, then $a$ may be called a \emph{witness} to the provability of $A$, or \emph{evidence} of the truth of $A$.
In this case, the judgment $a:A$ is derivable in type theory (for some $a$) precisely when the analogous judgment ``$A$ has a proof'' is derivable in first-order logic (modulo differences in the axioms assumed and in the encoding of mathematics, as we will discuss throughout the book).
% (Already here type theory has an advantage in that the term $a$ records the essence of the proof of $A$, whereas in first-order logic that information is only available by backtracking through the derivation of ``$A$ has a proof''.)

On the other hand, if the type $A$ is being treated more like a set than like a proposition (although as we will see, the distinction can become blurry), then ``$a:A$'' may be regarded as analogous to the set-theoretic statement ``$a\in A$''.
However, there is an essential difference in that ``$a:A$'' is a \emph{judgment} whereas ``$a\in A$'' is a \emph{proposition}.
In particular, when working internally in type theory, we cannot make statements such as ``if $a:A$ then it is not the case that $b:B$'', nor can we ``disprove'' the judgment ``$a:A$''.

A good way to think about this is that in set theory, ``membership'' is a relation which may or may not hold between two pre-existing objects ``$a$'' and ``$A$'', while in type theory we cannot talk about an element ``$a$'' in isolation: every element \emph{by its very nature} is an element of some type, and that type is (generally speaking) uniquely determined.
Thus, when we say informally ``let $x$ be a natural number'', in set theory this is shorthand for ``let $x$ be a thing and assume that $x\in\nat$'', whereas in type theory ``let $x:\nat$'' is an atomic statement: we cannot introduce a variable without specifying its type.

At first glance, this may seem an uncomfortable restriction, but it is arguably closer to the intuitive mathematical meaning of ``let $x$ be a natural number''.
In practice, it seems that whenever we actually \emph{need} ``$a\in A$'' to be a proposition rather than a judgment, there is always an ambient set $B$ of which $a$ is known to be an element and $A$ is known to be a subset.
This situation is also easy to represent in type theory, by taking $a$ to be an element of the type $B$, and $A$ to be a predicate on $B$; see \autoref{subsec:prop-subsets}.

A last difference between type theory and set theory is the treatment of equality.
The familiar notion of equality in mathematics is a proposition: e.g.\ we can disprove an equality or assume an equality as a hypothesis.
Since in type theory, propositions are types, this means that equality is a type: for elements $a,b:A$ (that is, both $a:A$ and $b:A$) we have a type ``$\id[A]ab$''.
(In \emph{homotopy} type theory, of course, this equality proposition can behave in unfamiliar ways: see \autoref{sec:identity-types}, \autoref{cha:basics}, and the rest of the book).

However, in type theory there is also a need for an equality \emph{judgment}, existing at the same level as the judgment ``$x:A$''.
This is called \emph{judgmental equality} or \emph{definitional equality}, and we write it as $a\jdeq b$ or $a \jdeq_A b$.
It is helpful to think of this as meaning ``equal by definition''.
For instance, if we define a function $f:\nat\to\nat$ by the equation $f(x)=x^2$, then the expression $f(3)$ is equal to $3^2$ \emph{by definition}.
It does not make sense to negate or assume an equality-by-definition; we cannot say ``if $x$ is equal to $y$ by definition, then $z$ is not equal to $w$ by definition''.
Whether or not two expressions are equal by definition is just a matter of expanding out the definitions; in particular, it is algorithmically decidable.

As type theory becomes more complicated, judgmental equality can get more subtle than this, but it is a good intuition to start from.
Alternatively, if we regard a deductive system as an algebraic theory, then judgmental equality is simply the equality in that theory, analogous to the equality between elements of a group---the only potential for confusion is that there is \emph{also} an object \emph{inside} the deductive system of type theory (namely the type ``$a=b$'') which behaves internally as a notion of ``equality''.

The reason we \emph{want} a judgmental notion of equality is so that it can control the other form of judgment, ``$a:A$''.
For instance, suppose we have given a proof that $3^2=9$, i.e.\ we have derived the judgment $p:(3^2=9)$ for some term $p$.
Then the same proof $p$ ought to count as a proof that $f(3)=9$, since $f(3)$ is $3^2$ \emph{by definition}.
The best way to represent this is with a rule saying that given the judgments $a:A$ and $A\jdeq B$, we can derive the judgment $a:B$.

Thus, for us, type theory will be a deductive system based on two forms of judgment:
\begin{center}
\begin{tabular}{c|l}
  \textbf{Judgment} & \textbf{Meaning}\\\hline
  $a : A$ & $a$ is an object of type $A$.\\
  $a \jdeq_A b$ & $a$ and $b$ are definitionally equal objects of type $A$.
\end{tabular}
\end{center}
When introducing a definitional equality, i.e., defining one thing to equal another, we will use the symbol ``$\defeq$''.
Thus, the above definition of the function $f$ would be written as $f(x)\defeq x^2$.

Because judgments cannot be put together into more complicated statements, the symbols ``$:$'' and ``$\jdeq$'' bind more loosely than anything else.\footnote{In formalized type theory, commas and turnstiles can bind even more loosely.
  For instance, $x:A,y:B\vdash c:C$ is parsed as $((x:A),(y:B))\vdash (c:C)$.
  However, in this book we will not use any notation of this sort.}
Thus, for instance, ``$p:x=y$'' should be parsed as ``$p:(x=y)$'', which makes sense since ``$x=y$'' is a type, and not as ``$(p:x)=y$'', which is senseless since ``$p:x$'' is a judgment and cannot be equal to anything.
Similarly, ``$A\jdeq x=y$'' can only be parsed as ``$A\jdeq(x=y)$'', although in extreme cases such as this, one ought to add parentheses anyway to aid reading comprehension.
This is perhaps also an appropriate place to mention that the common mathematical notation ``$f:A\to B$'', expressing the fact that $f$ is a function from $A$ to $B$, can be regarded as a typing judgment, if we use ``$A\to B$'' as notation for the type of functions from $A$ to $B$ (which is standard practice in type theory; see \autoref{sec:pi-types}).

Judgments may depend on assumptions of the form $x:A$, where $x$ is a variable and $A$ is a type.
For example, we may construct an object $m + n : \Nat$ under the assumptions that $m,n : \Nat$.
Another example is that assuming $A$ is a type, $x,y : A$, and $p : x =_A y$, we may construct an element $p^{-1} : y =_A x$.
The collection of all such assumptions is called the \emph{context}; from a topological point of view it may be thought of as a ``parameter space''.

If the type $A$ in an assumption $x:A$ represents a proposition, then the assumption is a type-theoretic version of a \emph{hypothesis}: we assume that the proposition $A$ holds.
When types are regarded as propositions, we may omit the names of their proofs.
Thus, in the second example above we may instead say that assuming $x =_A y$, we can prove $y =_A x$.
However, since we are doing proof-relevant mathematics, we will frequently refer back to proofs as objects.
In the example, we may want to establish that $p^{-1}$ together with the proofs of transitivity and reflexivity behave like a groupoid; see \autoref{cha:basics}.

In the rest of this chapter, we attempt to give an informal presentation of Type Theory, sufficient for the purposes of this book; we will give a more formal account in an Appendix.
Aside from some fairly obvious rules (such as the fact that judgmentally equal things can always be substituted for each other), the rules of type theory can be grouped into \emph{type formers}.
Each type former consists of a way to construct types (possibly making use of already-constructed types), together with rules for the construction and behavior of elements of that type.
In most cases, these rules follow a fairly predictable pattern, but we will not attempt to make this precise.


\section{Universes}
\label{sec:universes}

A universe is a type whose elements are types. As in naive set theory
we may wish for a universe of all types $\UU_\infty$ including itself
$\UU_\infty : \UU_\infty$ but as in set
theory this is unsound, i.e. we can deduce that every type,
including the empty type representing the proposition False, is inhabited. Using a
representation of sets as trees we can directly encode Russell's
paradox \cite{thierry:trees} or alternatively avoiding the use of
inductive types to define trees we can encode the Burali-Forti paradox
(that the collection of ordinals cannot be an ordinal) this is due to
Girard \cite{girard:paradox}.

To avoid these paradoxes we introduce a hierarchy of universes
\[ \UU_0 : \UU_1 : \UU_2 : \dots \]
where every universe $\UU_i$ is an element of the next universe
$\UU_{i+1}$. Moreover we assume that our universes are
\emph{cumulative}, that is that all the elements of the $i$th
universe are also elements of the $i+1$th universe, i.e. if
$A:\UU_i$ then $A:\UU_{i+1}$
\footnote{This rule breaks the principle that every element
  has a uniquely determined type. This could be avoided by introducing
  an explicit lifting operator.}

We say that our type theory is predicative if types in a universe $\UU_i$ are introduced only with reference to types in the same or lower universes. Most (all ?) constructions introduced in this book are predicative. An impredicative type is one which is obtained by quantifying over all types at a certain level including itself. In classical Mathematics we can quantify over all truth values which are identified with the type of booleans. Doing the same in type theory would involve quantifying over all types at the current level and is hence impredicative. As a consequence assuming the principle of excluded middle, which identifies the small type of booleans with the universe, leads to an impredicative theory.

When we say that $A$ is a type we mean that it inhabits some universe
$\UU_i$. We usually want to avoid to mention the level $i$ explicitely
and just assume that levels can be assigned in a consistent way. We
may write $A:\UU$ omitting the level. This way we can even write
$\UU:\UU$ which can be read as $\UU_i:\UU_{i+1}$ having left the
indizes implicit. 

\section{Function types}
\label{sec:function-types}

Given types $A,B :\UU$ we can construct the type $A \to B:\UU$ of functions with domain $A$
and codomain $B$. Unlike in set theory functions are not defined as
functional relations but functions in type theory are a primitive
concept. We explain the function type by prescribing what we can do with functions, 
how to construct them and what equalities they induce.

Given a function $f : A \to B$ and an element of the domain $a : A$ we
can \textbf{apply} the function to obtain an element of the codomain
$f(a) : B$. 

But how can we construct elements of $A \to B$? There are two ways:
either by direct definition or by using
$\lambda$-abstraction. Introducing a function by definition means that
we introduce a function by giving it a name, lets say $f$ and saying
we define $f : A \to B$ by giving an equation
\[ f(x) \defeq b \]
where $x$ is a variable and $b$ is an expression which may use $x$.
We have to check that $b : B$ assuming $x:A$.

Now we can compute $f(a)$ by replacing the variable $x$ in $b$ with
$a$. As an example consider the function $f : \Nat \to \Nat$ which is
defined by $f(x) = x+x$. Now $f(2)$ is judgmentally equal to $2+2$.

If we don't want to introduce a name for the function we can use
$\lambda$-abstraction. Given $b : B$ assuming $x:A$ as above we can
construct 
\[ (\lam{x:A}b) : A \to B \]
Applying a $\lambda$-abstraction to an argument $a:A$ introduces the
definitional equality 
\[(\lam{x:A}b)(a) \jdeq b'\]
 where $b'$ is the
expression $b$ in which all occurences of $x$ have been replaced by $a$.
This is called $\beta$-conversion, or $\beta$-reduction if we use this
equality in a computation by replacing the left hand side with the
right hand side. Reusing the same example we have that
\[ (\lam{x:\nat}x+x) : \nat \to \nat \]
and the judgmental equality
\[ (\lam{x:\nat}x+x)(2) \jdeq 2+2. \]

Note that we usually do not assume $\eta$-conversion which says that $\lam{x:A}f(x)$ is judgmentally equal to $f$, if $f$ does not contain occurences of $x$. However, we will be able to prove $\lam{x:A} f(x) = f$ for any $f:A \to B$ using the principle of function extensionality, which we are going to assume.

The introduction of functions by explicit definitions can be reduced
to ordinary definitions using $\lambda$-abstraction: we can read 
the definition of $f: A\to B$ by
\[ f(x) \defeq b \]
as 
\[ f \defeq \lam{x:A}b .\]

When doing calculations involving variables we have to be a bit
careful when replacing a variable with an expression involving
variables because we want to preserve the binding structure of
expressions we build. By the binding structure we mean the
invisible link generated by binders like $\lambda$, $\Pi$ and
$\Sigma$ (the latter we are going to meet soon) between the place wehere the variable is introduced and where it is used. As an example consider $f : \nat \to (\nat \to \nat)$
defined as 
\[ f(x) \defeq \lam{y:\nat} x + y \] 
Now if we have assumed somewhere that $y : \nat$ what is $f(y)$. It would be wrong to just naively replace $x$ by $y$ everywhere obtaining $\lam{y:\nat} y + y$ because this means that $y$ gets \textbf{captured}. Previously $y$ was referring to our assumption and now it is referring to the parameter of the anonymous function. Hence, this naive substitution would destroy the binding structure and would allow us to perform calculations which are semantically unsound.

But what is $f(y)$? Note that bound
variables like $y$ in the expression $\lam{y:\nat} x + y$
have only a local meaning and can be consistently replaced by any
other variable, preserving the binding structure. Indeed $\lam{y:\nat} x + y$ is judgmentally equal to
$\lam{z:\nat} x + z$ - this is called $\alpha$-conversion. Now
using $\alpha$-conversion we can give an answer to the question 
what is $f(y)$ namely  $\lam{z:\nat} y + z$. Clearly, this is not
the only option there are many other choices of names but this doesn't
matter because all of them are \textbf{$\alpha$-equivalent}.

It seems that we can only define functions in one variable. One
way to define functions in several parameters would be to use the
cartesian product which we will introduce later. E.g. a function with
parameters $A$ and $B$ and results in $C$ can be given the type 
$A \times B \to C$. However, there is another choice which avoids
using product types --- this is called \textbf{currying} in functional
programming. Using currying we use instead the type $A \to (B \to C)$
which can be written without the brackets $A \to B \to C$ because
by convention $\to$ associates to the right. Given $a : A$ and $b : B$
we can apply a function $f : A \to B \to C$ to obtain $f(a)(b) :
C$. To avoid the proliferation of brackets we allow ourselves to
write $f(a)(b)$ as $f(a,b)$ even though there are no products
involved. Our notation for explicit definitions naturally extends to
this situation we can define a named function $f : A \to B \to C$ by
giving an equation
\[ f(x,y) \defeq c\]
where $c:C$ assuming $x:A$ and $y:B$. Using $\lambda$-abstraction this
corresponds to
\[ f \defeq \lam{x:A}{y:B} c \] We use currying for functions of any number of arguments not just 2 the scheme for $n>2$ is a straightforward extension of what we have just described.
 
% In the sequel we shall write $a(x)$ for a term with a variable $x$ and
% $a(b)$ for the same term where each occurence of $x$ has been
% replaced by $b$. Moreover, we shall assume that this substitution
% operation avoids the capture of bound variables, if necessary by
% consistently renaming bound variables ($\alpha$-conversion).
% As an example consider $b(x) \defeq \prd{y:A}x = y$, we don't want
% $B(y)$ to be equal to $\prd{y:A}y = y$ because the variable $y$ we are
% substitute doesn't refer to the bound variable. Instead we rename the
% bound variable to avoid capture $B(y) \jdeq \prd{y':A}y = y'$.

% A \textbf{family of types} over an indexing type $A:\UU$ and a 
% type $B(x) : \UU$ given $x:A$. This can be generalized to a sequence of indexing
% types $A_i(x_0,\dots x_{i-1})$ each depending on the previous indexing
% types and a family $B(x_0,\dots,x_n) : \UU$ depending on all of them. In the
% propositions as types view families correspond to predicates:
% assigning to each element of the indexing types a type corresponding
% to the proposition that this particular instance of the predicate holds.

% An example is the type $\Vect(A,n)$ of
% vectors ($n$-tuples) of elements $a_i : A$ for $0 < i< n$ of type $A$:
% $\seq{a_0,\dots,a_n} : \Vect(A,n)$ which is family indexed
% over $A:\UU$ and $n : \Nat$. Another example is the type $x =_A y$ which is a
% family indexed over $A:\UU$ and $x,y : A$.

\section{Dependent function types (\texorpdfstring{$\Pi$}{Π}-types)}
\label{sec:pi-types}

In type theory we are often using a more general version of function
types called a $\Pi$-type or dependent function type. The elements of
a $\Pi$-type are functions whose codomain type can vary depending on the
element of the domain the function is applied to. The name $\Pi$-type
is used because this type corresponds to the infinite cartesian
product over a given type.

To model a type varying over a given type $A$ we use functions whose
domain is a universe $B : A \to \UU$. These functions are called
\textbf{families of types} and they correspond to families of sets as used in
set theory. An example of a family is the family of finite sets $\Fin
: \nat \to \UU$ where $\Fin(n)$ is a type with exactly $n$ elements
which we denote as $0_n,1_n,\dots,(n-1)_n$. 

Now given a type $A:\UU$ and a family $B:A \to \UU$ we construct
the type of dependent functions $\prd{x:A}B(x) : \UU$. If $B$ is
constant then the dependent product type is the ordinary function type:
\[(\prd{x:A} B) \jdeq (A \to B).\]
Indeed, all the constructions of $\Pi$-types are generalisations of the corresponding constructions on ordinary function types.

We can introduce dependent functions by explicit definitions: to
define $f : \prd{x:A}B(x)$ where $f$ is the name of a function to be
defined we need a $b : B(x)$ assuming $x:A$ and we write
\[ f(x : A) \defeq b \]
Alternatively, we can use $\lambda$-abstraction 
\[ \lam{x:A} b : \prd{x:A} B(x) \]
The equalities are the same as for the ordinary function type, i.e.
given $a:A$ we have $f(a) \jdeq b'$ and  
$(\lam{x:A} b)(a) \jdeq b'$ where $b' $ is obtained by replacing all
occurences of $x$ in $b$ by $a$.

An example of a dependent function is $\fmax : \prd{x:\nat} \Fin(n+1)$
which returns the largest element in a finite type, $\fmax(n) \defeq
n_{n+1}$. 
Another source of dependent function types
are function which are \emph{polymorphic} over a given universe,
e.g. the polymorphic indentity function $\idf : \prd{A:U} A \to A$
which we define as $\idf(A) \defeq \lam{x:A}x$.

As for ordinary functions we use currying to define functions in
several parameters. However in the dependent case the 2nd domain may
depend on the first one and the codomain may depend on both. That is
given $A:U$, $B : A \to U$ and $C : \prd{x:A}(B(x) \to C$ we construct
the type $\prd{x:A}{y : B(x)} C(x,y)$ of function with two
arguments. Given $a:A$ and $b:B(a)$ we build $f(a)(b) : C(a,b)$ which
as before we write as $f(a,b) : C(a,b)$.


% When we view propositions as types the non-dependent function type
% corresponds to implication, i.e. given types $P,Q$ a proof of $P \to
% Q$ is a function $f$ assigning to each proof $p:P$ a proof $f(p) : Q$.

% The dependent function type corresponds to universal
% quantification: Given a type $A$ and a family $P(x)$ assuming $x:A$
% which we view as a predicate over $A$ we can form $\prd{x:A}B(x)$
% whose elements are proofs that $B(x)$ holds for all $x:A$ by assigning
% to each $a:A$ a proof $f(a) : B(a)$.

% We summarize that from the type-theoretic point of view the concepts
% of an ordinary function, implication and universal quantification are
% unified using the notion of a dependent function type or $\Pi$-type.

\section{Product types}
\label{sec:finite-product-types}

Given types $A,B:\UU$ we introduce the type $A\times B:\UU$ which
corresponds to the cartesian product of two types. Its elements are
pairs $\tup{a}{b} : A \times B$ where $a:A$ and $b:B$. We also
introduce a nullary product type which is called the unit type $\unit
: \UU$ which has only one element $\star : \unit$.

How can we define functions out of a product type? Let us first
consider the definition of a non-dependent function $f : A\times B \to
C$. To define such a function it is enough to prescibe how $f$ is
behaving when applied to a pair $f(\tup{a}{b})$. This is achieved by
providing a function $g : A \to B \to C$ and defining
\[ f(\tup{a}{b}) \defeq g(a)(b) \]
We avoid here to write $g(a,b)$ to emphasize that $g$ is not a
function over a product. 

In particular we can derive the functions
\begin{eqnarray*}
  \fst & : & A \times B \to A \\
  \snd & : & A \times B \to B
\end{eqnarray*}
with the defining equations 
\begin{eqnarray*}
  \fst(\tup{a}{b}) & \defeq & a \\
  \snd(\tup{a}{b}) & \defeq & b
\end{eqnarray*}

We can package this principle into a constant which we call the
\emph{recursor} for product
types, that is given types $A,B : \UU$ we have
\[\rec{A\times B} : \prd{C:\UU'}(A \to B \to C) \to A \times B \to C\]
with the defining equation
\[\rec{A\times B}(C,g,\tup{a}{b}) = g(a)(b). \]
The name recursor is a bit unfortunate since no recursion is taking
place. However, this will change once we consider inductive types like
the natural numbers.
Note that the universes $\UU,\UU'$ may be different, i.e. we can
construct functions in higher or lower universes. 
We leave it as a simple exercise to show that the recursor can be
derived from the projections and vice versa.
% Ex: Derive from projections

We also have a recursor for the unit type:
\[\rec{\unit} : \prd{C:\UU'}C \to \unit \to C\]
with the defining equation
\[ \rec{\unit}(C,c,\star) \defeq c \]
However, this eliminator is completely useless
because we could have defined such a function directly
by simply ignoring the argument of type $\unit$.

To be able to derive dependent functions over the product type we have
to generalize the previous section. Given $C: A \times B \to U$ we
define a function $f : \prd{x : A \times B} C(x)$ by providing a
function $g : \prd{x:A}\prd{y:B} C(\tup{x}{y})$ and the defining equation
\[ f(x,y) \defeq g(x)(y) \] 
An example is that we can prove the principle of surjective pairing, i.e.
we can construct a function
\[ \spr : \prd{x:A \times B} \tup{\fst(x)}{\snd{x}} =_{A\times B} x \]
Here we are using the identity type which we are going to introduce below. All
we need here is to know that there s a proof of reflexivity $\refl{x} : x =_A x$ for $x:A$. 
Given this we can define
\[ \spr(\tup{a}{b}) \defeq \refl{\tup{a}{b}} \]
This construction works because in the case that $x \defeq \tup{a}{b}$ we can 
calculate 
\[ \tup{\fst(\tup{a}{b})}{\snd{\tup{a}{b}}} \jdeq \tup{a}{b} \]
using the defining equations for the projections. And hence 
\[ \refl{\tup{a}{b}} : \tup{\fst(\tup{a}{b})}{\snd{\tup{a}{b}}} = \tup{a}{b} \]
This means that to prove a property for all elements of a product it is enough 
to prove it for its canonical elements, the tuples. This trivial example shows that the construction of a dependent function corresponds to a proof by induction.
% example ?

We can package this principle into another constant which we call
\emph{induction} for product types, that is given $A,B : \UU$ we have
\[ \ind{A\times B} : \prd{C:A \times B \to \UU'} (\prd{x:A}{y:B}
C(\tup{x}{y})) \to \prd{x:A \times B} C(x) \]
with the defining equation 
\[ \ind{A\times B}(C,g,\tup{a}{b}) \defeq g(a)(b) \]
It is easy to see that the recursor is a special case of induction
in the case that the family $C$ is constant.
We can read induction propositionally as saying that a property which
is true for all pairs holds for all elements of the product type.

Induction for the unit type turns out to be more useful then the
recursor: 
\[ \ind{\unit} : \prd{C:\unit \to \UU'} C(\star) \to \prd{x:\unit}C(x)\]
with the defining equation
\[ \ind{\unit}(C,c,\star) \defeq c \]
Induction enables us to \emph{prove} that the only inhabitant of the
unit type is $\star$, that is we can construct
\[\un : \prd{x:\unit} x = * \]
by using the defining equations
\[\un(\star) \defeq \refl{\star} \]
or equivalently by using induction:
\[\un \defeq \ind{\unit}(\lam{x:\unit} x = *,\refl{\star}) \]

\section{Dependent pair types (\texorpdfstring{$\Sigma$}{Σ}-types)}
\label{sec:sigma-types}

As before for function types in type theory we are often using a
more general version of the product type which allow the type of
the 2nd component to vary dependent on the choice of the first
component of a pair. This is called the $\Sigma$-type because it
corresponds to an infinite sum (in the sense of coproducts or
disjoint union) over a given type.

Given a type $A:\UU$ and a family $B : A \to \UU$ the dependent
pair type is written as $\sm{x:A} B(x) : \UU$. Its elements are
pairs $\tup{a}{b} : \sm{x:A} B(x)$ given $a:A$ and $b:B(a)$.
If $B$ is constant then the dependent pair type is the
ordinary cartesian product type:
\[ (\sm{x:A} B(x)) \jdeq (A \times B).\]
All the constructions on $\Sigma$-types arise as straightforward generalisations of the ones for product types.

To define a non-dependent function out of a $\Sigma$-type
$f : (\sm{x:A} B(x)) \to C$ we provide a function 
$g : \prd{x:A} B(x) \to C$ and we can define $f$ via the defining
equation
\[ f(\tup{a}{b}) \defeq g(a)(b) \]
We can derive the projections for $\Sigma$-types:
\begin{eqnarray*}
  \fst & : & \sm{x : A}B(x) \to A \\
  \snd & : & \prd{p:\sm{x : A}B(x)}B(\fst(p))
\end{eqnarray*}
which have the interesting twist that the first projection appears
in the type of the second. Hence we have to define it first!
We apply the principle we have just introduced and define
\begin{eqnarray*}
  \fst(\tup{a}{b}) & \defeq & a \\
  \snd(\tup{a}{b}) & \defeq & b
\end{eqnarray*}
To convince ourselves that the equation defining the 2nd
projection is correct we note that $B (\fst(\tup{a}{b})) \jdeq
B(a)$ using the defining equation for the first projection and
indeed $b : B(a)$. 

We package this construction into the recursor for $\Sigma$:
\[ \rec{\sm{x:A}B(x)} : \Pi{C:\UU'}(\prd{x:A} B(x) \to C) \to
(\sm{x:A}B(x)) \to C \]
with the defining equation
\[ \rec{\sm{x:A}B(x)}(C,g,\tup{a}{b}) \defeq g(a)(b) \]

To construct a dependent function out of a $\Sigma$-type we 
need a family $C : (\sm{x:A} B(x)) \to \UU'$ and a function
\[ g : \prd{x:A} B(x) \to C(\tup{a}{b}) \]
then we can derive a function 
\[ f : \prd{p : \sm{x:A}B(x)} C(p) \]
with  defining equation
\[ f(\tup{a}{b}) \defeq g(a)(b).\]
This scheme can be packaged into induction for $\Sigma$-types
\begin{align*}
  \ind{\Sigma{x:A}B(x)} : & \prd{C:(\sm{x:A} B(x)) \to \UU'}\\
   & (\prd{x:A} B(x) \to C(\tup{a}{b})) \\
   & \to \prd{p : \sm{x:A}B(x)}
\end{align*}
with the defining equation 
\[ \ind{\sm{x:A}B(x)}(C,g,\tup{a}{b}) \defeq g(a)(b) \]
And as before the recursor arises as the special case of induction
when the family $C$ is constant.

As an example we derive the \emph{type-theoretic axiom of choice}
which isn't really an axiom because it is provable from the rules
for $\Sigma$-types. We assume types $A,B : \UU$ and a 
family $R : A \to B \to \UU$ to derive
\[ \ac : (\prd{x:A} \sm{y :B} R(x,y)) \to (\sm{f:A\to B}
\prd{x:A} R(x,f(x))) \]
Reading $\Pi$ as forall and $\Sigma$ as there exists, the type
expresses: \emph{if for all $x:A$ there is a $y:B$ such that
  $R(x,y)$ then there is a function $f : A \to B$ such that for
  all $x:A$ we have that $R(x,f(x))$.} Using the projections we have
just introduced we define $\ac$ as
\[ \ac(g) \defeq \tup{\lam{x:A} \fst(g(x))}{\lam{x:A} \snd(g(x))} \]
We convince ourselves that indeed given $g:\prd{x:A} \sm{y :B}
R(x,y)$:
\begin{eqnarray*}
\lam{x:A} \fst(g(x)) & : &  A \to  B \\
\lam{x:A} \snd(g(x)) & : &  \prd{x:A} R(a,\fst(g(x))) 
\end{eqnarray*}
and observing that 
\[ \prd{x:A} R(x,\fst(f(x))) \jdeq (\lam{f:A\to B} \prd{x:A}
R(x,f(x)))(\lam{x:A} \fst(g(x))) \]
we can conclude that
\[ \tup{\lam{x:A} \fst(g(x))}{\lam{x:A} \snd(g(x))} : (\sm{f:A\to B}
\prd{x:A} R(x,f(x))) .\]
This construction shows that this is very different to the axiom
of choice in set theory: the premise was already given by a
function whose result are pairs, all we have done is the take it apart into two
functions: one representing the choice and the other its correctness.
% ex use ind
% derive dependent ac

\section{Coproduct types}
\label{sec:coproduct-types}

The coproduct of two types $A+B:\UU$ given $A,B:\UU$  corresponds to the \emph{disjoint
  union} in set theory. Here it is a fundamental construction
since the union of types doesn't exist. We also introduce a
nullary version: the empty type $\emptyt:\UU$.

The elements of $A+B$ are $\inl(a) : A+B$ for $a:A$ and
$\inr(b):A+B$ for $b:B$. There are no elements in the empty type. 

To construct a non-dependent function $f : A+B \to C$ we need 
functions $g_0 : A \to C$ and $g_1 : B \to C$. Then $f$ is defined
via the defining equations
\begin{eqnarray*}
  f(\inl(a)) & \defeq & g_0(a) \\
  f(\inl(b)) & \defeq & g_1(b) 
\end{eqnarray*}
This is the function $f$ is defined by cases. As before we can
derive the recursor:
\[ \rec{A+B} : \prd{C:A+B \to \UU'}(A \to C) \to (B\to C) \to A+B
\to C\]
with the defining equations
\begin{eqnarray*}
\rec{A+B}(C,g_0,g_1,\inl(a)) & \defeq & g_0(a) \\
\rec{A+B}(C,g_0,g_1,\inr(b)) & \defeq & g_1(b)
\end{eqnarray*}

We can always construct a function $f : \emptyt \to C$ without
having to give any defining equations because $f$ will never be
used on any canonical elements. Thus the recursor for $f$ is
$\rec{\emptyt} : \prd{C:\UU'} \emptyt \to C$
which constructs the canonical function over the empty set or
logically corresponds to the principle \emph{ex falso quod libet}. 

To construct a dependent function assume as given a family 
$C: (A + B) \to \UU'$. To define a function $f:\prd{x:A+B}C(x)$ we
require 
\begin{eqnarray*}
  g_0 & : & \prd{a:A} C(\inl(a)) \\
  g_1 & : & \prd{b:B} C(\inr(b))
\end{eqnarray*}
to derive $f$ with the defining equations:
\begin{eqnarray*}
  f(\inl(a)) & \defeq & g_0(a) \\
  f(\inl(b)) & \defeq & g_1(b) 
\end{eqnarray*}
We package this scheme into induction for coproducts:
\begin{align*}
 \ind{A+B} : & \prd{C: (A + B) \to \UU'}   \\
& \prd{a:A} C(\inl(a)) \to \prd{b:B} C(\inr(b))\\
& \to \prd{x:A+B}C(x) 
\end{align*}
As before the recursor arises in the case that the family $C$ is
constant. 

Induction for the empty type
\[ \ind{\emptyt} : \prd{C:\emptyt \to \UU}\prd{z:\emptyt} C(z) \]
gives us a way to define a trivial dependent function out of the
empty type. In the presence of $\eta$-equality it is derivable
from the recursor.
% ex

\section{The type of booleans}
\label{sec:type-booleans}

The type of booleans $\bool:\UU$ has exactly two elements 
$\btrue,\bfalse : \bool$. It is clear that we could construct this
type out of coproduct and unit types as $\unit + \unit$. However,
since it is used frequently we give the explicit rules here.
Indeed, we are going to observe that we can also go the other way
and derive binary coproducts from $\Sigma$-types and $\bool$.

To derive a function $f : \bool \to C$ we need $c_0,c_1 : C$ and
add the defining equations
\begin{eqnarray*}
  f(\bfalse) & \defeq & c_0 \\
  f(\btrue) & \defeq & c_1
\end{eqnarray*}
The recursor corresponds to the if-then-else construct in
functional programming:
\[ \rec{\bool} : \prd{C:\UU'}  C \to C \to \bool \to C \]
with the defining equations
\begin{eqnarray*}
  \rec{bool}(C,c_0,c_1,\bfalse) & \defeq & c_0 \\
  \rec{bool}(C,c_0,c_1,\btrue) & \defeq & c_1
\end{eqnarray*}

Given a family $C : \bool \to \UU$ to derive a dependent function 
$f : \prd{x:\bool}C(x)$ we need $c_0:C(\bfalse)$ and $c_1 : C(\btrue)$ and
add the defining equations
\begin{eqnarray*}
  f(\bfalse) & \defeq & c_0 \\
  f(\btrue) & \defeq & c_1
\end{eqnarray*}
Induction tells us that true and false are indeed the only
elements of this type:
\[ \ind{\bool} : \prd{C:\bool \to \UU'}  C(\bfalse) \to C(\btrue)
\to \prd{x:\bool} C(x) \]
with the defining equations
\begin{eqnarray*}
  \ind{bool}(C,c_0,c_1,\bfalse) & \defeq & c_0 \\
  \ind{bool}(C,c_0,c_1,\btrue) & \defeq & c_1
\end{eqnarray*}

Given $\Sigma$ and $\bool$ we could have defined
\[ A + B \defeq \sm{x:\bool} \rec{\bool}(\UU,A,B,x) \]
and
\begin{eqnarray*}
  \inl(a) & \defeq & \tup{\bfalse}{a} \\
  \inr(a) & \defeq & \tup{\btrue}{a} \\
\end{eqnarray*}
We leave it as an exercise to derive induction. Does the derived
induction constant satsifies the same defining equations?
%ex
Certainly this construction just reflects the fact that
$\Sigma$-types are general big disjoint union from which as a
special case with index $\bool$ we can derive ordinary binary
coproducts. 

Indeed, we can play the same game for products and $\Pi$-types:
\[ A \times B \defeq \prd{x:\bool}\rec{\bool}(\UU,A,B,x) \]
Tupling can be constructed using induction for bool:
\[ \tup{a}{b} \defeq \ind{\bool}(\rec{\bool}(\UU,A,B),a,b) \]
While the projections are straightforward applications
\begin{eqnarray*}
  \fst(p) & \defeq & p(\btrue) \\
  \snd(p) & \defeq & p(\bfalse) \\
\end{eqnarray*}
the derivation of the induction for products is a bit more
involved and requires function extensionality. We get an
induction constant but not the same judgmental equalities.
This is a recurrent issue with extensional encodings in type
theory. 

Unlike in classical mathematics we
do not identify propositions with $\bool$ (instead we identify
propositions with types). Hence the type $A \to \bool$ does not
represent all subsets of $A$ but only the decidable ones.

\section{Identity types}
\label{sec:identity-types}

While the previous constructions can be seen as generalisations of
standard set theoretic constructions, the identity type seems to be
a particular feature of type theory. Moreover, homotopy type theory is
based on a reinterpretation of identity types embracing a
proof-relevant understanding of identity types: there can be more than
one reason that two objects are equal corresponding two the fact that
there can be more than one paths between two points.

Given a type $A:\UU$ and two elements $a,b:A$ we can form the type $a=_A b:\UU$ in the same universe. The idea is that $a=_A b:\UU$ is the type of proofs that $a$ and $b$ are equal, or from the homotopy point f view that there is a path between $a$ and $b$. In particular there is a canonical element
$\refl : \prd{a:A} a =_A a$ --- the proof of reflexivity.

There are two different but equivalent elimination principles for
identity types:
\begin{description}
\item[Martin-L\"{o}f Rule:] 
Given a family 
\[ C : \prd{x,y:A}\prd{p:x =_A y} \UU \]
and a function
\[ c :  \prd{x:A} C(x,x,\refl{x})\]
we can construct a function
\[ f : \prd{x,y:A}{p:x =_A y} C(x,y,p) \]
with the defining equality 
\[ f(x,x,\refl{x}) \defeq c(x) \]

The Martin-L\"of rule corresponds to the following induction principle:
\begin{align*}
 \ind{=_A}^{ML} : & \prd{C : \prd{x,y:A}\prd{p:x =_A y} \UU}  \\
     & \prd{x:A} C(x,x,\refl{x})  \\
     & \to  \prd{x,y:A}{p:x =_A y} C(x,y,p)
\end{align*}
with the equality
\[ \ind{=_A}^{ML}(C,c,x,x,\refl{x}) \defeq c(x) \]
This induction constant is also called $J$.

\item[Paulin-Mohring Rule:] 

Fix an element $a:A$, given a family
\[ C : \prd{x:A}{p : a =_A x} \UU \]
and en element
\[ c : C(a,\refl{a}) \]
we obtain a function
\[ f : \prd{x:A}{p:a = x} C(x,p) \]
with the defining equality
\[ f(a,\refl{a}) \defeq c \]

Correspondingly, there is an induction constant
\begin{align*}
\ind{=_A}^{PM} : & \prd{a:A}{C : \prd{x:A}{p : a =_A x} \UU} \\
& C(a,\refl{a}) \\
& \to \prd{x:A}{p : a =_A x} C(x,p) 
\end{align*}
with the equality
\[ \ind{=_A}^{PM}(a,C,c,a,\refl{a}) \defeq c \]
%\[ g(x)(x,\refl{x}) \defeq d(x) \]
\end{description}
These principles arise from different views on the identity type,
while the Martin-L\"of rule is based on the view that the identity
type is a binary relation in the type-theoretic sense, i.e. a family
indexed by two variables, the Paulin-Mohring rule is based on viewing
the identity type over a given $a:A$, that is $a =_A -$ as a predicate
indexed only by one variable.

It is easy to see that the Martin-L\"of rule follows from the
Paulin-Mohring rule: We assume 
the premises of the Martin-L\"of rule
\begin{eqnarray*}
C & : & \prd{x,y:A}\prd{p:x =_A y} \UU  \\
c & :  & \prd{x:A} C(x,x,\refl{x})
\end{eqnarray*}
now given an element $x:A$ we can instantiate both obtaining
\begin{eqnarray*}
C' & : & \prd{y:A}\prd{p:x =_A y} \UU  \\
C' & \defeq & C(x) \\
c' & : & C'(x,\refl{x}) \\
c' & \defeq & c(x)
\end{eqnarray*}
Clearly, $C'$ and $c'$ match the premises of the Paulin-Mohring rule and hence we can construct 
\begin{eqnarray*}
g : \prd{y:A}{p : x = y} C'(y,p)
\end{eqnarray*}
with the defining equality
\[ g(x,\refl{x}) \defeq c' \]
Now we observe that $g$'s codomain is equal to $C(x,y,p)$ and discharging our assumption
$x:A$ we can derive a function 
\[ f : \prd{x,y:A}{p : x =_A y} C(x,y,p) \]
with the required definitional equality.

In terms of induction constants we can derive $\ind{=_A}^{ML}$ from $\ind{=_A}^{PM}$ using
\[ \ind{=_A}^{ML}(C,c,x,y,p) \defeq \ind{=_A}^{PM}(x,C(x),c(x),y,p) \]

The other direction is a bit more tricky: To derive the Paulin-Mohring rule from the Martin-L\"of rule 
we assume $a : A$ and the premises of the Paulin-Mohring rule:
\begin{eqnarray*}
C & : & \prd{x:A}{p : a =_A x} \UU \\  
c & : & C(a,\refl{a})
\end{eqnarray*}
it is not clear how we can use a particular instance of the Martin-L\"of rule to derive the conclusion of 
the Paulin-Mohring rule. However, we can generalize this and construct an instance of the Martin-L\"of rule which shows 
all possible instantiations of the Paulin-Mohring rule:
\begin{eqnarray*}
D & : & \prd{x,y:A}\prd{p:x =_A y} \UU' \\
D(x,y,p) & \defeq & \prd{C : \prd{z:A}{p : x =_A z} \UU} C(x,\refl{x}) \to C(y,p)
\end{eqnarray*}
we can construct the function
\begin{eqnarray*}
d & : & \prd{x : A} D(x,x,\refl{x}) \\
d & \defeq & \lam{C:\prd{z:A}{p : x =_A z} \UU}\lam{x:C(x,\refl{x})} x
\end{eqnarray*}
and hence using the Martin-L\"of rule obtain
\[ f : \prd{x,y:A}{p:x =_A y} D(x,y,p) \]
with $f(x,x,\refl(x)) \defeq d(x)$. Unfolding the definition of $D$ we can expand the type of $f$:
\[ f : \prd{x,y:A}{p:x =_A y}{C : \prd{z:A}{p : x =_A z} \UU} C(x,\refl{x}) \to C(y,p) \]
now given $x:A$ and $p:a =_A x$ we can derive the conclusion of the Paulin-Mohring rule:
\[ f(a,x,p,C,c) : C(x,p) \]
We notice that we can also derive the correct definitional equality.

In terms of induction constants the construction becomes:
\begin{align*}
\ind{=_A}^{PM}(a,C,c,x,p) \defeq \\
\ind{=_A}^{ML}( & \lam{x,y:A}{p:x =_A y} \prd{C : \prd{z:A}{p : x =_A z} \UU} \\
&\qquad C(x,\refl{x}) \to C(y,p),\\
& \lam{C:\prd{z:A}{p : x =_A z} \UU}\lam{x:C(x,\refl{x})} x,\\
& a,x,p,C,c) 
\end{align*}

The construction given above uses universes: that is if we want to model $\ind{=_A}^{PM}$ with $C : \prd{x:A}{p : a =_A x} \UU_i$ we need to use $\ind{=_A}^{ML}$ with $D:\prd{x,y:A}\prd{p:x =_A y} \UU_{i+1}$ since $D$ quantifies over al $C$ of the given type. While this is compatible with our definition of universes, one may wonder wether we can derive $\ind{=_A}^{PM}$ without using universes. This is indeed possible: we can show that $\ind{=_A}^{ML}$ entails transport (see ??) and contractibility of singletons (see ???) and that these two principles allow us to derive $ind{=_A}^{PM}$ without using universes. We leave the details of this construction as an exercise.

We can use the eliminator for equality to establish that equality is an equivalence relation, that evey function preserves equality and that every family is stable under equality. We leave the details to the next chapter where this principles are explained and derived in the context of homotopy type theory. 



\section{The natural numbers}
\label{sec:inductive-types}

The type we have introduced so far preserve finiteness. The simplest infinite type we can think of and which also turns out to be extremely useful is the type $\nat : \UU$ of natural numbers. The elements of $\nat$ are constructed using $0 : \nat$ and the successor operation $\suc : \nat \to \nat$. When denoting natural numbers we adopt the usual decimal notation $1 = \suc(0), 2 = \suc(1), 3 = \suc(2), \dots$.

To construct a non-dependent function $f : \nat \to C$ out of the natural numbers, it is enough to provide $c_0 : C$ and $c_s : \nat \to C \to C$. This gives rise to $f$ with the defining equations
\begin{eqnarray*}
  f(0) & \defeq & c_0 \\
  f(\suc(n) & \defeq & c_s(n,f(n))
\end{eqnarray*}
We say that $f$ is defined by \textbf{primitive recursion}. As an example we define addition $\add : \nat \to \nat \to \nat$ with $C = \nat \to \nat$:
\begin{align*}
  \add_0 & : \nat \to nat \\
  \add_0 (n) & \defeq n \\
  \add_s & : \nat \to (\nat \to \nat) \to (\nat \to \nat) \\
  \add_s(n,g,m) & = \suc(g(m))
\end{align*}
We thus obtain $\add : \nat \to \nat \to \nat$ satisfying the definitional equalities
\begin{eqnarray*}
  \add(0,n) & \jdeq & n \\
  \add(\suc(m),n) & \jdeq & \suc(\add(m,n)) 
\end{eqnarray*}
As usual we write $\add(m,n)$ as $m+n$. 
% ex: define multiplication and exponentiation.

The corresponding recursion combinator is 
\[\rec{\nat}  : \prd{C:\UU} C \to (\nat \to C \to C) \to \nat \to C \]
with the defining equations
\begin{eqnarray*}
\rec{\nat}(C,c_0,c_s,0)  & \defeq & c_0 \\
\rec{\nat}(C,c_0,c_s,\suc(n)) & \defeq & c_s(n,\rec{nat}(C,C_0,c_s,n)  
\end{eqnarray*}
%ex derive rec from it
Using $\rec{\nat}$ we can present $\add : \nat \to \nat \to \nat$ as the following term:
\[
\rec{\nat}(\nat \to \nat,\lam{n:\nat} n,\lam{n:\nat,g:\nat \to \nat,m :\nat} \suc(g(m))) : \nat \to \nat \to \nat  
\]
Note that we can define more than the usual primitive recursive functions because we can use higher function types. 
In particular the Ackermann function is definable in our setting.

We follow the same approach as before and show how to generalize primitive recursion to dependent functions, giving rise to an induction principle. Assume as given a family $C : \nat \to \UU$ and $c_0 : C(n)$ and $c_s : \prd{n:\nat} C(n) \to C(\suc(n))$ we construct $f : \prd{n:\nat} C(n)$ with the defining equations:
\begin{eqnarray*}
  f(0) & \defeq & c_0 \\
  f(\suc(n) & \defeq & c_s(n,f(n))
\end{eqnarray*}
We can also package this into an induction constant
\[\ind{\nat}  : \prd{C:\nat\to \UU} C(0) \to (\prd{n : \nat} C(n) \to C(\suc(n)) \to \prd{n : \nat} C(n) \]
with the defining equations
\begin{eqnarray*}
\ind{\nat}(C,c_0,c_s,0)  & \defeq & c_0 \\
\ind{\nat}(C,c_0,c_s,\suc(n)) & \defeq & c_s(n,\rec{nat}(C,C_0,c_s,n)  
\end{eqnarray*}

As an example consider the \emph{proof} that $+$ is associative. We need $\apfunc{\suc} : \prd{m,n:\nat} m =_\nat n \to \suc(m) =_\nat \suc(n)$ which is easily derivable using the induction principle for equality types (see ??? for the general definition of $\apfunc{-}$). To derive
\[\ass : \prd{i,j,k:\nat} i + (j + k) = (i + j) + k \]
it is sufficient to supply
\begin{align*}
  \ass_0 & : & \prd{j,k:\nat} 0 + (j + k) = (0+ j) + n \\
  \ass_s & : & \prd{n:\nat}(\prd{j,k:\nat} i + (j + k) = (i + j) + n) \\
  && \to \prd{j,k:\nat} \suc(n) + (j + k) = (\suc(n) + j) + n 
\end{align*}
To derive $\ass_0$ we use that $0+n \defeq n$ and hence  $0 + (j + k) \jdeq (0+ j) + n$ Hence we can just set
\[ ass_0(j,k) \defeq \refl{j+k} \]
For $\ass_s$ we exploit $\suc(m)+n \jdeq \suc(m+n)$ and hence 
\begin{eqnarray*}
   \suc(n) + (j + k)  & \jdeq & \suc(n+(j+k)) \\
   (\suc(n)+j)+k & \jdeq & \suc((n+j)+k
\end{eqnarray*}
Hence we can define
\[\ass_s(n,h,j,k) \defeq \apfunc{\suc}(n+(j+k),(n+j)+k,h(j,k)) \]

Using $\ind{nat}$ we can construct an expression 
\begin{align*}
  \ind{nat}(&\lam{i:\nat}\prd{j,k:nat} i + (j + k) = (i + j) + k ,\\
  & \lam{j,k:\nat}\refl{j+k}, \\
  & \lam{n:\nat}{h:\prd{j,k:\nat} i + (j + k) = (i + j) + n}{j,k:\nat}\apfunc{\suc}(n+(j+k),(n+j)+k,h(j,k)) )\\
& : \prd{i,j,k:\nat} i + (j + k) = (i + j) + k
\end{align*}

% The paradigm example of an inductive type is the type $\nat$ of unary natural numbers.  These objects of type $\nat$ are generated from the natural number $0$ by repeatedly applying the successor operation.  So we say that there are two {\bf introduction rules} for $\nat$. 
%   \[ 0:\nat\]
% and
%   \[ \mbox{ from $n:\nat$ infer $\suc(n):\nat$}.\]
% The inductive character of $\nat$ is captured by the, so called, elimination and computation rules for $\nat$.  We prefer to call them the rules for defining a function on $\nat$ by primitive recursion. 

% If $C[x]$ is a family on $x:\nat$, $c_0:C[0]$ and $c_\suc:\prd{x:nat,y:C[x]}C[\suc(x)$ then we may introduce a function $f:\Pi_{x:\nat}C(x)$ by the following primitive recursion defining equations; one defining equation for each introduction rule.

%   \[\left\{\begin{array}{rl}
% f(0)\defeq& c_0, \mbox{ and}\\
% f(\suc(x))\defeq& c_\suc(x,f(x))\mbox{ for } x:\nat
%   \end{array}\right.\]
% It is sometimes convenient to make explicit the dependence of $f$ on $c_0$ and $c_\suc$ by using the primitive recursive operator $\rec{\nat}$.  So we have
%   \[ \rec{\nat}(x,c_0,c_\suc):\prd{x:\nat}C(x)\]
% with the defining equations
% \begin{eqnarray*}
%   \rec{\nat}(x,c_0,c_\suc)(0) & \defeq & c_0 \\
%   \rec{\nat}(x,c_0,c_\suc)(\suc(n)) & \defeq & c_s(n, \rec{\nat}(x,c_0,c_\suc)) \\
% \end{eqnarray*}

% As an example we define the function $(+)(m) : \nat \to \nat$ for any
% $m : \nat$ using the constant family $\nat$ and the defining
% equations:
% \begin{eqnarray*}
%   m + 0 & \defeq & m \\
%   m + \suc(n) & \defeq & \suc(m + n)
% \end{eqnarray*}
% By abstracting $m$ using the rules for $\Pi$-types we can derive the
% binary function $- + - : \nat \to (\nat \to \nat)$.

% We use the same principle to prove properties by induction. While it
% is trivial to see that $+$ is right neutral because $m + 0 \jdeq m$
% we need to prove that $0 + m =_\Nat m$ is inhabited. We do this by
% constructing a function $f : \Pi_{n : \nat} 0 + n =_\Nat n$. 
% The family is $0 + n =_\Nat n$ over $n : \Nat$. In the case
% for $0$ we have that $0 + 0 \jdeq 0$ hence we can define
% \[ f(0) \defeq \refl{0} \]
% For $\suc(n)$ we have that $0 + \suc(n) \jdeq \suc(0 +
% n)$ hence we define 
% \[ f(\suc(n)) \defeq \mapfunc{\suc}(f(n)). \]
% In this case we can show that the type is propositional, that is that
% all elements of the type are equal. In this situation we may omit any
% explicit reference to proof objects and the type-theoretic proof does
% not look different to a conventional one in predicate logic. 

\section{Propositions as types}
\label{sec:pat}

\textbf{TODO:} A discussion of how to speak type formers in English, e.g.\ ``for all $x:A$, something or other'' means ``$\prd{x:A}$ something or other'', and so on.
For now, we will use ``$A$ or $B$'' to mean $A+B$ and ``there exists $x:A$'' to mean $\sm{x:A}$, but perhaps we should mention that we will refine it further later on.
For instance, sometimes classical statements involving these connectives need to be interpreted differently (see \autoref{subsec:prop-subsets} and wherever we discuss $(-1)$-truncations).

We will say that a type $A$ is \textbf{inhabited} to mean that we assert $A$ itself as a proposition (i.e.\ we construct a term belonging to $A$, usually unnamed).



% Local Variables:
% TeX-master: "main"
% End:
