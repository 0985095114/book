\newcommand{\zero}{\ensuremath{\mathbf{0}}\xspace}
\newcommand{\one}{\ensuremath{\mathbf{1}}\xspace}
\newcommand{\two}{\ensuremath{\mathbf{2}}\xspace}
\newcommand{\three}{\ensuremath{\mathbf{3}}\xspace}
\newcommand{\nat}{\ensuremath{\mathbf{N}}\xspace}
\newcommand{\true}{\ensuremath{\mathbf{true}}\xspace}
\newcommand{\false}{\ensuremath{\mathbf{false}}\xspace}
\newcommand{\rec}{\ensuremath{\mathbf{rec}}\xspace}
\newcommand{\z}{\ensuremath{0}\xspace}
\newcommand{\s}{\ensuremath{\mathbf{s}}\xspace}
\newcommand{\alt}{\;|\;\;}
\newcommand{\der}{\vdash}

\chapter{Induction}
\label{cha:induction}

% Local Variables:
% TeX-master: "main"
% End:

\section{Inductive types}

An \emph{inductive type} can be intuitively understood as a type generated by a certain finite collection of constructors. To specify an inductive type formally,
we use a schematic definition, where we list the name and type of each constructor separately, e.g.:
\begin{align*}
  \two \defeq \; & \true : \two \\
         \alt & \false : \two
\end{align*}
The above definition declares the type $\two$ of Booleans to be the inductive type generated by two constant constructors $\true$ and $\false$. We can similarly define the types $\zero$ (aka Empty, Void), $\one$ (aka Unit), $\three$, and so on, with 0, 1, 3, or more constructors respectively. 

Another extremely important inductive type is the type $\nat$ of natural numbers:  
\begin{align*}
  \nat \defeq \; & \z : \nat \\
        \alt & \s : \nat \to \nat 
\end{align*}
As expected, $\nat$ is generated by a constant constructor $\z$ for 0 and a unary constructor $\s$ taking a natural number $n : \nat$ to its successor $\s(n) : \nat$. It is understood that for any inductive type, different constructors construct different terms and each constructor itself is injective. This ensures that as desired, 0 is not a successor of any other natural number and that no two natural numbers have the same successor.

What can we do with such an inductive type? An intuitive understanding of an inductive type $A$ is that it \emph{behaves as if the only inhabitants of $A$ were the terms constructed solely by applying the constructors of $A$}; we refer to these particular terms as the \emph{canonical terms of $A$}. In an empty context,
each term of $A$ is canonical - for instance, any closed term of an identity type is necessarily the identity path. In the setting of nonempty contexts,  
interesting and often nontrivial behavior may occur, such as the one exhibited by identity types.

In the case of the type $\two$ of Booleans, assuming that each term is canonical simply means that each term $b : \two$ is identical to either $\true$ or $\false$. In particular, we have the principle of \emph{dependent elimination}:

\begin{itemize}
\item When proving a statement $\Gamma \der b : \two E(b) : \type$ about \emph{all} Booleans, it suffices to prove it for $\true$ and $\false$, i.e., give proofs
$\Gamma \der e_t : E(\true)$ and $\Gamma \der e_f : E(\false)$.
\end{itemize}

Furthermore, the resulting proof $\Gamma, b : \two \der \rec_\two(E,e_t,e_f,b): E(b)$ behaves as expected when applied to the constructors $\true$ and $\false$; this principle is expressed by the \emph{computation rules} associated with dependent elimination:
\begin{itemize}
\item The proof $\Gamma \der \rec_\two(E,e_t,e_f,\true) : E(\true)$ is identical to $e_t$.
\item The proof $\Gamma \der \rec_\two(E,e_t,e_f,\false) : E(\true)$ is identical to $e_f$.
\end{itemize}
For simplicity we often omit the ambient context $\Gamma$.

The rules for the type $\two$ of Booleans allow us to reason by \emph{case analysis}. Since neither of the two constructors takes any arguments, this is all we need for Booleans. However, for more complicated types such as the type $\nat$ of natural numbers, true induction is often needed (hence the name \emph{inductive type}):

\begin{itemize}
\item When proving a statement $x : \nat \der E(x) : \type$ about \emph{all} natural numbers, it suffices to prove it for $\z$ and for $\s(n)$, assuming it holds
for $n$. This entails giving the proofs $\der e_z : E(\z)$ and $n : \nat, y : E(n) \der e_s(n,y) : E(\s(n))$.
\end{itemize}
The variable $y$ represents our inductive hypothesis. The associated computation rules are as expected:
\begin{itemize}
\item The proof $\der \rec_\nat(E,e_z,e_s,\z) : E(\z)$ is identical to $e_z$.
\item For any $n : \nat$, the proof $\der \rec_\nat(E,e_z,n.y.e_s(n,y),\s(n)) : E(\s(n))$ is identical to $e_s(n,\rec_\nat(E,e_z,n.y.e_s(n,y),n))$.
\end{itemize}
The dependent function $\rec_\nat$ can thus be understood as defined recursively.


\section{Weak inductive types}
Outline of inductive types with propositional computation rules, equivalence between dependent and simple elim, encoding of nats as W-types