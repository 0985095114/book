\let\zero\emptyt
\let\one\unit
\let\two\bool
\newcommand{\twoh}{\ensuremath{\mathbf{2^h}}\xspace}
\newcommand{\three}{\ensuremath{\mathbf{3}}\xspace}
\newcommand{\natw}{\ensuremath{\mathbf{N^w}}\xspace}
\newcommand{\nath}{\ensuremath{\mathbf{N^h}}\xspace}
\let\true\btrue
\let\false\bfalse
\newcommand{\z}{\ensuremath{0}\xspace}
\newcommand{\zw}{\ensuremath{0^\mathbf{w}}\xspace}
\newcommand{\myInd}{\ensuremath{\mathbf{Ind \;}}\xspace}
\let\s\suc
\newcommand{\sw}{\ensuremath{\mathbf{s^w}}\xspace}
\newcommand{\alt}{\;|\;\;}
\newcommand{\disj}[2]{#1 + #2}
\newcommand{\der}{\vdash}
\newcommand{\dbl}{\ensuremath{\mathbf{double}}}
\newcommand{\nalg}{\mathsf{\nat Alg}}
\newcommand{\nhom}{\mathsf{\nat Hom}}
\newcommand{\ishinitw}{\ensuremath{\mathsf{isHinit_W}}}
\newcommand{\ishinitn}{\ensuremath{\mathsf{isHinit_\nat}}}
\newcommand{\w}{\mathsf{W}}
\newcommand{\walg}{\mathsf{\w Alg}}
\newcommand{\whom}{\mathsf{\w Hom}}

\chapter{Induction}
\label{cha:induction}

In \autoref{cha:typetheory}, we introduced many ways to form new types from old ones.
Except for (dependent) function types and universes, all these rules are special cases of the general notion of \emph{inductive definition}.
In this chapter we study inductive definitions more generally.


\section{Finite types and natural numbers}
\label{sec:bool-nat}

An \emph{inductive type} $X$ can be intuitively understood as a type ``freely generated'' by a certain finite collection of \emph{constructors}, each of which is a function (of some number of arguments) with codomain $X$.
This includes functions of zero arguments, which are simply elements of $X$.

When describing a particular inductive type, we list the constructors with bullets.
For instance, the type \bool from \autoref{sec:type-booleans} is inductively generated by the following constructors:
\begin{itemize}
\item $\btrue:\bool$
\item $\bfalse:\bool$
\end{itemize}
Similarly, $\unit$ is inductively generated by the constructor:
\begin{itemize}
\item $\ttt:\unit$
\end{itemize}
while $\emptyt$ is inductively generated by no constructors at all.
An example where the constructor functions take arguments is the coproduct $A+B$, which is generated by the two constructors
\begin{itemize}
\item $\inl:A\to A+B$
\item $\inr:B\to A+B$.
\end{itemize}
And an example with a constructor taking multiple arguments is the cartesian product $A\times B$, which is generated by one constructor
\begin{itemize}
\item $(-,-) : A\to B \to A\times B$.
\end{itemize}
Crucially, we also allow constructors of inductive types that take arguments from the inductive type being defined.
For instance, the type $\nat$ of natural numbers has constructors
\begin{itemize}
\item $0:\nat$
\item $\suc:\nat\to\nat$.
\end{itemize}

How should we intuitively understand the notion of an inductive type? Taking the type $\two$ as the simplest example, we would expect it to behave \emph{as if} its only inhabitants were $\true$ and $\false$. We note that the \emph{as if} clause is crucial: in a nonempty context, it is generally not the case that any $b : \two$ is either $\true$ or $\false$; for instance $b$ may be a variable declared in the context. Even when considering only empty contexts, the conclusion that any $\cdot \vdash b : \two$ is either $\true$ or $\false$ is far from trivial - this statement is known as \emph{canonicity for $\two$}. While several proofs of this result exist for Martin-L{\"o}f type theory, whether canonicity holds in the presence of the Univalence Axiom is currently an open problem.

The intuition that the inductive type $\two$ behaves as if its only inhabitants were $\true$ and $\false$ is made precise in the following principle of \emph{(dependent) elimination}:

\begin{itemize}
\item When proving a statement $E : \two \to \type$ about \emph{all} Booleans, it suffices to prove it for $\true$ and $\false$, i.e., give proofs
$ e_t : E(\true)$ and $e_f : E(\false)$.
\end{itemize}

Furthermore, the resulting proof $\rec\two(E;e_t;e_f): \prd{b : \two}E(b)$ behaves as expected when applied to the constructors $\true$ and $\false$; this principle is expressed by the \emph{computation rules}:
\begin{itemize}
\item We have $\rec\two(E;e_t;e_f;\true) \equiv e_t$.
\item We have $\rec\two(E;e_t;e_f;\false) \equiv e_f$.
\end{itemize}

The rules for the type $\two$ of Booleans allow us to reason by \emph{case analysis}. Since neither of the two constructors takes any arguments, this is all we need for Booleans. For natural numbers, however, case analysis is generally not sufficient: in the case corresponding to the inductive step $s(n)$, we also want to presume that the statement being proven has already been shown for $n$. This gives us the following principle of \emph{induction}: 
\begin{itemize}
\item When proving a statement $E : \nat \to \type$ about \emph{all} natural numbers, it suffices to prove it for $\z$ and for $\s(n)$, assuming it holds
for $n$. This entails giving the proofs $e_z : E(\z)$ and $e_s : \prd{n : \nat}{y : E(n)} E(\s(n))$.
\end{itemize}
The variable $y$ represents our inductive hypothesis. Like for Booleans, we also have the associated computation rules for the function $\rec\nat(E;e_z;e_s) : \prd{x:\nat} E(x)$:
\begin{itemize}
\item We have $\rec\nat(E;e_z;e_s;\z) \equiv e_z$.
\item For any $n : \nat$, we have $\rec\nat(E;e_z;e_s;\s(n)) \equiv e_s(n;\rec\nat(E;e_z;e_s;n))$.
\end{itemize}
The dependent function $\rec\nat(E;e_z;e_s)$ can thus be understood as being defined recursively on the argument $x : \nat$, via the recurrences $e_z$ and $e_s$: When $x$ is zero, the function simply returns $e_z$. When $x$ is the successor of another natural number $n$, the result is obtained by taking the recurrence $e_s$ and plugging in the specific predecessor $n$ and the recursive call value $\rec\nat(E;e_z;e_s;n)$.

As an example we look at how to define a function on natural numbers which doubles its argument. We wish to apply dependent elimination with the constant type family $E \defeq \lambda(x : \nat), \nat$ since the intended type of $\dbl$ is $\nat \to \nat$. We first need to supply the value of $\dbl(\z)$, which is easy: we put $e_z \defeq \z$. Next, to compute the value of $\dbl(\s(n))$ for a natural number $n$, we first compute the value of $\dbl(n)$ and then perform the successor operation twice. This is captured by the recurrence $e_s(n,y) \defeq \s(\s(y))$; the variable $y$ stands for the result of the recursive call $\dbl(n)$. Thus, we define
\[ \dbl \defeq \rec\nat(\big(\lambda(x :\nat), \nat\big); \; \z; \;  \big(\lambda(n: \nat) \lambda (y:\nat), \s(\s(y))\big)) \]
This indeed has the correct computational behavior: for example, we have 
\begin{align*}
\dbl(\s(\s(\z))) & \equiv e_s(\s(\z), \dbl(\s(\z))) \\
                 & \equiv \s(\s(\dbl(\s(\z)))) \\
                 & \equiv \s(\s(e_s(\z,\dbl(\z)))) \\
                 & \equiv \s(\s(\s(\s(\dbl(\z))))) \\
                 & \equiv \s(\s(\s(\s(e_z)))) \\
                 & \equiv \s(\s(\s(\s(\z))))
\end{align*}
as desired, where an arbitrary evaluation strategy was chosen.

There are many similar cases when one wants to use dependent elimination into a constant family.
This is referred to as \emph{simple elimination}, which corresponds to recursion---instead of proving a separate statement $P(n)$ for each natural number $n$ as done in the dependent case, we are recursively constructing a function into a simple type $C$. In this case, we omit the $\lambda$-binder for simplicity and write $\rec\nat(C; \ldots)$ instead of $\rec\nat(\lambda (n : \nat), C; \ldots)$

The dependent elimination principle is quite strong and allows us to prove a variety of interesting theorems. For example, by specifying the terms $e_z$ and $e_s$, we  uniquely determine how the recursor behaves on canonical terms, and thus on all natural numbers. If we now have another function which obeys the same recurrence, then our intuition suggests these two functions should be equal. It turns out this is indeed the case and we have the following \emph{uniqueness principle}:

\begin{thm}\label{thm:nat-uniq}
Let $f,g : \prd{x:\nat} E(x)$ be two functions which satisfy the recurrences $e_z : E(\z)$ and $e_s : \prd{n : \nat}{y : E(n)} E(\s(n))$ up to propositional equality, i.e., such that
\begin{align*}
\id{f(\z)}{e_z} \\ 
\id{g(\z)}{e_z}
\end{align*}
and 
\begin{align*}
\prd{n : \nat} \id{f(\s(n))}{e_s(n, f(n))} \\
\prd{n : \nat} \id{g(\s(n))}{e_s(n, g(n))}
\end{align*}
Then $f$ and $g$ are equal. 
\end{thm}

\begin{proof}
We use dependent elimination with the type $E(x) \defeq \id{f(x)}{g(x)}$. For the base case, we have \[f(\z) = e_z = g(\z)\]
For the inductive case, assume $n : \nat$ such that $f(n) = g(n)$. Then
\[ f(\s(n)) = e_s(n, f(n)) = e_s(n, g(n)) = g(\s(n)) \]
The first and last equality follow from the assumptions on $f$ and $g$. The middle equality follows from the inductive hypothesis and the fact that application preserves equality. This gives us pointwise equality between $f$ and $g$; invoking function extensionality finishes the proof.
\end{proof}
We note that the function $f$ is only required to satisfy the recurrences \emph{up to propositional equality}. The theorem itself only asserts propositional equality between functions---indeed, it is possible to construct functions which satisfy the same recurrence but are not definitionally equal (exercise). It is also possible to have a function which satisfies more than one set of recurrences (exercise).

Similar uniqueness theorems can generally be formulated and shown for other inductive types as well. In the next section, we show how to use the uniqueness property together with univalence to prove that natural numbers are completely characterized by their introduction, elimination, and computation rules.

\medskip

We conclude this section by pointing out that inductive types give rise to \emph{homotopy-initial algebras}, in the following sense. Taking natural numbers as our running example, we define:

\begin{defn}
A \emph{$\nat$-algebra} is a type $C$ with two terms $c_0 : C$, $c_s : C \to C$. Thus,
\begin{align*}
\nalg \defeq \sm {C : \type} C \times (C \to C)
\end{align*}
\end{defn}

\begin{defn}
Fix any $\nat$-algebras $(C,c_0,c_s)$ and $(D,d_0,d_s)$. A \emph{$\nat$-homomorphism} between them is a function $h : C \to D$ such that $h(c_0) = d_0$ and $h(c_s(c)) = d_s(h(c))$ for any $c : C$. Thus,
\begin{align*}
\nhom \; (C,c_0,c_s) \; (D,d_0,d_s) \defeq \sm {h : C \to D} \\ (\id{h(c_0)}{d_0}) \times \prd{c:C} \id{h(c_s(c))}{d_s(h(c))}
\end{align*}
\end{defn}

\begin{defn}
A $\nat$-algebra $\chi_i$ is called \emph{homotopy-initial}, or \emph{h-initial} for short, if for any other $\nat$-algebra $\chi$, the type of $\nat$-homomorphisms from $\chi_i$ to $\chi$ is contractible. Thus,
\begin{align*}
\ishinitn \; \chi_i \defeq \prd{\chi : \nalg} \iscontr \; (\nhom \; \chi_i\; \chi)
\end{align*}
\end{defn}

As expected, h-initial algebras are unique:
\begin{thm}
Any two h-initial $\nat$-algebras $\chi_i$ and $\chi'_i$ are equal.
\end{thm}

We now have the following theorem; the proof is only sketched as it involves a large amount of technical detail.
\begin{thm}
The $\nat$-algebra $(\nat, \zero, \s)$ is homotopy initial.
\end{thm}
\begin{proof}[Sketch of proof]
Fix an arbitrary $\nat$-algebra $(C,c_0,c_s)$. Using elimination on $\nat$ it is easy to construct a $\nat$-homomorphism into $C$. This will be our
center of contraction. To show that any other homo-morphism is equal to the chosen one, we appeal to the uniqueness theorem for $\nat$.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Uniqueness of inductive types}
\label{sec:appetizer-univalence}

\newcommand{\natp}{\ensuremath{\nat'}\xspace}
\newcommand{\zp}{\ensuremath{0'}\xspace}
\newcommand{\Sp}{\ensuremath{\suc'}\xspace}
\newcommand{\dblp}{\ensuremath{\mathbf{double'}}}

Assume one has another definition of the natural numbers, for example:
\[  \natp : \type \defeq \myInd \left\{
\begin{array}{ll}
\zp : & \natp \\
\Sp : & \natp \to \natp
\end{array}
\right. \]
Again, when proving a statement $E : \natp \to \type$ for all these new natural numbers, it suffices to give the proofs $e_z : E(\zp)$ and $e_s : \prd{n : \natp}{x : E(n)} E(\Sp(n))$. In the same way as before, the function $\rec\natp(E;e_z;e_s) : \prd{n:\natp} E(n)$
has the following computation rules:
\begin{itemize}
\item We have $\rec\natp(E;e_z;e_s;\zp) \equiv e_z$.
\item For any $n : \natp$, we have $\rec\natp(E;e_z;e_s;\Sp(n)) \equiv e_s(n;\rec\natp(E;e_z;e_s;n))$.
\end{itemize}
What is the relation between $\nat$ and $\natp$?
This is not just an academic question, since the structure of the natural numbers
can be found in lists over a type with one element (arguably the oldest appearance,
on the wall of the cave), in the non-negative integers, as substructure of the
rationals and the reals, and so on. One desideratum is certainly that it is easy to
transfer results from one appearance of the natural numbers to any other.

Recall the example of the function $\dbl$ defined above. A similar function
for our new natural numbers is readily defined by duplication and adding primes:
\[ \dblp \defeq \rec\natp(\natp; \; \zp; \;  \lambda(n: \natp) \lambda (m:\natp), \Sp(\Sp(m))) \]
Simple as this may seem, it has the obvious drawback of leading to a
proliferation of duplicates. Not only functions have to be
duplicated, but also all lemmas and their proofs. For example,
an easy result such as  $\prd{n : \nat} \dbl(\s(n))=\s(\s(\dbl(n)))$ as well
as its proof by induction has to be `primed' (duplicated and then adding primes).
This becomes totally impractical when formalizing serious mathematics.
In contrast, informal mathematics just proclaims that $\nat$ and $\natp$ are
equal and can be substituted for each other whenever the need arises.

To improve this situation, we first observe that we have the following definable maps:
\begin{itemize}
\item $f \defeq \rec\nat(\nat; \; \zp; \;  \lambda(n: \nat), \Sp)
       : \nat \to\natp$,
\item $g \defeq \rec\natp(\natp; \; \z; \;  \lambda(n: \natp), \s)
       : \natp \to\nat$.
\end{itemize}
Since the composition of $g$ and $f$ satisfies the same recurrences as the identity function on $\nat$, Theorem~\ref{thm:nat-uniq} gives that $\prd{n : \nat} \id{g(f(n))}{n}$. The `primed' version of the same theorem gives $\prd{n : \natp} \id{f(g(n))}{n}$.

This makes it possible to transfer functions on $\nat$ to functions on $\natp$ (and vice versa) by using the maps $f$ and $g$ from above:
\[ \dblp \defeq \lambda(n :\natp), f(\dbl(g(n))) \]
It is an easy exercise to show that this version of $\dblp$ is equal to the earlier one.

The ability to transfer functions in this way is an improvement over `priming' but the easy result
$\prd{n : \natp} \dblp(\Sp(n))=\Sp(\Sp(\dblp(n)))$ still has to be reproved
by induction on $n:\natp$ using lots of $f$'s and $g$'s.

Here is where the Univalence Axiom steps in: using function extensionality, the identities $\prd{n : \nat} \id{g(f(n))}{n}$
and $\prd{n : \natp} \id{f(g(n))}{n}$ imply that $g\circ{}f = \idfunc[\nat]$ and $f\circ{}g = \idfunc[\natp]$. Thus
$\nat$ and $\natp$ are in fact equivalent as types.
The Univalence Axiom then gives us that $\nat$ and $\natp$ are
\emph{equal} as types, which means that there is
a path from $\nat$ to $\natp$ (and back). Functions, lemmas and proofs (and more)
can be now be transported along such paths, with considerably less overhead. For example, using the
Univalence Axiom, we can easily obtain a function carrying a proof of
$\prd{n : \natp} \dblp(\Sp(n))=\Sp(\Sp(\dblp(n)))$ to a proof of the corresponding lemma in $\nat$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Coproducts and dependent sums}
\label{sec:coprod-depsum}

A \emph{coproduct} of two types $A$ and $B$ behaves much like a disjunction: we can choose to supply either an element of $A$ or an element of $B$.
Schematically, we declare this as follows:
\[ \disj{A}{B} : \type \defeq \myInd \left\{
\begin{array}{ll}
\inl : & A \to (\disj{A}{B}) \\
\inr : & B \to (\disj{A}{B})
\end{array}
\right. \]

In this definition, $A$ and $B$ act as \emph{parameters}, i.e., they denote arbitrary types. In other words, the type constructor $+$ is really a function taking the types $A$ and $B$ as an argument and returning a new inductive type $\disj{A}{B}$. Although omitted from the above schema for clarity, the constructors $\inl$ and $\inr$ likewise take the types $A$ and $B$ as the first 2 arguments, followed by an element of $A$ (for \inl) or of $B$ (for \inr).

Elimination for a coproduct amounts to case analysis:
\begin{itemize}
\item When proving a statement $E : (\disj{A}{B}) \to \type$ about \emph{all} terms of the coproduct $\disj{A}{B}$, it suffices to prove it for $\inl(a)$ and $\inr(b)$, i.e., give proofs $e_l : \prd{a:A} E(\inl(a))$ and $e_r : \prd{b:B} E(\inr(b))$.
\end{itemize}
The associated computation rules for the function $\rec{\disj{A}{B}}(E;e_l;e_r) : \prd{x:\disj{A}{B}} E(x)$ are as expected:
\begin{itemize}
\item For each $a : A$, we have $\rec{\disj{A}{B}}(E;e_l;e_r;\inl(a)) \equiv e_l(a)$.
\item For each $b : B$, we have $\rec{\disj{A}{B}}(E;e_l;e_r;\inr(b)) \equiv e_r(b)$.
\end{itemize}

A \emph{product} of two types $A$ and $B$ (denoted by $A \times B$) is the type of pairs $(a,b)$, where $a : A$ and $b : B$. A \emph{dependent sum} is a generalization of this concept, where we allow the type $B$ to depend on $A$. A simple example is the type of pairs $(n,(c_1,\ldots,c_n))$, where the first component is a natural number $n : \nat$ and the second component is a vector $(c_1,\ldots,c_n) : \mathbf{Vec}_C(n)$ of length $n$ over another type $C : \type$.
Given $A : \type$ and $B : A \to \type$, the dependent sum of $A$ and $B$ is given schematically as
\[ \sm{a:A} B(a) : \type \defeq \myInd \left\{
\begin{array}{ll}
\mathbf{pair} : & \prd{a:A}{b:B(a)} \\
& \sm{a:A} B(a)
\end{array}
\right. \]
Thus, the parameterized inductive type $\sm{a:A} B(a)$ has a single constructor $\mathbf{pair}$. Its first two arguments (not shown) are the parameters $A$ and $B$; the remaining two arguments are the respective components $a : A$ and $b : B(a)$. We will often denote $\mathbf{pair}(a,b)$ simply by $(a,b)$.

The elimination and computation rules are very simple:

\begin{itemize}
\item When proving a statement $E : \big(\sm{a:A} B(a)\big) \to \type$ about \emph{all} terms of the dependent sum $\sm{a:A} B(a)$, it suffices to prove it for a pair $(a,b)$, i.e., give a proof $e : \prd{a:A}{b:B(a)} E((a,b))$.
\end{itemize}

\begin{itemize}
\item For any $a : A$ and $b : B(a)$, we have $\rec{\sm{a:A} B(a)}(E;e;(a,b)) \equiv e(a,b)$.
\end{itemize}
Using the elimination operator, it is very easy to construct the well-known projection functions $\pi_1$ and $\pi_2$, extracting the first resp. the second component of a pair.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{W-types}
\label{sec:w-types}

Martin-L{\"o}f's W-types, also known as the types of well-founded trees, are a generalization of such types as natural numbers, lists, and binary trees. A particular W-type is specified by giving two parameters $A : \type$ and $B : A \to \type$, written $\wtype{a:A} B(a)$.

The type $A$ represents the type of \emph{labels} for $\wtype{a :A} B(a)$, which function as constructors (however, we reserve that word for use in schematic definitions of inductive types). For instance, when defining natural numbers as a W-type, the type $A$ would be the type $\two$ inhabited by the two terms $\true$ and $\false$, since there are precisely two ways how to obtain a natural number---either it will be zero or a successor of another natural number. 

The type family $B : A \to \type$ is used to record the arity of labels: a label $a : A$ will take a family of inductive arguments, indexed over $B(a)$. We can therefore think of the ``$B(a)$-many'' arguments of $a$. These arguments are represented by a function $f : B(a) \to \wtype{a :A} B(a)$, with the understanding that for any $b : B(a)$, $f(b)$ is the ``$b$-th'' argument to the label $a$. The W-type $\wtype{a :A} B(a)$ can thus be thought of as the type of well-founded trees, where nodes are labeled by terms of $A$ and each node labeled by $a : A$ has $B(a)$-many branches.

In the case of natural numbers, the label $\true $ has arity 0, since it constructs the constant zero; the label $\false$ has arity 1, since it constructs the successor of its argument. We can capture this by using simple elimination on $\two$ to define a function $\rec\two(\bbU;\zero;\one)$ into a universe of types; this function returns the empty type $\zero$ for $\true$ and the unit type $\one$ for $\false$. We can thus define
\[ \natw \defeq \wtype{b:\two} \rec\two(\bbU;\zero;\one) \]
where the superscript \textbf{w} serves to distinguish this version of natural numbers from the previously used one.
Similarly, we can define the type of lists over $A$ as a W-type with $\disj{\one}{A}$ many labels: one nullary label for the empty list, plus one unary label for each $a : A$, corresponding to appending $a$ to the head of a list:

\[ \lst A \defeq \wtype{x:\disj{\one}{A}} \rec{\disj{\one}{A}}(\bbU; \; \zero; \; \lambda{(a:A)}, \one) \]
Schematically, we define a general W-type as follows:
\[ \wtype{x:A} B(x) : \type \defeq \myInd \left\{
\begin{array}{ll}
\supp : & \prd{a:A}{f : B(a) \to \wtype{x:A} B(x)} \\ & \wtype{x:A} B(x)
\end{array}
\right. \]

The constructor $\supp$ (short for supremum) takes a label $a : A$ and a function $f : B(a) \to \wtype{a:A} B(a)$ representing the arguments to $a$, and constructs a new element of $\wtype{x:A} B(x)$. Using our previous encoding of natural numbers as W-types, we can for instance define
\begin{align*}
\zw \defeq \supp(\true, \; \lambda(x : \zero), \rec\zero(\natw;x))
\end{align*}
Put differently, we use the label $\true$ to construct $\zw$. Then, $\rec\two(\bbU;\zero;\one; \true)$ evaluates to $\zero$, as it should since $\true$ is a nullary label. Thus, we need to construct a function $f : \zero \to \natw$, which represents the (zero) arguments supplied to $\true$. This is of course trivial, using simple elimination on $\zero$ as shown. Similarly, we can define
\begin{align*}
1^\mathbf{w} \defeq \supp(\false, \; \lambda(x : \one), 0^\mathbf{w}) \\
2^\mathbf{w} \defeq \supp(\false, \; \lambda(x : \one), 1^\mathbf{w})
\end{align*}
and so on.

We have the following elimination rule for W types:
\begin{itemize}
\item When proving a statement $E : \big(\wtype{x:A} B(x)\big) \to \type$ about \emph{all} terms of the $W$-type $\wtype{x:A} B(x)$, it suffices to prove it for $\supp(a,f)$, assuming it holds for all $f(b)$ with $b : B(a)$. 
In other words, it suffices to give a proof 
\begin{align*}
e : & \; \prd{a:A}{f : B(a) \to \wtype{x:A} B(x)}\\ & \; \prd{g : \prd{b : B(a)} E(f(b))} E(\supp(a,f))
\end{align*}
\end{itemize}

The variable $g$ represents our inductive hypothesis, namely that all arguments of $a$ satisfy $E$. To state this, we quantify over all terms of type $B(a)$, since each $b : B(a)$ corresponds to one argument $f(b)$ of $a$.

How would we define the function $\dbl$ on natural numbers encoded as a W-type? We would like to use the (simple) elimination on $\natw$ into the type $\natw$ itself. We thus need to construct a suitable function
\[e : \prd{a : \two}{f : B(a) \to \natw}{g : B(a) \to \natw} \nat\]
which will represent the recurrence for the $\dbl$ function; for simplicity we denote the type $\rec\two(\bbU;\zero;\one)$ by $B$.

Clearly, $e$ will be a function taking $a : \two$ as its first argument. The next step is to perform case analysis on $a$ and proceed based on whether it is $\true$ or $\false$. This suggests the following form
\[ e \defeq \lambda(a : \two), \; \rec\two(C,e_t,e_f,a) \]
where \[C \defeq \prd{f : B(a) \to \natw}{g : B(a) \to \natw} \natw\]
If $a$ is $\true$, the type $B(a)$ becomes $\zero$. Thus, given $f : \zero \to \natw$ and $g : \zero \to \natw$, we want to construct an element of $\natw$. Since the label $\true$ represents $\zero$, it needs zero inductive arguments and the variables $f$ and $g$ are irrelevant. We return $\zw$ as a result:
\[ e_t \defeq \lambda(f:\zero \to \natw)\lambda(g:\zero \to \natw), \; \zw \]
Analogously, if $a$ is $\false$, the type $B(a)$ becomes $\one$.
Since the label $\false$ represents the successor operator, it needs one inductive argument---the predecessor---which is represented by the variable $f : \one \to \natw$.
The value of the recursive call on the predecessor is represented by the variable $g : \one \to \natw$.
Thus, taking this value (namely $g(\ttt)$) and applying the successor operator twice thus yields the desired result:
\begin{align*}
 e_f \defeq & \; \lambda(f:\one \to \natw)\lambda(g:\one \to \natw), \\
  & \; \supp(\false, (\lambda (x:\one), \; \supp(\false, (\lambda (y : \one), \; g(\ttt)))))
\end{align*}
Putting this together, we thus have
\[ \dbl \defeq \rec\natw(\natw; e) \]
with $e$ as defined above.

The associated computation rule for the function $\rec{\wtype{x:A} B(x)}(E;e) : \prd{w : \wtype{x:A} B(x)} E(w)$ is as follows.
\begin{itemize}
\item For any $a : A$ and $f : B(a) \to \wtype{x:A} B(x)$ we have 
\begin{align*}
& \rec{\wtype{x:A} B(x)}(E;e;\supp(a,f)) \equiv \\ & e(a,f,\big(\lambda (b : B(a)), \; \rec{\wtype{x:A} B(x)}(E;f(b))\big))
\end{align*}
\end{itemize}
In other words, the function $\rec{\wtype{x:A} B(x)}(E;e)$ satisfies the recurrence $e$.

By the above computation rule, the function $\dbl$ behaves as expected:
\begin{align*}
\dbl(\zw) & \equiv \rec\natw(\natw; e; \supp(\true, \; \lambda(x : \zero), \rec\zero(\natw;x))) \\
& \equiv e(\true, \big(\lambda(x : \zero), \; \rec\zero(\natw;x)\big), \\
 & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \big(\lambda(x : \zero), \; \dbl(\rec\zero(\natw;x))\big)) \\
 & \equiv e_t(\big(\lambda(x : \zero), \; \rec\zero(\natw;x)\big), \big(\lambda(x : \zero), \; \dbl(\rec\zero(\natw;x))\big)) \\
 & \equiv \zw \\
 \\
\dbl(1^\mathbf{w}) & \equiv \rec\natw(\natw; e; \supp(\false, \; \lambda(x : \one), \zw)) \\
& \equiv e(\false, \big(\lambda(x : \one), \zw\big), \big(\lambda(x : \one), \; \dbl(\zw)\big)) \\
 & \equiv e_f(\big(\lambda(x : \one), \zw\big), \big(\lambda(x : \one), \; \dbl(\zw)\big)) \\
 & \equiv \supp(\false, (\lambda (x:\one), \; \supp(\false,(\lambda (y : \one), \; \dbl(\zw))))) \\
 & \equiv \supp(\false, (\lambda (x:\one), \; \supp(\false,(\lambda (y : \one), \; \zw)))) \\
 & \equiv 2^\mathbf{w}
\end{align*}
and so on.

\medskip

Just like for natural numbers, we can prove a uniqueness theorem for 
W-types:
\begin{thm}\label{thm:w-uniq}
Let $g,h : \prd{w:\wtype{x:A}B(x)} E(w)$ be two functions which satisfy the recurrence $e : \prd{a,f} (\prd{b : B(a)} E(f(b))) \to E(\supp(a,f))$ up to propositional equality, i.e., such that
\begin{align*}
\prd{a,f} \id{g(\supp(a,f))}{e(a,f,\lambda (b: B(a)), g(f(b)))} \\
\prd{a,f} \id{h(\supp(a,f))}{e(a,f,\lambda (b: B(a)), h(f(b)))}
\end{align*}
Then $g$ and $h$ are equal. 
\end{thm}

Like for natural numbers, we have the corresponding notions of $\w$-algebras, $\w$-homomorphisms, and h-initiality for W-types.
\begin{defn}
A \emph{$\w$-algebra} for a type $A : \type$ and type family $B : A \to \type$ is a type $C$ with a function $c : \prd{a:A} (B(a) \to C) \to C$. Thus,
\begin{align*}
\walg \; A \; B \defeq \sm {C : \type} \prd{a:A} (B(a) \to C) \to C
\end{align*}
\end{defn}

\begin{defn}
Fix $A : \type, B : A \to \type$ and any $\w$-algebras $(C,c),(D,d) : \walg \; A \; B$. A \emph{$\w$-homomorphism} between them is a function $h : C \to D$ such that $h(c(a,f)) = d(a,\lambda (b : B(a)), \; h(f(b)))$ for any $a : A$ and $f : B(a) \to C$. Thus,
\begin{align*}
\whom \; A \; B \; (C, c) \; (D,d) \defeq \sm {h : C \to D} \\ \prd{a:A}{f:B(a)\to C} \id{h(c(a,f))}{\lambda (b : B(a)), \; h(f(b))}
\end{align*}
\end{defn}

\begin{defn}
A $\w$-algebra $\chi_i$ is called \emph{h-initial} if for any other $\w$-algebra $\chi$, the type of $\w$-homomorphisms from $\chi_i$ to $\chi$ is contractible. Thus,
\begin{align*}
\ishinitw \; A \; B \; \chi_i \defeq \prd{\chi : \walg \; A \; B} \iscontr \; (\whom \; A \; B \; \chi_i\; \chi)
\end{align*}
\end{defn}

Similarly, we have the analogous theorem stating that W-types give rise to homotopy-initial algebras.
\begin{thm}\label{thm:w-hinit}
For any type $A : \type$ and type family $B : A \to \type$, the $\w$-algebra $(\wtype{x:A}{B(x)}, \supp)$ is h-initial.
\end{thm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Homotopy-inductive types}
\label{sec:htpy-inductive}

In the previous section we showed how to encode natural numbers as W-types, with 
\begin{align*}
\natw & \defeq \wtype{b:\two} \rec\two(\bbU;\zero;\one) \\
\zw & \defeq \supp(\true, (\lambda (x : \zero), \; \rec\zero(\natw;x))) \\
\sw & \defeq \lambda (n : \natw), \; \supp(\false, (\lambda (x : \one), \; n))
\end{align*}
We also showed how one can define a $\dbl$ function on $\natw$ using simple elimination. When it comes to dependent elimination, however, this encoding is no longer satisfactory: given $E : \natw \to \type$ and recurrences $e_z : E(\zw)$ and $e_s : \prd{n : \natw}{y : E(n)} E(\sw(n))$, we can only construct a dependent function $r(E,e_z,e_s) : \prd{n : \natw} E(n)$ satisfying the given recurrences \emph{propositionally}. This means that the computation rules for natural numbers, which give definitional equalities, cannot be derived from the rules for W-types in any obvious way.

This problem goes away if instead of the conventional inductive types we consider \emph{homotopy-inductive types}, where all computation rules are stated up to a path, i.e., the symbol $\equiv$ is replaced by $=$. For instance, the computation rule for the homotopy version of W-types $\mathsf{W^h}$ becomes:
\begin{itemize}
\item For any $a : A$ and $f : B(a) \to \wtypeh{x:A} B(x)$ we have 
\begin{align*}
& \rec{\wtypeh{x:A} B(x)}(E;\supp(a,f)) = \\ & e(a,f,\big(\lambda (b : B(a)), \; \rec{\wtypeh{x:A} B(x)}(E;f(b))\big))
\end{align*}
\end{itemize}

Homotopy-inductive types have an obvious disadvantage when it comes to computational properties---the behavior of any function constructed using the elimination principle can now only be characterized propositionally. However, there are numerous advantages to this approach as well---for instance, the notion of a homotopy-inductive type is now internal to the type theory. This means we can, e.g., form a type of all natural numbers objects and make assertions about it. In the case of W-types, we can characterize $\wtype{x:A} B(x)$ as any type endowed with a supremum function and a dependent eliminator satisfying the appropriate computation rule:
\begin{align*}
\w_d \; A \; B \defeq & \; \sm{W : \type} \\
                      & \; \sm{\supp : \prd {a} (B(a) \to W) \to W} \\
                      & \; \prd{E : W \to \type} \\
                      & \; \prd{e : \prd{a,f} (\prd{b : B(a)} E(f(b))) \to E(\supp(a,f))} \\
                      & \; \sm{\rec : \prd{w : W} E(w)} \\
                      & \; \prd{a,f} \rec{}(\supp(a,f)) = e(a,\lambda (b: B(a)), \rec{}(f(b)))
\end{align*}

\begin{thm}
For any $A : \type$ and $B : A \to \type$, the type $\w_d \; A \; B$ is a mere proposition.
\end{thm}

It turns out that there is an equivalent characterization of W-types using simple elimination, plus certain \emph{uniqueness} and \emph{coherence} laws. First we give the rule for simple elimination:

\begin{itemize}
\item When constructing a function from the the $W$-type $\wtypeh{x:A} B(x)$ into the type $C$, it suffices to give its value for $\supp(a,f)$, assuming we are given the values of all $f(b)$ with $b : B(a)$.
In other words, it suffices to construct a function
\begin{align*}
c : & \; \prd{a:A} (B(a) \to C) \to C
\end{align*}
\end{itemize}
The associated computation rule for $\rec{\wtypeh{x:A} B(x)}(C,c) : (\wtype{x:A} B(x)) \to C$ is as follows:
\begin{itemize}
\item For any $a : A$ and $f : B(a) \to \wtypeh{x:A} B(x)$ we have 
\begin{align*}
& \rec{\wtypeh{x:A} B(x)}(C;c;\supp(a,f)) = \\ & c(a,\lambda (b : B(a)), \; \rec{\wtypeh{x:A} B(x)}(C;c;f(b)))
\end{align*}
i.e., there is a witness $\beta(C;c;a;f)$ for the above equality.
\end{itemize}

Furthermore, we have the following uniqueness rule, saying that any two functions defined by the same recurrence are equal:
\begin{itemize}
\item Let $C : \type$ and $c : \prd{a:A} (B(a) \to C) \to C$ be given. Let $g,h : (\wtypeh{x:A} B(x)) \to C$ be two functions which satisfy the recurrence $c$ up to propositional equality, i.e., such that we have terms
\begin{align*}
\beta_g : \prd{a,f} \id{g(\supp(a,f))}{c(a,\lambda (b: B(a)), g(f(b)))} \\
\beta_h : \prd{a,f} \id{h(\supp(a,f))}{c(a,\lambda (b: B(a)), h(f(b)))}
\end{align*}
Then $g$ and $h$ are equal, i.e.\ there is an equality $\alpha(C;c;f;g,\beta_g;\beta_h) : g = h$.
\end{itemize}

We note that in the dependent case the uniqueness rule is derivable from dependent elimination (\autoref{thm:w-uniq}). When the eliminator is simple rather than dependent, this rule is no longer derivable---and in fact, the statement is not even true (exercise). We hence postulate it as an axiom, together with the following coherence law, which tells us how the proof of uniqueness behaves on canonical terms:
\begin{itemize}
\item
For any $a : A$ and $f : B(a) \to C$, the following diagram commutes propositionally:
\[\xymatrix{
\;\;\;\;\; g(\supp(x,f)) \ar_{\alpha(\supp(x,f))}[d] \ar^{\beta_g \;\;\;\;\;\;\;\;\;\;}[r] & c(a,\lambda (b: B(a)), g(f(b)))
\ar^{c(a,-)(\funext \; \lambda (b), \; \alpha(f(b)))}[d] \\
\;\;\;\;\; h(\supp(x,f)) \ar_{\beta_h \;\;\;\;\;\;\;\;\;\;}[r] & c(a,\lambda (b: B(a)), h(f(b))) \\
}\]
where $\alpha$ abbreviates the path $\alpha(C;c;f;g,\beta_g;\beta_h) : g = h$.
\end{itemize}

Putting all of this data together yields another characterization of $\wtype{x:A} B(x)$, as a type with a supremum function, satisfying simple elimination, computation, uniqueness, and coherence rules:
\begin{align*}
\w_s \; A \; B \defeq & \; \sm{W : \type} \\
                      & \; \sm{\supp : \prd {a} (B(a) \to W) \to W} \\
                      & \; \prd{C : \type} \\
                      & \; \prd{c : \prd{a} (B(a) \to C) \to C} \\
                      & \; \sm{\rec : W \to C} \\
                      & \; \sm{\beta : \prd{a,f} \rec{}(\supp(a,f)) = c(a,\lambda (b: B(a)), \rec{}(f(b)))} \\
                      & \; \prd{g : W \to C} \\
                      & \; \prd{h : W \to C} \\
                      & \; \prd{\beta_g : \prd{a,f} g(\supp(a,f)) = c(a,\lambda (b: B(a)), g(f(b)))} \\
                      & \; \prd{\beta_h : \prd{a,f} h(\supp(a,f)) = c(a,\lambda (b: B(a)), h(f(b)))} \\
                      & \; \sm{\alpha : \prd {w : W} g(w) = h(w)} \\
                      & \; \alpha(\supp(x,f)) \ct \beta_h = \beta_g \ct c(a,-)(\funext \; \lambda (b), \; \alpha(f(b)))
\end{align*}

\begin{thm}
For any $A : \type$ and $B : A \to \type$, the type $\w_s \; A \; B$ is a mere proposition.
\end{thm}

Finally, we have a third, very concise characterization of $\wtype{x:A} B(x)$ as an h-initial $\w$-algebra:
\begin{align*}
\w_h \; A \; B \defeq \sm{\chi : \walg \; A \; B} \ishinitw(\chi)
\end{align*}

\begin{thm}
For any $A : \type$ and $B : A \to \type$, the type $\w_h \; A \; B$ is a mere proposition.
\end{thm}

It turns out all three characterizations of W-types are in fact equivalent:
\begin{thm}
For any $A : \type$ and $B : A \to \type$, we have
\[ \w_d \; A \; B \simeq \w_s \; A \; B \simeq \w_h \; A \; B \]
\end{thm}

Thus, we have the following corrollary, which is an improvement over \autoref{thm:w-hinit}:

\begin{cor}
The types satisfying the formation, introduction, elimination, and propositional computation rules for W-types are precisely the homotopy-initial $\w$-algebras.
\end{cor}

Finally, as desired, it can be shown that homotopy-natural numbers can be encoded as homotopy-W-types:

\begin{thm}
The rules for natural numbers with propositional computation rules can be derived from the rules for W-types with propositional computation rules.
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The general syntax of inductive definitions}
\label{sec:strictly-positive}

So far, we have been discussing only particular inductive types: $\emptyt$, $\unit$, $\bool$, $\nat$, coproducts, products, $\Sigma$-types, $W$-types, etc.
However, an important aspect of type theory is the ability to define \emph{new} inductive types, rather than being restricted only to some particular fixed list of them.
In order to be able to do this, however, we need to know what sorts of ``inductive definitions'' are valid or reasonable.

To see that not everything which ``looks like an inductive definition'' makes sense, consider the following ``constructor'' of a type $C$:
\begin{itemize}
\item $g:(C\to \nat) \to C$.
\end{itemize}
What would the recursion principle of $C$ be?
It seems clear that it should require a type $P$ together with a function $h:(P\to \nat)\to P$, and allow us to construct a function $f:C\to P$.
But what would the computation rule for $f$ be?
Looking at other computation rules, we would expect something like ``$f(g(\alpha)) \jdeq h(f(\alpha))$'' for $\alpha:C\to\nat$, but ``$f(\alpha)$'' does not make sense --- there is no way to take a function $C\to\nat$ and ``apply'' a function $f:C\to P$ to it and obtain a function $P\to \nat$.
The induction principle of $C$ is even more problematic; it's not even clear how to write down the hypotheses.
(In fact, the situation is even worse than this: even if we ignore the absence of a computation rule and an induction principle, such ``inductive definitions'' are inconsistent.
See \autoref{ex:loop,ex:loop2}.)

This example suggests one restriction on inductive definitions: the domains of all the constructors must be \emph{covariant functors} of the type being defined.
In other words, if we replace all occurrences of the type being defined with a variable $X:\type$, then each domain of a constructor must be an expression that can be made into a covariant functor of $X$.
This is the case for all the examples we have considered so far.
For instance, with the constructor $\inl:A\to A+B$, the relevant functor is constant at $A$ (i.e.\ $X\mapsto A$), while for the constructor $\suc:\nat\to\nat$, the functor is the identity functor ($X\mapsto X$).

However, this necessary condition is also not sufficient.
Covariance prevents the inductive type from occurring on the left of a single function type, as in the ``constructor'' $g:(C\to\nat)\to C$ above, since this yields a contravariant functor rather than a covariant one.
However, since the composite of two contravariant functors is covariant, \emph{double} function types such as $((X\to \nat)\to \nat)$ are once again covariant.

\dots

\medskip

This counterexample suggests that we should ban an inductive type from ever appearing on the left of an arrow in the domain of its constructors, even if that appearance is nested in other arrows so as to eventually become covariant.
(Similarly, we also forbid it from appearing in the domain of a dependent function type.)
This restriction is called \textbf{strict positivity} (ordinary ``positivity'' being essentially covariance), and it turns out to suffice.

In conclusion, therefore, a valid inductive definition of a type $W$ consists of a list of \emph{constructors}.
Each constructor is assigned a type that is a function type taking some number (possibly zero) of inputs (possibly dependent on one another) and returning an element of $W$.
Finally, we allow $W$ itself to occur in the input types of its constructors, but only strictly positively.
This essentially means that each argument of a constructor is itself some iterated function type with codomain $W$.
For instance, the following is a valid constructor type:
\begin{equation}
  c:(A\to W) \to (B\to C \to W) \to D \to W \to W.\label{eq:example-constructor}
\end{equation}
All of these function types can also be dependent functions ($\Pi$-types).

Now once we have such an inductive definition, what can we do with it?
Firstly, there is a \emph{recursion principle} stating that for any type $P$ equipped with the same data given to $W$ by the constructors, we have an induced function $f:W\to P$.
For the example constructor~\eqref{eq:example-constructor}, we would require $P$ to be equipped with a function of type
\begin{equation}\label{eq:example-rechyp}
  d:(A\to P) \to (B\to C \to P) \to D \to P \to P
\end{equation}
and similarly for any other constructors of $W$.
Under these hypotheses, the recursion principle yields $f:W\to P$, which moreover ``preserves the constructor data'' in the evident way --- this is the computation rule, where we use covariance of the inputs.
For instance, in the example~\eqref{eq:example-constructor}, the computation rule says that for any $\alpha:A\to W$, $\beta:B\to C\to W$, $\delta:d$, and $\omega:W$, we have
\[ f(c(\alpha,\beta,\delta,\omega)) \jdeq d(f\circ \alpha, f\circ \beta, \delta, f(\omega)). \]
As we have before in particular cases, when defining a particular function $f$, we may write these rules with $\defeq$ as a way of specifying the data $d$ and say that $f$ is defined by them.

The \emph{induction principle} for a general inductive type $W$ is only a little more complicated.
Of course, we start with a type family $P:W\to\type$, which we require to be equipped with constructor data ``lying over'' the constructor data of $W$.
That means we transform the type of each constructor by replacing all occurrences of $W$ not by $P$, but by an element $w:W$ together with an element of $P(w)$.
Similarly, we replace a function argument such as $A\to W$ by a function $\alpha:A\to W$ together with another function of type $\prd{a:A} P(\alpha(a))$.

In the example of~\eqref{eq:example-constructor}, the corresponding hypothesis for the induction principle would require
\begin{multline*}
d : \dprd{\alpha:A\to W}\left(\dprd{a:A} P(\alpha(a))\right) \to
\dprd{\beta:B\to C\to W} \left(\dprd{b:B}{c:C} P(\beta(b,c))\right) \to\\
\dprd{\delta:D}
\dprd{\omega:W} P(\omega) \to
P(c(\alpha,\beta,\delta,\omega)).
\end{multline*}
The corresponding computation rule would be
\[ f(c(\alpha,\beta,\delta,\omega)) \jdeq
d(\alpha, f\circ\alpha, \beta, f\circ \beta, \delta, \omega, f(\omega)).
\]

We will not attempt to give a formal presentation of the grammar of a valid inductive definition and its resulting induction and recursion principles.
This is possible to do (indeed, it is necessary to do if implementing a computer proof assistant), but provides no additional insight.
With practice, one learns to automatically deduce the induction and recursion principles for any inductive definition, and to use them without having to think twice.


\section*{Exercises}

\begin{ex}
  Construct two functions on natural numbers which satisfy the same recurrence $(e_z, e_s)$ but are not definitionally equal.
\end{ex}

\begin{ex}\label{ex:bool}
Show that for any dependent type $E : \two \to \type$, the space of dependent functions $\prd{b : \two} E(b)$ is equivalent to the space of recurrences $ E(\true) \times E(\false)$. 
\end{ex}

\begin{ex}
  (Continuing \autoref{ex:bool}) Formulate a corresponding statement for natural numbers and show that it fails to hold. Why?
\end{ex}

\begin{ex}
  Show that if we assume simple instead of dependent elimination for W-types, the uniqueness property (analogue of Thm.~\ref{thm:w-uniq}) fails to hold.
\end{ex}

\begin{ex}\label{ex:loop}
  Suppose that in the ``inductive definition'' of the type $C$ at the beginning of \autoref{sec:strictly-positive}, we replace the type \nat by \emptyt.
  Using only the ``obvious'' recursion principle for such a definition, construct an element of \emptyt.
\end{ex}

\begin{ex}\label{ex:loop2}
  Similarly, derive a contradiction from an ``inductive type'' $D$ with one constructor $(D\to D) \to D$.
\end{ex}

% Local Variables:
% TeX-master: "main"
% End:

