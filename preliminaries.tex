\chapter{Type theory}
\label{cha:typetheory}

\section{Type theory versus set theory}
\label{sec:types-vs-sets}

Type theory in general, and homotopy type theory in particular, is (among other things) a foundational language for mathematics, i.e., an alternative to Zermelo-Fraenkel set theory.
However, it behaves differently from the latter in several important ways, which can take some getting used to.
Explaining these differences carefully requires us to be a little bit more formal here than we will be in the rest of the book.
As stated in the introduction, our goal is to write type theory \emph{informally}; but for a mathematician accustomed to set theory, a little bit more precision at the beginning can help avoid some common misconceptions and mistakes.

First of all, we note that a set-theoretic foundation has two ``layers'': first there is the deductive system of first-order logic, and then formulated inside this system there are the axioms of a particular theory (such as ZFC).
Thus, set theory is not only about sets, but rather about the interplay between sets (the objects of the second layer) and propositions (the objects of the first layer).

By contrast, type theory is its own deductive system: it need not be formulated inside any superstructure such as first-order logic.
In particular, rather than having two basic notions (sets and propositions), type theory has one basic notion, namely \emph{types}.
Propositions (statements which we can prove, disprove, assume, negate, and so on\footnote{Confusingly, it is also a common practice (dating back to Euclid) to use the word ``proposition'' synonomously with ``theorem''.
  We will confine ourselves to the logician's usage, according to which a \emph{proposition} is a statement \emph{susceptible to} proof, whereas a \emph{theorem} (or ``lemma'' or ``corollary'') is such a statement that \emph{has been} proven.
Thus ``$0=1$'' and its negation ``$\neg(0=1)$'' are both propositions, but only the latter is a theorem.}) are identified with particular types, via the correspondence shown in Table~\ref{tab:pov} on page~\pageref{tab:pov}.
Thus, the mathematical activity of \emph{proving a theorem} is identified with a special case of the mathematical activity of \emph{constructing an object}---in this case, an inhabitant of a type that represents a proposition.

This leads us to another difference between type theory and set theory, but to explain it we must say a little about deductive systems in general.
Informally, a deductive system is a collection of rules for deriving things called \emph{judgments}.
If we think of a deductive system as a formal game, then the judgments are the ``positions'' in the game which we reach by following the game rules.
We can also think of a deductive system as a sort of algebraic theory, in which case the judgments are the elements (like the elements of a group) and the deductive rules are the operations (like the group multiplication).
From a logical point of view, the judgments can be considered to be the ``external'' statements, living in the metatheory, as opposed to the ``internal'' statements of the theory itself.

In the deductive system of first-order logic (on which set theory is based), there is only one judgment: that a given proposition has a proof.
A rule of first-order logic such as ``from $A$ and $B$ infer $A\wedge B$'' is actually a rule of ``proof construction'' which says that given the judgments ``$A$ has a proof'' and ``$B$ has a proof'', we may deduce that ``$A\wedge B$ has a proof''.
Note that the judgment ``$A$ has a proof'' exists at a different level from the \emph{proposition} $A$ itself, which is an internal statement of the theory.
% In particular, we cannot manipulate it to construct propositions such as ``if $A$ has a proof, then $B$ does not have a proof''---unless we are using our set-theoretic foundation as a meta-theory with which to talk about some other axiomatic system.

The basic judgment of type theory, analogous to ``$A$ has a proof'', is written ``$a:A$'' and pronounced as ``the term $a$ has type $A$'', or more loosely ``$a$ is an element of $A$'' (or, in homotopy type theory, a ``point'' of $A$).
When $A$ is a type representing a proposition, then $a$ may be called a \emph{witness} to the provability of $A$, or \emph{evidence} of the truth of $A$.
In this case, the judgment $a:A$ is derivable in type theory (for some $a$) precisely when the analogous judgment ``$A$ has a proof'' is derivable in first-order logic (modulo differences in the axioms assumed and in the encoding of mathematics, as we will discuss throughout the book).
% (Already here type theory has an advantage in that the term $a$ records the essence of the proof of $A$, whereas in first-order logic that information is only available by backtracking through the derivation of ``$A$ has a proof''.)

On the other hand, if the type $A$ is being treated more like a set than like a proposition (although as we will see, the distinction can become blurry), then ``$a:A$'' may be regarded as analogous to the set-theoretic statement ``$a\in A$''.
However, there is an essential difference in that ``$a:A$'' is a \emph{judgment} whereas ``$a\in A$'' is a \emph{proposition}.
In particular, when working internally in type theory, we cannot make statements such as ``if $a:A$ then it is not the case that $b:B$'', nor can we ``disprove'' the judgment ``$a:A$''.

A good way to think about this is that in set theory, ``membership'' is a relation which may or may not hold between two pre-existing objects ``$a$'' and ``$A$'', while in type theory we cannot talk about an element ``$a$'' in isolation: every element \emph{by its very nature} is an element of some type, and that type is (generally speaking) uniquely determined.
Thus, when we say informally ``let $x$ be a natural number'', in set theory this is shorthand for ``let $x$ be a thing and assume that $x\in\nat$'', whereas in type theory ``let $x:\nat$'' is an atomic statement: we cannot introduce a variable without specifying its type.

At first glance, this may seem an uncomfortable restriction, but it is arguably closer to the intuitive mathematical meaning of ``let $x$ be a natural number''.
In practice, it seems that whenever we actually \emph{need} ``$a\in A$'' to be a proposition rather than a judgment, there is always an ambient set $B$ of which $a$ is known to be an element and $A$ is known to be a subset.
This situation is also easy to represent in type theory, by taking $a$ to be an element of the type $B$, and $A$ to be a predicate on $B$; see \autoref{subsec:prop-subsets}.

A last difference between type theory and set theory is the treatment of equality.
The familiar notion of equality in mathematics is a proposition: e.g.\ we can disprove an equality or assume an equality as a hypothesis.
Since in type theory, propositions are types, this means that equality is a type: for elements $a,b:A$ (that is, both $a:A$ and $b:A$) we have a type ``$\id[A]ab$''.
(In \emph{homotopy} type theory, of course, this equality proposition can behave in unfamiliar ways: see \autoref{sec:identity-types}, \autoref{cha:basics}, and the rest of the book).

However, in type theory there is also a need for an equality \emph{judgment}, existing at the same level as the judgment ``$x:A$''.
This is called \emph{judgmental equality} or \emph{definitional equality}, and we write it as $a\jdeq b$ or $a \jdeq_A b$.
It is helpful to think of this as meaning ``equal by definition''.
For instance, if we define a function $f:\nat\to\nat$ by the equation $f(x)=x^2$, then the expression $f(3)$ is equal to $3^2$ \emph{by definition}.
It does not make sense to negate or assume an equality-by-definition; we cannot say ``if $x$ is equal to $y$ by definition, then $z$ is not equal to $w$ by definition''.
Whether or not two expressions are equal by definition is just a matter of expanding out the definitions; in particular, it is algorithmically decidable.

As type theory becomes more complicated, judgmental equality can get more subtle than this, but it is a good intuition to start from.
Alternatively, if we regard a deductive system as an algebraic theory, then judgmental equality is simply the equality in that theory, analogous to the equality between elements of a group---the only potential for confusion is that there is \emph{also} an object \emph{inside} the deductive system of type theory (namely the type ``$a=b$'') which behaves internally as a notion of ``equality''.

The reason we \emph{want} a judgmental notion of equality is so that it can control the other form of judgment, ``$a:A$''.
For instance, suppose we have given a proof that $3^2=9$, i.e.\ we have derived the judgment $p:(3^2=9)$ for some term $p$.
Then the same proof $p$ ought to count as a proof that $f(3)=9$, since $f(3)$ is $3^2$ \emph{by definition}.
The best way to represent this is with a rule saying that given the judgments $a:A$ and $A\jdeq B$, we can derive the judgment $a:B$.

Thus, for us, type theory will be a deductive system based on two forms of judgment:
\begin{center}
\begin{tabular}{c|l}
  \textbf{Judgment} & \textbf{Meaning}\\\hline
  $a : A$ & $a$ is an object of type $A$.\\
  $a \jdeq_A b$ & $a$ and $b$ are definitionally equal objects of type $A$.
\end{tabular}
\end{center}
When introducing a definitional equality, i.e., defining one thing to equal another, we will use the symbol ``$\defeq$''.
Thus, the above definition of the function $f$ would be written as $f(x)\defeq x^2$.

Because judgments cannot be put together into more complicated statements, the symbols ``$:$'' and ``$\jdeq$'' bind more loosely than anything else.\footnote{In formalized type theory, commas and turnstiles can bind even more loosely.
  For instance, $x:A,y:B\vdash c:C$ is parsed as $((x:A),(y:B))\vdash (c:C)$.
  However, in this book we will not use any notation of this sort.}
Thus, for instance, ``$p:x=y$'' should be parsed as ``$p:(x=y)$'', which makes sense since ``$x=y$'' is a type, and not as ``$(p:x)=y$'', which is senseless since ``$p:x$'' is a judgment and cannot be equal to anything.
Similarly, ``$A\jdeq x=y$'' can only be parsed as ``$A\jdeq(x=y)$'', although in extreme cases such as this, one ought to add parentheses anyway to aid reading comprehension.
This is perhaps also an appropriate place to mention that the common mathematical notation ``$f:A\to B$'', expressing the fact that $f$ is a function from $A$ to $B$, can be regarded as a typing judgment, if we use ``$A\to B$'' as notation for the type of functions from $A$ to $B$ (which is standard practice in type theory; see \autoref{sec:pi-types}).

Judgements may depend on assumptions of the form $x:A$, where $x$ is a variable and $A$ is a type.
For example, we may construct an object $m + n : \Nat$ under the assumptions that $m,n : \Nat$.
Another example is that assuming $A$ is a type, $x,y : A$, and $p : x =_A y$, we may construct an element $p^{-1} : y =_A x$.
The collection of all such assumptions is called the \emph{context}; from a topological point of view it may be thought of as a ``parameter space''.

If the type $A$ in an assumption $x:A$ represents a proposition, then the assumption is a type-theoretic version of a \emph{hypothesis}: we assume that the proposition $A$ holds.
When types are regarded as propositions, we may omit the names of their proofs.
Thus, in the second example above we may instead say that assuming $x =_A y$, we can prove $y =_A x$.
However, since we are doing proof-relevant mathematics, we will frequently refer back to proofs as objects.
In the example, we may want to establish that $p^{-1}$ together with the proofs of transitivity and reflexivity behave like a groupoid; see \autoref{cha:basics}.

In the rest of this chapter, we attempt to give an informal presentation of Type Theory, sufficient for the purposes of this book; we will give a more formal account in an Appendix.
Aside from some fairly obvious rules (such as the fact that judgmentally equal things can always be substituted for each other), the rules of type theory can be grouped into \emph{type formers}.
Each type former consists of a way to construct types (possibly making use of already-constructed types), together with rules for the construction and behavior of elements of that type.
In most cases, these rules follow a fairly predictable pattern, but we will not attempt to make this precise.


\section{Universes}
\label{sec:universes}

A universe is a type whose elements are types. As in naive set theory
we may wish for a universe of all types $\UU_\infty$ including itself
$\UU_\infty : \UU_\infty$ but as in set
theory this is unsound, i.e. we can deduce that every type,
including the empty type representing the proposition False, is inhabited. Using a
representation of sets as trees we can directly encode Russell's
paradox \cite{thierry:trees} or alternatively avoiding the use of
inductive types to define trees we can encode the Burali-Forti paradox
(that the collection of ordinals cannot be an ordinal) this is due to
Girard \cite{girard:paradox}.

To avoid these paradoxes we introduce a hierarchy of universes
\[ \UU_0 : \UU_1 : \UU_2 : \dots \]
where every universe $\UU_i$ is an element of the next universe
$\UU_{i+1}$. Moreover we assume that our universes are
\emph{cumulative}, that is that all the elements of the $i$th
universe are also elements of the $i+1$th universe, i.e. if
$A:\UU_i$ then $A:\UU_{i+1}$
\footnote{This rule breaks the principle that every element
  has a uniquely determined type. This could be avoided by introducing
  an explicit lifting operator.}

We say that our type theory is predicative if types in a universe
$\UU_i$ are introduced only with reference to types in the same or
lower universes. Most (all ?) constructions introduced in this book
are predicative. An impredicative type is one which is obtained by
quantifying over all types at a certain level including itself. In
classical Mathematics we can quantify over all truth values which are
identified with the type of booleans. Doing the same in type theory
would involve quantifying over all types at the current level and is
hence impredicative. As a consequence assuming excluded middle which
identifies the small type of booleans with the universe, leads to an
impredicative theory.

When we say that $A$ is a type we mean that it inhabits some universe
$\UU_i$. We usually want to avoid to mention the level $i$ explicitely
and just assume that levels can be assigned in a consistent way. We
may write $A:\UU$ omitting the level. This way we can even write
$\UU:\UU$ which can be read as $\UU_i:\UU_{i+1}$ having left the
indizes implicit. 

In the sequel we shall write $T[x]$ for a term with a variable $x$ and
$T[T']$ for the same term where each occurence of $x$ has been
replaced by $T'$. Moreover we shall assume that this substitution
operation avoids the capture of bound variables, if necessary by
consistently renaming bound variables ($\alpha$-conversion).
As an example consider $B[x] \defeq \prd{y:A}x = y$, we don't want
$B[y]$ to be equal to $\prd{y:A}y = y$ because the variable $y$ we are
substitute doesn't refer to the bound variable. Instead we rename the
bound variable to avoid capture $B[y] \jdeq \prd{y':A}y = y'$.

A \textbf{family of types} over an indexing type $A:\UU$ and a 
type $B[x] : \UU$ given $x:A$. This can be generalized to a sequence of indexing
types $A_i[x_0,\dots x_{i-1}]$ each depending on the previous indexing
types and a family $B[x_0,\dots,x_n] : \UU$ depending on all of them. In the
propsoitions of types view families correspond to predicates,
assigning to each element of the indexing types a type corresponding
to the proposition that this particular instance of the predicate holds.

An example is the type $\Vect(A,n)$ of
vectors ($n$-tuples) of elements $a_i : A$ for $0 < i< n$ of type $A$:
$\seq{a_0,\dots,a_n} : \Vect(A,n)$ which is family indexed
over $n : \Nat$. Another example is the type $x =_A y$ which is a
family indexed over $x,y : A$. Indeed, xsboth examples are also
indexed by $A:\UU$.

\section{Dependent function types (\texorpdfstring{$\Pi$}{Π}-types)}
\label{sec:pi-types}

Functions are a primitive concept in type theory, i.e. they are not
viewed as a special case of relations. We assume functions to be
constructive that is given an element of the input type we can
effectively compute the result of the function applied to this
element. In type theory we introduce the general concept of a
dependent function type which allows the codomain to vary over the
domain of the function --- this corresponds to the infinite cartesian
product of types as the notation suggests. 

Given a type $A$
and a family $B[x]$ indexed over $x:A$ we can form the type
\[\prd{x:A}B(x)\] of dependent functions. Non-dependent functions arise
in the special case when $B$ is a constant family (i.e., $B$ does not
dependent on $x$) in which case we write $A \to B$.

Given a type $A$ an example of a dependent function of type
$\prd{n:\Nat}\Vect(A,n)$
is the function $f$ which constructs an $n$-tuple of $a$s, i.e. 
\[ f(a) \defeq \seq{\underbrace{a,a,\dots a}_{\mbox{$n$-times}}} \] % \vec{\underbrace{a}{$n$-times}}
 Another example
of a dependent function is the proof of reflexivity which has the type
$\prd{x:A}x = x$.

Given a dependent function $f : \prd{x:A}B[x]$ and $a : A$ we can
apply $f$ to $a$ which we write as $f(a) : B[a]$.  Such a function $f$ may be introduced by a defining equation
  \[ f(x)\defeq b[x]\mbox{ for } x:A,\]
where $b[x]$ is a term for an object of type $B[x]$ for $x:A$. So if $a$ is a term for an object of type $A$ we may substitute $a$ for $x$ in the defining equation to give a definitional equality
  \[ f(a)\jdeq b[a]:B[a]\]
and we have $f(a):B[a]$. 

If we want to avoid giving a name to a function we can use
$\lambda$-abstraction 
\[\lam{x:A}b[x] : \prd{x:A}B[x]\]
$\lambda$-abstractions induce the judgmental equality
$(\lam{x:A}b[x])(a) \jdeq b[a]$ ($\beta$-equality). Note that
we do not assume $\eta$-equality $\lam{x:A} f(x) \jdeq f$ (where
$x$ does not occur in $f$) but this is provable for propositional
equality using functional extensionality.

For example we may define a function $f : \Nat \to \Nat$ by defining $f(n)
\defeq n+2$ and then $f(2) \jdeq 2+2$. If we want to avoid giving the function a name we can use
$\lambda$-abstraction, i.e. we can write $\lam{n:\Nat}n+2 : \Nat
\to \Nat$. 

There are other means by which to define a function,
e.g. using the fact that $\Nat$ is inductively defined we can define
functions by primitive recursion which induces a number of
definitional equalities. In general we may use the
elimination principle associated to a particular type to define
functions - we will introduce these elimination principles with the
type they eliminate over.

Function in several arguments correspond to iterated function types,
i.e. a function $f$ with arguments in $A$ and $B$ and results in $C$ has
the type $f : \prd{x:A}\prd{y : B[x]}C[x,y]$ which can be also
written as $f : \prd{x:A,y:B[x]}C[x,y]$. Given an element $a:A$ we can apply
$f$ to it obtaining $f(a) : \prd{y:B[a]}C[a,y]$. Given also an element $b:B$ we
can continue applying $f$ obtaining $f(a)(b) : C[a,b]$ which we may also
write as $f(a,b):C[a,b]$. Using this device of \emph{currying} we can
represent $n$-ary functions without using product types. 
This device is commonly used in functional programming for
non-dependent functions. Consequently the symbol $\to$ is
right-associative,  $A \to B \to C \jdeq A \to (B \to C)$ so that we can avoid
brackets when writing the types of $n$-ary non-dependent functions.

When we view propositions as types the non-dependent function type
corresponds to implication, i.e. given types $P,Q$ a proof of $P \to
Q$ is a function $f$ assigning to each proof $p:P$ a proof $f(p) : Q$.

The dependent function type corresponds to universal
quantification: Given a type $A$ and a family $P[x]$ assuming $x:A$
which we view as a predicate over $A$ we can form $\prd{x:A}B[x]$
whose elements are proofs that $B[x]$ holds for all $x:A$ by assigning
to each $a:A$ a proof $f(a) : B[a]$.

We summarize that from the type-theoretic point of view the concepts
of an ordinary function, implication and universal quantification are
unified using the notion of a dependent function type or $\Pi$-type.

\section{Dependent pair types (\texorpdfstring{$\Sigma$}{Σ}-types)}
\label{sec:sigma-types}

As before in the case of functions the dependent pair type is a
generalisation of the ordinary cartesian product. At the same time 
the dependent pair type can be viewed as an infinite sum over the
indexing type - hence  the symbol $\Sigma$. Given a family $B[x]$
indexed over $x:A$ we can form the type $\sm{x:A}B[x]$. Elements of $\sm{x:A}B[x]$ are
pairs $\tup{a}{b} : \sm{x:A}B[x]$ where $a:A$ and $b:B[a]$. If the
family $B$ does not depend on $A$ we write $A \times B$.

What are the functions out of $\Sigma$-type? In type-theoretic
terminology: what is the elimination principle for $\Sigma$? We certainly need the projections:
\begin{eqnarray*}
  \fst & : & \sm{x : A}B[x] \to A \\
  \snd & : & \prd{p:\sm{x : A}B[x]}B[\fst(p)]
\end{eqnarray*}
with the defining equations 
\begin{eqnarray*}
  \fst(\tup{a}{b}) & \defeq & a \\
  \snd(\tup{a}{b}) & \defeq & b
\end{eqnarray*}
Note that the 2nd projection has a dependent function type using the
first projection. The projections are instances of a more general
scheme to define functions which is the first instance of an
elimination principle. The basic idea is to formulate an induction law
for $\Sigma$-types and then observe that it has a straightforward
computational interpretation.

We require a family of types $C[p]$ indexed by elements of the
$\Sigma$-type $p : \sm{x : A}B[x] \to A$ --- this is our induction
predicate. Now we assume that we can \emph{prove} $C$ for the canonical
elements of the $\Sigma$-types - the tuples. This corresponds to being
able to construct an element of the following type
\[ m:\prd{x:A}{y:B[x]}C[\tup{x}{y}] \]
Given $m$ we now claim that $C$ holds for \emph{all} elements of the
$\Sigma$-types which corresponds to being able to construct a function 
\[ f : \prd{p:\sm{x : A}B[x]}C[p]\]
with the defining equation
\[ f(\tup{a}{b}) \defeq m(a,b) \]
Intuitively, this elimination principle tells us that to define a
function over a $\Sigma$-type it is enough to define it for its
canonical elements of the form $\tup{a}{b}$. We leave it as an
exercise to show that the projections are particular instances of this
scheme. 

The reader may wonder wether it is enough to just have the
projections. Indeed, the elimination principle allows us to derive an
additional fact which we can formulate using identity types which are
introduced below: the principle of surjective pairing which can be
viewed as an $\eta$-rule for $\Sigma$-types:
\[ \spr : \prd{p : \sm{x:A} B[x]} \tup{\fst(p)}{\snd(p)} = p \]
To prove $sp$ we only need to show
\[ \tup{\fst(\tup{a}{b})}{\snd(\tup{a}{b})} = \tup{a}{b} \]
and using the defining equations for $\fst$ and $\snd$ this is
equivalent to
\[ \tup{a}{b} = \tup{a}{b} \]
which can be used using reflexivity, that is we define:
\[ \spr(\tup{a}{b}) \defeq \refl{\tup{a}{b}} \]

% However,
% If $C$ is a family of types on $p:\Sigma_{x:A}B(x)$ and 
% $c:\Pi(x:A)(y:B(x)),C((x,y))$, then we may define 
% $f:(\Pi z:\Sigma_{x:A}B(x))C(z)$ with defining equation
%   \[f((x,y))\defeq c(x)(y)\mbox{ for } x:A,y:B(x).\]

% As special cases we can derive the projections: to derive the 1st
% projection let $C_1(p) \defeq A$ be the constant family and $c_1 : \Pi (x:A)(y:B(x),A$
% defined as $c_1(x)(y) \defeq x$ to derive $\pi_1 : \Sigma_{x:A}B(x)) \to A$
% with the defining equation $\pi_1 (x,y) \defeq x$. To derive the 2nd
% projection we use $C_2(p) \defeq B (\pi_1(p))$ and $c_2 : \Pi
% (x:A)(y:B(x)),B(\pi_1(x,y))$ note that the codomain $B(\pi_1(x,y))$ is
% definitionally equal to $B(x)$ and hence we can use $c_2(x)(y) \defeq
% y$ to construct $\pi_2 : \Pi(p : \Sigma_{x:A}{B(x)}),B(\pi_1 p)$ with
% the defining equation $\pi_2(x,y) \defeq y$.

As an example consider the type $\sm{n:\Nat}\Vect(A,n)$ of tuples of
arbitrary length, its elements are of the form
$\tup{n}{\seq{a_0,\dots,a_{n-1}}}$ with $a_i : A$. Indeed this type is
equivalent to the type of lists or finite sequences over $A$. Another example is the type
$\sm{n:\Nat}n+n = n$ which expresses the (true) proposition that
there exists a natural number which is equal to its doubling. An
element of this type is the pair $\tup{0}{p}$ where $p: 0 = 0+0$ is a
proof that $0$ is equal to its doubling.

A more involved example is the type-theoretic axiom of choice. Indeed,
in this is a misnomer since this principle is actually provable in
type theory. Assume
there are types $A,B$ and a family $R(x,y)$ indexed over $x:A$ and
$y:B$. Then we can show that from assuming 
\[p : \prd{x:A}\sm{y :B}R(x,y)\] 
we can define 
\begin{eqnarray*}
a(p) & : & \sm{f : A \to B} \prd{x:A} R(x,(fx)) \\
a(p) & \defeq & (\lam{x:A} \fst(p(x)),\lam{x:A} \snd(p(x)))
\end{eqnarray*}

Using types to represent propositions we use the non-dependent
form of $\Sigma$ to represent conjunction, given types $P,Q$ the type
$P \times Q$ has as proofs pairs of proofs $p:P$ and $q:Q$ and hence
represents the statment that both $P$ and $Q$ hold. On the other hand
gven a family of types $P[x]$ indexed over $x:A$ representing a
predicate, then $\sm{x:A} P[x]$ represents existential quantification:
its elements are pairs of an element $a : A$ and a proof $p : P[a]$
that the property holds for this particular element.

As before for $\Pi$-types we note the versatility of $\Sigma$-types,
which generalize cartesian product, conjunction and existential
quantification. 

\section{Finite types}
\label{sec:finite-types}

A finite type can be presented by listing its elements. In homotopy
type theory there is only one finite type for each finite cardinal
since we finite types with the same number of elements are equivalent.
Indeed, we only need three finite types to generate all others: 
\begin{itemize}
\item the empty type $\emptyt$ with no elements,
\item the unit type $\unit$ with exactly one element $\star:\unit$,
\item and the type of booleans $\bool$ with exactly two elements
  $\btrue,\bfalse : \bool$
\end{itemize}
The elimination principles for all three types are straightforward:
\begin{itemize}
\item If $C[x]$ is a family on $x:\emptyt$ then we have $f:\prd{z:\emptyt}C[z]$ with no defining equation.
\item If $C[x]$ is a family on $x:\unit$ and $c:C[\star]$ then we have $f:\prd{z:\unit}C[z]$ with defining equation
  \[ f(\star)\defeq c.\]
\item If $C[x]$ is a family on $x:\bool$, $c_0:C[\bfalse]$ 
and $c_1:C[\btrue]$ then we have $f:\prd{z:\bool}C[z]$ with the
defining equations
\begin{eqnarray*}
f(\bfalse) & \defeq& c_0\\
f(\btrue) & \defeq & c_1
\end{eqnarray*}
\end{itemize}
Using $\Sigma$-types we can derive the coproduct (or disjoint union) $A+B$
of two types $A,B$ which in particular shows that we con construct all
finite types using $\emptyt$ and $S(X) = X+1$. We define
\[A + B \defeq \sm{x:\bool}F(x)\]
where $F : \bool \to \UU$ is
defined as $F(\bfalse) \defeq A$ and $F(\btrue)\defeq B$. The
injections can be defined as:
\begin{align*}
& \inl :  A \to A+B\\
& \inl(a) \defeq (\bfalse,a)\\
& \inr :  B \to A+B\\
& \inr(b) \defeq (\btrue,b)
\end{align*}
We can derive the elimination principle for $A+B$:
Given  $C[x]$ is a family on $x:A+A'$ and
\begin{eqnarray*}
c_\inl & : & \prd_{x:A}C[\inl(x)] \\
c_\inr & : & \prd{x':A'}C(\inr(x'))
\end{eqnarray*}
then we can define $f:\prd_{z:A+A'}C[z]$ with defining equations
\begin{eqnarray*}
f(\inl(x)) & \defeq& c_\inl(x)  \\
f(\inr(x) & \defeq & c_\inr(x)  
\end{eqnarray*}

The technique to derive $+$ from $\Sigma$ can also be used to derive
$\times$ form $\Pi$ namely given $A,B:\UU$ we can define 
\[ A \times B \defeq \prd{x:\bool}F(x)\] where $F$ is defined as for
$+$ above. The projections can be derived by applications to $\bfalse$
and $\btrue$. To derive the eliminator we need the principle of
functional extensionality. Hence there are two ways to derive $A\times
B$ : either as a non-dependent $\Sigma$-type or as a special case of
$\Pi$ where the domain is boolean.

The types $\emptyt$ and $\unit$ represent the logical constants
\emph{False} and \emph{True}. Indeed, \emph{False} has no proof while
\emph{True} has one trivial proof. Unlike in classical mathematics we
do not identify propositions with $\bool$ (instead we idnetify
propositions with types). Hence the type $A \to \bool$ does not
represent all subsets of $A$ but only the decidable ones.


\section{Identity types}
\label{sec:identity-types}

While the previous constructions can be seen as generalisations of
standard set theoretic constructions, the identity type seems to be
a particular feature of type theory. Moreover, homotopy type theory is
based on a reinterpretation of identity types embracing a
proof-relevant understanding of identity types: there can be more than
one reason that two objects are equal corresponding two the fact that
there can be more than one paths between two points.

Given a type $A$ and two elements $a,b:A$ we can form the type $a=_A
b$ in the same universe. The canonical way to prove an equality is
reflexivity $\refl{a} : a =_A a$ which proves that identical elements
are equal. In type theory objects in this type are intended to
represent proofs of the proposition that $a$ and $a'$ are identical.
So, in particular there is an object $\refl{a}: a =_A a'$ whenever
$a:A$. In Homotopy Type Theory, when $A$ is understood as a space and
$a,a'$ are understood as points of the space the type $Id_A(a,a')$ is
to be understood as the type of paths from the point $a$ to the point
$a'$.

There are two different but equivalent elimination principles for
identity types:
\begin{description}
\item[Martin-L\"{o}f Rule:] 
Given a family $C[x,y,p]$ indexed by $x,y:A$ and $p:x=_A y$ and 
\[c:\prd{x:A} C[x,x,\refl{x}]\]
we can derive a function $f:\prd{x,y:A,p:x = y}C[x,y,p]$ with the
defining equation 
\[ f(x,x,\refl{x}) \defeq c(x) \]
\item[Paulin-Mohring Rule:] 
Given $x:A$ and a family $D[x][y,q]$ indexed by $y:A,q:x=_A y$ and 
\[ d[x]:D[x][x,\refl{x}] \]
we can derive a function $g[x]:\prd{y:A,q:x = y}C[y,q]$ with the defining
equation 
\[ g[x](x,\refl{x}) \defeq d[x] \]
\end{description}
These principles arise from different views on the identity type,
while the Martin-L\"of rule is based on the view that the identity
type is a binary relation in the type-theoretic sense, i.e. a family
indexed by two variables, the Paulin-Mohring rule is based on viewing
the identity type over a given $x:A$, that is $x =_A -$ as a predicate
indexed only by one variable.

It is easy to see that the Martin-L\"of rule follows from the
Paulin-Mohring rule: Assume as given $C[x,y,p]$ and $c$ as in the
premise of the Martin-L\"of rule: now given $x:A$ we define $D[x][y,q] :=
C[x,y,q]$ and $d[x] \defeq c(x)$ we obtain $g[x]:\prd{y:A,q:a = y}C[x,y,q]$
with the defining equation $g[x](x,\refl{x}) \defeq c(x)$. Now
abstracting $x$ we obtain $f : \prd{x,y:A,p:x =_A y}C[x,y,p]$ with the
defining equation $f(x) \defeq g[x]$ and hence 
\begin{eqnarray*}
f(x,x,\refl{x}) & \jdeq & g[x](x,\refl{x})  \\
                   & \jdeq & c(x)
\end{eqnarray*}
as required.

The other direction is a bit more involved: We assume the Martin-L\"of
rule and derive two principles:
\begin{description}
\item[transport] Given a family $P[x]$ over
$x:A$ and $p : a =_A b, x : P[a]$ we obtain $\trans{p}{x} : P[b]$ with
the defining equation $\trans{\refl{a}}{x} \defeq x$.
\item[singleton is contractible]  Given $x:A$ we define 
\[ \sgl(x) := \Sigma y:A, x =_A y \]
and we show that 
\[\sctr[x] : \prd{p : \sgl(x)} (x,\refl{x}) =_{\sgl(x)}  p \]
with
\[ \sctr[x](\tup{x}{\refl{x}}) \defeq \refl{\tup{x}{\refl{x}}} \]
\end{description}
Now to derive the Paulin-Mohring rule assume $D[x][y,q]$ and
$d[x]:D[x][x,\refl{x}]$. We derive $E[x][p]$ indexed over $p:\sgl(x)$
with $E[x][p] \defeq D[x][\fst(x),\snd(x)]$. Using the equations for
the projections we also have that $d[x] : E[x][\tup{x}{\refl{x}}]$.
Now using transport along $\sctr$ we derive 
$h[x] : \prd{s : \sgl{x}}E[x][s]$ as 
\[h[x](s) \defeq \trans{\sctr(s)}{d[x]}\]
and now $g[x]:\prd{y:A,q:x = y}C[y,q]$ can be obtained as
$g[x](y,q) \defeq h[x](\tup{y}{q})$ with 
\begin{eqnarray*}
g[x](x , \refl{x}) & \jdeq & h[x](\tup{x}{\refl{x}} \\
& \jdeq & \trans{\sctr(\tup{x}{\refl{x})}}{d[x]}\\
& \jdeq & \trans{\refl{\tup{x}{\refl{x}}}}{d[x]}\\
& \jdeq & d[x].
\end{eqnarray*}

To derive transport from the Martin-L\"of rule for $P[x]$ over
$x:A$ and $C[x,y,q]\defeq P[x] \to P[y]$ and $c(x) : P[x] \to P[x]$ is
the identity function $c(x)(p) \defeq p$ we obtain 
$f : \prd{x,y:A}{q : x = y} P[x] \to P[y]$ with
$f (x,x,refl{x})(p) \defeq p$ which we denote as 
$\trans{q}{p} \defeq f(x,y,q)(p)$.

We are left to derive $\sctr$ from the Martin-L\"of rule:
We use $C[x,y,p] \defeq \tup{y}{p} =_{\sgl(x)} \tup{x}{\refl{x}}$
with $c(x) \defeq \refl{\tup{x}{\refl{x}}}$ and using the Martin-L\"of
  rule: 
\[f : \prd{x,y:A}{p:x = y} \tup{y}{p} =_{\sgl(x)} \tup{x}{\refl{x}}\]
 with $f(x,x,\refl{x}) \defeq \refl{\tup{x}{\refl{x}}}$. We now define
$\sctr[x](p) = f(x,\fst(p),\snd(p)$.

We are going to use the Paulin-Mohring rule to show that $=$ is an
equivalence relation. Assuming a type $A$ we show that there is a function
$p^{-1} : b = a$ given $p : a = b$ using the family $b = a$ and
the defining equation
\[ (\refl{a})^{-1} = \refl{a} \]
Similarily we can show transitivity: assume there is $p : a = b$ and
we want to derive $p \ct - : b = c \to a = b$ by
\[ p \ct \refl{b} \defeq p.\]
Since we already have the constructor $\refl{}$ we have shown that $=_A$
is an equivalence relation. 

\section{Inductive types}
\label{sec:inductive-types}

The paradigm example of an inductive type is the type $\nat$ of unary natural numbers.  These objects of type $\nat$ are generated from the natural number $0$ by repeatedly applying the successor operation.  So we say that there are two {\bf introduction rules} for $\nat$. 
  \[ 0:\nat\]
and
  \[ \mbox{ from $n:\nat$ infer $\suc(n):\nat$}.\]
The inductive character of $\nat$ is captured by the, so called, elimination and computation rules for $\nat$.  We prefer to call them the rules for defining a function on $\nat$ by primitive recursion. 

If $C$ is a family on $\nat$, $c_0:C(0)$ and $c_\suc(x,y):C(\suc(x))$ for $x:\nat,y:C(x)$ then we may introduce a function $f:\Pi_{x:\nat}C(x)$ by the following primitive recursion defining equations; one defining equation for each introduction rule.

  \[\left\{\begin{array}{rl}
f(0)\defeq& c_0, \mbox{ and}\\
f(\suc(x))\defeq& c_\suc(x,f(x))\mbox{ for } x:\nat
  \end{array}\right.\]
It is sometimes convenient to make explicit the dependence of $f$ on $c_0$ and $c_\suc$ by using the primitive recursive operator $\rec_\nat$.  So we have
  \[ \rec_\nat(x,c_0,c_\suc):C(x)\mbox{ for } x:\nat\]
with the defining equations


As an example we define the function $m + - : \nat \to \nat$ for any
$m : \nat$ using the constant family $\nat$ and the defining
equations:
\begin{eqnarray*}
  m + 0 & \defeq & m \\
  m + \suc(n) & \defeq & \suc(m + n)
\end{eqnarray*}
By abstracting $m$ using the rules for $\Pi$-types we can derive the
binary function $- + - : \nat \to (\nat \to \nat)$.

We use the same principle to prove properties by induction. While it
is trivial to see that $+$ is right neutral because $m + 0 \jdeq m$
we need to prove that $0 + m =_\Nat m$ is inhabited. We do this by
constructing a function $f : \Pi_{n : \nat} 0 + n =_\Nat n$. 
The family is $0 + n =_\Nat n$ over $n : \Nat$. In the case
for $0$ we have that $0 + 0 \jdeq 0$ hence we can define
\[ f(0) \defeq \refl{0} \]
For $\suc(n)$ we have that $0 + \suc(n) \jdeq \suc(0 +
n)$ hence we define 
\[ f(\suc(n)) \defeq \mapfunc{\suc}(f(n)). \]
In this case we can show that the type is propositional, that is that
all elements of the type are equal. In this situation we may omit any
explicit reference to proof objects and the type-theoretic proof does
not look different to a conventional one in predicate logic. 

\section{Propositions as types}
\label{sec:pat}

\textbf{TODO:} A discussion of how to speak type formers in English, e.g.\ ``for all $x:A$, something or other'' means ``$\prd{x:A}$ something or other'', and so on.
For now, we will use ``$A$ or $B$'' to mean $A+B$ and ``there exists $x:A$'' to mean $\sm{x:A}$, but perhaps we should mention that we will refine it further later on.
For instance, sometimes classical statements involving these connectives need to be interpreted differently (see \autoref{subsec:prop-subsets} and wherever we discuss $(-1)$-truncations).


% Local Variables:
% TeX-master: "main"
% End:
