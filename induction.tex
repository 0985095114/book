\newcommand{\zero}{\ensuremath{\mathbf{0}}\xspace}
\newcommand{\one}{\ensuremath{\mathbf{1}}\xspace}
\newcommand{\two}{\ensuremath{\mathbf{2}}\xspace}
\newcommand{\twoh}{\ensuremath{\mathbf{2^h}}\xspace}
\newcommand{\three}{\ensuremath{\mathbf{3}}\xspace}
%\newcommand{\nat}{\ensuremath{\mathbf{N}}\xspace}
\newcommand{\natw}{\ensuremath{\mathbf{N^w}}\xspace}
\newcommand{\nath}{\ensuremath{\mathbf{N^h}}\xspace}
\newcommand{\lst}{\ensuremath{\mathbf{List}}\xspace}
\newcommand{\true}{\ensuremath{\mathbf{true}}\xspace}
\newcommand{\false}{\ensuremath{\mathbf{false}}\xspace}
\newcommand{\supp}{\ensuremath{\mathbf{sup}}\xspace}
\newcommand{\z}{\ensuremath{0}\xspace}
\newcommand{\zw}{\ensuremath{0^\mathbf{w}}\xspace}
\newcommand{\wtype}[1]{\ensuremath{W}(#1),\xspace}
\newcommand{\wtypeh}[1]{\ensuremath{W^h}(#1),\xspace}
\newcommand{\ind}{\ensuremath{\mathbf{Ind \;}}\xspace}
\newcommand{\s}{\ensuremath{\mathbf{s}}\xspace}
\newcommand{\sw}{\ensuremath{\mathbf{s^w}}\xspace}
\newcommand{\alt}{\;|\;\;}
\newcommand{\disj}[2]{#1 + #2}
\newcommand{\der}{\vdash}
\newcommand{\dbl}{\ensuremath{\mathbf{double}}}

\chapter{Induction}
\label{cha:induction}

\section{Booleans and natural numbers}\footnote{This should probably go in an earlier chapter.}

An \emph{inductive type} can be intuitively understood as a type generated by a certain finite collection of constructors. To specify an inductive type formally,
we will use a schematic definition, where we list the name and type of each constructor separately, e.g.:
\[ \two : \type \defeq \ind \left\{ 
\begin{array}{ll}
\true : &\two \\
\false : &\two
\end{array}
\right. \]     
The above definition declares the type $\two$ of Booleans to be the inductive type generated by two constant constructors $\true$ and $\false$. We can similarly define the types $\zero$ (aka Empty, Void), $\one$ (aka Unit), $\three$, and so on, with 0, 1, 3, or more constructors respectively. In the case of $\one$, the single constructor will be denoted by $\ttt$.

Another extremely important inductive type is the type $\nat$ of natural numbers:  
\[  \nat : \type \defeq \ind \left\{ 
\begin{array}{ll}
\z : & \nat \\
\s : & \nat \to \nat
\end{array}
\right. \]
As expected, $\nat$ is generated by a constant constructor $\z$ for the natural number zero and a unary constructor $\s$ taking a natural number $n : \nat$ to its successor $\s(n) : \nat$. 

What can we do with such an inductive type? An intuitive understanding of an inductive type $A$ is that it \emph{behaves as if the only inhabitants of $A$ were the terms constructed solely by applying the constructors of $A$}; we refer to these particular terms as the \emph{canonical terms of $A$}. In an empty context,
each term of $A$ is canonical - for instance, any closed term of $\two$ is necessarily either $\true$ or $\false$. In the setting of nonempty contexts,  
interesting and often nontrivial behavior may occur, such as the one exhibited by identity types which behave much like paths in a space.

In the case of the type $\two$ of Booleans, assuming that each term is canonical simply means that each term $b : \two$ must be either $\true$ or $\false$. In particular, we have the principle of \emph{(dependent) elimination}:

\begin{itemize}
\item When proving a statement $E : \two \to \type$ about \emph{all} Booleans, it suffices to prove it for $\true$ and $\false$, i.e., give proofs
$ e_t : E(\true)$ and $e_f : E(\false)$.
\end{itemize}

Furthermore, the resulting proof $\rec_\two(E;e_t;e_f): \prd{b : \two}E(b)$ behaves as expected when applied to the constructors $\true$ and $\false$; this principle is expressed by the \emph{computation rules}:
\begin{itemize}
\item We have $\rec_\two(E;e_t;e_f;\true) \equiv e_t$.
\item We have $\rec_\two(E;e_t;e_f;\false) \equiv e_f$.
\end{itemize}

The rules for the type $\two$ of Booleans allow us to reason by \emph{case analysis}. Since neither of the two constructors takes any arguments, this is all we need for Booleans. However, for more complex types such as the type $\nat$ of natural numbers, true induction is often needed (hence the name \emph{inductive type}):

\begin{itemize}
\item When proving a statement $E : \nat \to \type$ about \emph{all} natural numbers, it suffices to prove it for $\z$ and for $\s(n)$, assuming it holds
for $n$. This entails giving the proofs $e_z : E(\z)$ and $e_s : \prd{n : \nat}{y : E(n)} E(\s(n))$.
\end{itemize}
The variable $y$ represents our inductive hypothesis. Like for Booleans, we also have the associated computation rules for the function $\rec_\nat(E;e_z;e_s) : \prd{x:\nat} E(x)$:
\begin{itemize}
\item We have $\rec_\nat(E;e_z;e_s;\z) \equiv e_z$.
\item For any $n : \nat$, we have $\rec_\nat(E;e_z;e_s;\s(n)) \equiv e_s(n;\rec_\nat(E;e_z;e_s;n))$.
\end{itemize}
The dependent function $\rec_\nat(E;e_z;e_s)$ can thus be understood as being defined recursively on the argument $x : \nat$, via the recurrences $e_z$ and $e_s$: When $x$ is zero, the function simply returns $e_z$. When $x$ is the successor of another natural number $n$, the result is obtained by taking the recurrence $e_s$ and plugging in the specific predecessor $n$ and the recursive call value $\rec_\nat(E;e_z;e_s;n)$.

As an example we look at how to define a function on natural numbers which doubles its argument. We wish to apply dependent elimination with the constant type family $E \defeq \lambda(x : \nat), \nat$ since the intended type of $\dbl$ is $\nat \to \nat$. We first need to supply the value of $\dbl(\z)$, which is easy: we put $e_z \defeq \z$. Next, to compute the value of $\dbl(\s(n))$ for a natural number $n$, we first compute the value of $\dbl(n)$ and then perform the successor operation twice. This is captured by the recurrence $e_s(n,y) \defeq \s(\s(y))$; the variable $y$ stands for the result of the recursive call $\dbl(n)$. Thus, we define
\[ \dbl \defeq \rec_\nat(\big(\lambda(x :\nat), \nat\big); \; \z; \;  \big(\lambda(n: \nat) \lambda (y:\nat), \s(\s(y))\big)) \]
This indeed has the correct computational behavior: for example, we have 
\begin{align*}
\dbl(\s(\s(\z))) & \equiv e_s(\s(\z), \dbl(\s(\z))) \\
                 & \equiv \s(\s(\dbl(\s(\z)))) \\
                 & \equiv \s(\s(e_s(\z,\dbl(\z)))) \\
                 & \equiv \s(\s(\s(\s(\dbl(\z))))) \\
                 & \equiv \s(\s(\s(\s(e_z)))) \\
                 & \equiv \s(\s(\s(\s(\z))))
\end{align*}
as desired.

There are many similar cases when one wants to use dependent elimination with a constant family term. This is referred to as \emph{simple elimination}, which corresponds to recursion - instead of proving a separate statement $P(n)$ for each natural number $n$ as done in the dependent case, we are recursively constructing a function into a simple type $C$. In this case, we omit the $\lambda$-binder for simplicity and write $\rec_\nat(C; \ldots)$ instead of $\rec_\nat(\lambda (n : \nat), C; \ldots)$

The dependent elimination principle is quite strong and allows us to prove a variety of interesting theorems. For example, by specifying the terms $e_z$ and $e_s$, we  uniquely determine how the recursor behaves on canonical terms, and thus on all natural numbers. If we now have another function which obeys the same recurrence, then our intuition suggests these two functions should be equal. It turns out this is indeed the case and we have the following \emph{uniqueness principle}:

\begin{thm}
Let $f,g : \prd{x:\nat} E(x)$ be two functions which satisfy the recurrences $e_z : E(\z)$ and $e_s : \prd{n : \nat}{y : E(n)} E(\s(n))$ up to propositional equality, i.e., such that
\begin{align*}
\id{f(\z)}{e_z} \\ 
\id{g(\z)}{e_z}
\end{align*}
and 
\begin{align*}
\prd{n : \nat} \id{f(\s(n))}{e_s(n, f(n))} \\
\prd{n : \nat} \id{g(\s(n))}{e_s(n, g(n))}
\end{align*}
Then $f$ and $g$ are equal, i.e., we have $\alpha : \id[\prd{x :\nat} E(x)]{f}{g}$. 
\end{thm}

\begin{proof}
We use dependent elimination with the type $E(x) \defeq \id{f(x)}{g(x)}$. For the base case, we have \[f(\z) = e_z = g(\z)\]
For the inductive case, assume $n : \nat$ such that $f(n) = g(n)$. Then
\[ f(\s(n)) = e_s(n, f(n)) = e_s(n, g(n)) = g(\s(n)) \]
The first and last equality follow from the assumptions on $f$ and $g$. The middle equality follows from the inductive hypothesis and the fact that application preserves equality. This gives us pointwise equality between $f$ and $g$; invoking function extensionality finishes the proof.
\end{proof}
We note that the function $f$ is only required to satisfy the recurrences \emph{up to propositional equality}. The theorem itself only asserts propositional equality between functions - indeed, it is possible to construct functions which satisfy the same recurrence but are not definitionally equal (exercise). It is also possible to have a function which satisfies more than one set of recurrences (exercise).

Similar uniqueness theorems can generally be formulated and shown for other inductive types as well. In the next section, we show how to use the uniqueness property together with univalence to prove that natural numbers are completely characterized by their introduction, elimination, and computation rules.

%\begin{thm}\label{thm:uniq-types}
%Let $C$ be any other type satisfying the introduction, elimination, and computation rules for natural numbers, i.e., coming with terms 
%\begin{align*}
%c_z & : C \\
%c_s & : C \to C
%\end{align*}
%such that given $E : C \to \type$, $e_z : E(c_z)$, and $e_s : \prd{c : C}{y : E(c)} E(c_s(c))$ the following hold:
%\begin{itemize}
%\item We have a function $\rec_C(E;e_z;e_s) : \prd{c:C} E(c)$.
%\item We have $\rec_C(E;e_z;e_s,c_z) \equiv e_z$.
%\item For any $c : C$, we have $\rec_C(E;e_z;e_s,c_s(e)) \equiv e_s(c,\rec_C(E;e_z;e_s;c))$.
%\end{itemize}
%Then $C$ is equal to $\nat$, i.e., we have $\id{C}{\nat}$.
%\end{thm}
%\begin{proof}
%TODO
%\end{proof}

We conclude this section with a categorical observation. It turns out that inductive types give rise to \emph{homotopy-initial algebras}, in the following sense. Taking Booleans as the simplest example, we define:

\begin{defn}
A \emph{$\two$-algebra} is a type $C$ with two terms $c_0, c_1 : C$. Thus,
\begin{align*}
\mathtt{2Alg} \defeq \sm {C : \type} C \times C
\end{align*}
\end{defn}

\begin{defn}
Fix any $\two$-algebras $(C,c_0,c_1)$ and $(D,d_0,d_1)$. A \emph{$\two$-homo-morphism} between them is a function $h : C \to D$ mapping $c_0$ to $d_0$ and
$c_1$ to $d_1$ (up to propositional equality). Thus,
\begin{align*}
\mathtt{2Hom \; (C,c_0,c_1) \; (D,d_0,d_1)} \defeq \sm {h : C \to D} \; \id{h(c_0)}{d_0} \times \id{h(c_1)}{d_1}
\end{align*}
\end{defn}

\begin{defn}
A $\two$-algebra $\chi^*$ is called \emph{homotopy-initial} if for any other $\two$-algebra $\chi$, the type of $\two$-homomorphisms from $\chi^*$ to $\chi$ is contractible. Thus,
\begin{align*}
\mathtt{is\_hinitial \; \chi^* \defeq \fall{\chi : 2Alg} \; is\_contr \; (2Hom \; \chi^*\; \chi)}
\end{align*}
\end{defn}

As expected, homotopy-initial algebras are unique:
\begin{thm}
Any two homotopy-initial $\two$-algebras $\chi^*_1$ and $\chi^*_2$ are equal, i.e., we have $\id[\mathtt{2Alg}]{\chi_1^*}{\chi_2^*}$.
\end{thm}


We now have the following theorem; the proof is only sketched as it involves a large amount of technical detail.
\begin{thm}
The $\two$-algebra $(\two, \true, \false)$ is homotopy initial.
\end{thm}
\begin{proof}
(Rough sketch) Fix an arbitrary $\two$-algebra $(C,c_0,c_1)$. Using elimination on $\two$ it is easy to construct a $\two$-homomorphism into $C$. This will be our
center of contraction. To show that any other homomorphism is equal to the chosen one, we appeal to the uniqueness theorem for $\two$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Appetizer for univalence}

\newcommand{\natp}{\ensuremath{\mathbf{N'}}\xspace}
\newcommand{\zp}{\ensuremath{0'}\xspace}
\newcommand{\Sp}{\ensuremath{\mathbf{s'}}\xspace}
\newcommand{\dblp}{\ensuremath{\mathbf{double'}}}

Assume one has another definition of the natural numbers, for example:  
\[  \natp : \type \defeq \ind \left\{ 
\begin{array}{ll}
\zp : & \natp \\
\Sp : & \natp \to \natp
\end{array}
\right. \]
Also here, when proving a statement $E : \natp \to \type$ for all these new natural numbers, it suffices to give the proofs $e_z : E(\zp)$ and $e_s : \prd{n : \natp}{y : E(n)} E(\Sp(n))$. And again, the function $\rec_\natp(E;e_z;e_s) : \prd{x:\natp} E(x)$
has the following computation rules:
\begin{itemize}
\item We have $\rec_\natp(E;e_z;e_s;\zp) \equiv e_z$.
\item For any $n : \natp$, we have $\rec_\natp(E;e_z;e_s;\Sp(n)) \equiv e_s(n;\rec_\natp(E;e_z;e_s;n))$.
\end{itemize}
What is the relation between $\nat$ and $\natp$?
This is not just an academic question, since the structure of the natural numbers
can be found in lists over a type with one element (arguably the oldest appearance,
on the wall of the cave), in the non-negative integers, as substructure of the
rationals and the reals, and so on. One desideratum is certainly that it is easy to
transfer results from one appearance of the natural numbers to any other.

Recall the example of the function $\dbl$ defined above. A similar function
for our new natural numbers is readily defined by duplication and adding primes:
\[ \dblp \defeq \rec_\natp(\lambda(x :\natp), \natp; \; \zp; \;  \lambda(n: \natp) \lambda (y:\natp), \Sp(\Sp(y))) \]
Simple as this may seem, the drawback is that this leads to a
proliferation of duplicates. Not only functions have to be
duplicated, but also all lemmas and their proofs. For example,
an easy result such as  $\prd{n : \nat} \dbl(\s(n))=\s(\s(\dbl(n)))$ as well
as its proof by induction has to be `primed' (duplicated and adding primes).
This becomes totally impractical when formalizing serious mathematics.
In contrast, informal mathematics just proclaims that $\nat$ and $\natp$ are
equal and can be substituted for each other whenever the need arises.

As a first step to improve this situation, we observe that $\nat$ and $\natp$ are
\emph{isomorphic} via the following definable maps:
\begin{itemize}
\item $f \defeq \rec_\nat(\lambda(x :\nat), \nat; \; \zp; \;  \lambda(n: \nat), \Sp)
       : \nat \to\natp$, 
\item $g \defeq \rec_\natp(\lambda(x :\natp), \natp; \; \z; \;  \lambda(n: \natp), \s)
       : \natp \to\nat$.
\end{itemize}
By induction one proves that $\prd{n : \nat} \id{g(f(n))}{n}$ 
and $\prd{n : \natp} \id{f(g(n))}{n}$. 
[TOO TECHNICAL?

 : A formal proof of the first would read
\[\rec_\nat(\prd{n : \nat} \id{g(f(n))}{n};e_z;e_s)
: \prd{n : \nat} \id{g(f(n))}{n}
\] 
with  $e_z \defeq \refl{\z}$ and
$e_s \defeq \lambda(n: \nat)\lambda(y: \nat),\mathit{congr\s}\,(g(f(n))\, n\,y$ 
and
\[\mathit{congr\s} : \prd{m,n : \nat} \id{m}{n} \to\id{\s(m)}{\s(n)}\]
to be proved by path induction on $\id{m}{n}$.
END TOO TECHNICAL?]

The isomorphy of $\nat$ and $\natp$ makes it possible to define $\dblp$ from $\dbl$
without duplication, by using the isomorphisms $f$ and $g$ above:

\[ \dblp \defeq \lambda(x :\natp), f(\dbl(g(n))) \]

This is an improvement over `priming' but the easy result
$\prd{n : \natp} \dblp(\Sp(n))=\Sp(\Sp(\dblp(n)))$ still has to be reproved
by induction on $n:\natp$ using lots of $f$'s and $g$'s.
There is no easy way to obtain the proof for $\dblp$
by applying a function to the proof for $\dbl$.

The fundamental reason that it is still difficult to transfer results
from $\nat$ to $\natp$ is that they are only isomorphic and
not equal. True equality of $\nat$ and $\natp$ would give that their respective zeros are the same, their successor functions are the same, and that $\dbl:\natp\to\natp$, that is, $\dbl$ would be a function on $\natp$ as well. 
The need for transferring results would vanish, like in informal mathematics.

As a second step we observe that in proving $\prd{n : \nat} \id{g(f(n))}{n}$ 
and $\prd{n : \natp} \id{f(g(n))}{n}$ we have actually proved that
$g\circ{}f = \idfunc[\nat]$ and $f\circ{}g = \idfunc[\natp]$,
which implies that $\nat$ and $\natp$ are \emph{equivalent} 
in the sense of homotopy type theory.
The Univalence Axiom would give here that $\nat$ and $\natp$ are \emph{equal}
in the sense of homotopy type theory.
As a consequence one achieves almost the same ease of transferring results
as in informal mathematics.

OLD PART\footnote{Most is probably not necessary but we should state that this result applies to other inductive types too.}: Analogous theorems can likewise be shown for other inductive types.

We note that the proof of \ref{thm:uniq-types} uses univalence in a crucial way in order to go from equivalence to true equality. Using identity elimination on the equality $\id{C}{\nat}$ then shows that for any property $P : \bbU \to \type$ of types, $P(\nat)$ is inhabited iff $P(C)$ is. In other words, whatever can be shown for $\nat$ can be also shown for $C$ and vice versa. This is just one of the many results illustrating the power of the univalence axiom.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Disjoint unions and dependent sums}
A \emph{disjoint union} of two types $A$ and $B$ behaves much like a disjunction: we can choose to supply either a term of type $A$ or a term of type $B$. Schematically, we declare this as follows:
\[ \disj{A}{B} : \type \defeq \ind \left\{
\begin{array}{ll}
\inl : & A \to (\disj{A}{B}) \\
\inr : & B \to (\disj{A}{B})
\end{array}
\right. \]

In this definition, $A$ and $B$ act as \emph{parameters}, i.e., they denote arbitrary types. In other words, the type constructor $+$ is really a function taking the types $A$ and $B$ as an argument and returning a new inductive type $\disj{A}{B}$. Although omitted from the above schema for clarity, the constructors $\inl$ and $\inr$ likewise take the types $A$ and $B$ as the first 2 arguments, followed by a term of type $A$ (for \inl) or a term of type $B$ (for \inr).

Elimination for a disjoint union amounts to case analysis:
\begin{itemize}
\item When proving a statement $E : (\disj{A}{B}) \to \type$ about \emph{all} terms of the disjoint union $\disj{A}{B}$, it suffices to prove it for $\inl(a)$ and $\inr(b)$, i.e., give proofs $e_l : \prd{a:A} E(\inl(a))$ and $e_r : \prd{b:B} E(\inr(b))$.
\end{itemize}
The associated computation rules for the function $\rec_{\disj{A}{B}}(E;e_l;e_r) : \prd{x:\disj{A}{B}} E(x)$ are as expected:
\begin{itemize}
\item For each $a : A$, we have $\rec_{\disj{A}{B}}(E;e_l;e_r;\inl(a)) \equiv e_l(a)$.
\item For each $b : B$, we have $\rec_{\disj{A}{B}}(E;e_l;e_r;\inr(b)) \equiv e_r(b)$.
\end{itemize}

A \emph{product} of two types $A$ and $B$ (denoted by $A \times B$) is the type of pairs $(a,b)$, where $a : A$ and $b : B$. A \emph{dependent sum} is a generalization of this concept, where we allow the type $B$ to depend on $A$. A simple example is the type of pairs $(n,(c_1,\ldots,c_n))$, where the first component is a natural number $n : \nat$ and the second component is a vector $(c_1,\ldots,c_n) : \mathbf{Vec}_C(n)$ of length $n$ over another type $C : \type$.
Given $A : \type$ and $B : A \to \type$, the dependent sum of $A$ and $B$ is given schematically as
\[ \sm{a:A} B(a) : \type \defeq \ind \left\{
\begin{array}{ll}
\mathbf{pair} : & \prd{a:A}{b:B(a)} \\
& \sm{a:A} B(a)
\end{array}
\right. \]
Thus, the parameterized inductive type $\sm{a:A} B(a)$ has a single constructor $\mathbf{pair}$. Its first two arguments (not shown) are the parameters $A$ and $B$; the remaining two arguments are the respective components $a : A$ and $b : B(a)$. We will often denote $\mathbf{pair}(a,b)$ simply by $(a,b)$.

The elimination and computation rules are very simple:

\begin{itemize}
\item When proving a statement $E : \big(\sm{a:A} B(a)\big) \to \type$ about \emph{all} terms of the dependent sum $\sm{a:A} B(a)$, it suffices to prove it for a pair $(a,b)$, i.e., give a proof $e : \prd{a:A}{b:B(a)} E((a,b))$.
\end{itemize}

\begin{itemize}
\item For any $a : A$ and $b : B(a)$, we have $\rec_{\sm{a:A} B(a)}(E;e;(a,b)) \equiv e(a,b)$.
\end{itemize}
Using the elimination operator, it is very easy to construct the well-know projection functions $\pi_1$ and $\pi_2$, extracting the first resp. the second component of a pair.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{W-types}
Martin-L{\"o}f's W-types, also known as the types of well-founded trees, are a generalization of such types as natural numbers, lists, and binary trees. A particular W-type is specified by giving two parameters $A : \type$ and $B : A \to \type$, written $\wtype{a:A} B(a)$.

The type $A$ represents the type of \emph{labels} for $\wtype{a :A} B(a)$, which function as constructors (however, we reserve that word for use in schematic definitions of inductive types). For instance, when defining natural numbers as a W-type, the type $A$ would be the type $\two$ inhabited by the two terms $\true$ and $\false$, since there are precisely two ways how to obtain a natural number - either it will be zero or a successor of another natural number. 

The type family $B : A \to \type$ is used to record the arity of labels: a label $a : A$ will take $B(a)$-many inductive arguments. These arguments are represented by a function $f : B(a) \to \wtype{a :A} B(a)$, with the understanding that for any $b : B(a)$, $f(b)$ is the $b$-th argument to the label $a$. The W-type $\wtype{a :A} B(a)$ can thus be thought of as the type of well-founded trees, where nodes are labeled by terms of $A$ and each node labeled by $a : A$ has $B(a)$-many branches.

In the case of natural numbers, the label $\true $ has arity 0, since it constructs the constant zero; the label $\false$ has arity 1, since it constructs the successor of its argument. We can capture this by using simple elimination on $\two$ to define a function $\rec_\two(\bbU;\zero;\one)$ into a universe of types; this function returns the empty type $\zero$ for $\true$ and the unit type $\one$ for $\false$. We can thus define
\[ \natw \defeq \wtype{b:\two} \rec_\two(\bbU;\zero;\one) \]
where the superscript \textbf{w} serves to distinguish this version of natural numbers from the previously used one.
Similarly, we can define the type of lists over $A$ as a W-type with $\disj{\one}{A}$ many labels: one nullary label for the empty list, plus one unary label for each $a : A$, corresponding to appending the term $a$ to the head of a list:

\[ \lst \; A \defeq \wtype{x:\disj{\one}{A}} \rec_{\disj{\one}{A}}(\bbU; \; \zero; \; \lambda{(a:A)}, \one) \]
Schematically, we define a general W-type as follows:
\[ \wtype{x:A} B(x) : \type \defeq \ind \left\{
\begin{array}{ll}
\supp : & \prd{a:A}{f : B(a) \to \\ & \wtype{a:A} B(a)}  \wtype{x:A} B(x)
\end{array}
\right. \]

The constructor $\supp$ (short for supremum) takes a label $a : A$ and a function $f : B(a) \to \wtype{a:A} B(a)$ representing the arguments to $a$, and constructs a new term of $\wtype{x:A} B(x)$. Using our previous encoding of natural numbers as W-types, we can for instance define
\begin{align*}
\zw \defeq \supp(\true, \; \lambda(x : \zero), \rec_\zero(\natw;x))
\end{align*}
In other words, to construct $\zw$ we use the label $\true$. Now $\rec_\two(\bbU;\zero;\one; \true)$ evaluates to $\zero$, as it should since $\true$ is a nullary label. Thus, we need to construct a function $f : \zero \to \natw$, which represents the (zero) arguments supplied to $\true$. This is of course trivial, using simple elimination on $\zero$ as shown. Similarly, we can define
\begin{align*}
1^\mathbf{w} \defeq \supp(\false, \; \lambda(x : \one), 0^\mathbf{w}) \\
2^\mathbf{w} \defeq \supp(\false, \; \lambda(x : \one), 1^\mathbf{w})
\end{align*}
and so on.

We have the following elimination rule for W types:
\begin{itemize}
\item When proving a statement $E : \big(\wtype{a:A} B(a)\big) \to \type$ about \emph{all} terms of the $W$-type $\wtype{a:A} B(a)$, it suffices to prove it for $\supp(a,f)$, assuming it holds for all $f(b)$ with $b : B(a)$. 
In other words, it suffices to give a proof 
\begin{align*}
e : & \; \prd{a:A}{f : B(a) \to \wtype{a:A} B(a)}\\ & \; \prd{g : \prd{b : B(a)} E(f(b))} E(\supp(a,f))
\end{align*}
\end{itemize}

The variable $g$ represents our inductive hypothesis, namely that all arguments of $a$ satisfy $E$. To state this, we quantify over all terms of type $B(a)$, since each $b : B(a)$ corresponds to one argument $f(b)$ of $a$.

How would we define the function $\dbl$ on natural numbers encoded as a W-type? We would like to use the (simple) elimination on $\natw$ into the type $\natw$ itself. We thus need to construct a suitable term 
\[e : \prd{a : \two}{f : B(a) \to \natw}{g : B(a) \to \natw} \nat\]
which will represent the recurrence for the $\dbl$ function; for simplicity we denote the type $\rec_\two(\bbU;\zero;\one)$ by $B$.

Clearly $e$ will be a function taking $a : \two$ as its first argument. The next step is to perform case analysis on $a$ and proceed based on whether it is $\true$ or $\false$. This suggests the following form
\[ e \defeq \lambda(a : \two), \; \rec_\two(C,e_t,e_f,a) \]
where \[C \defeq \prd{f : B(a) \to \natw}{g : B(a) \to \natw} \natw\]
If $a$ is $\true$, the type $B(a)$ becomes $\zero$. Thus, given $f : \zero \to \natw$ and $g : \zero \to \natw$, we want to construct a term of type $\natw$. Since the label $\true$ represents $\zero$, it needs zero inductive arguments and the variables $f$ and $g$ are irrelevant. We return $\zero$ as a result:
\[ e_t \defeq \lambda(f:\zero \to \natw)\lambda(g:\zero \to \natw), \; \zw \]
Analogously, if $a$ is $\false$, the type $B(a)$ becomes $\one$. Since the label $\false$ represents the successor operator, it needs one inductive argument - the predecessor - which is represented by the variable $f : \one \to \natw$. The value of the recursive call on the predecessor is represented by the variable $g : \one \to \natw$. Taking this value - i.e., the term $g(\ttt)$ - and applying the successor operator twice thus yields the desired result:
\begin{align*}
 e_f \defeq & \; \lambda(f:\one \to \natw)\lambda(g:\one \to \natw), \\
  & \; \supp(\false, (\lambda (x:\one), \; \supp(\false, (\lambda (y : \one), \; g(\ttt)))))
\end{align*}
Putting this together, we thus have
\[ \dbl \defeq \rec_\natw(\natw; e) \]
with $e$ as defined above.

The computation rule for the function $\rec_{\wtype{a:A} B(a)}(E;e) : \prd{w : \wtype{a:A} B(a)} E(w)$ is as follows.
\begin{itemize}
\item For any $x : A$ and $f : B(x) \to \wtype{a:A} B(a)$ we have 
\begin{align*}
& \rec_{\wtype{a:A} B(a)}(E;\supp(x,f)) \equiv \\ & e(x,f,\big(\lambda (b : B(x)), \; \rec_{\wtype{a:A} B(a)}(E;f(b))\big))
\end{align*}
\end{itemize}
In other words, the function $\rec_{\wtype{a:A} B(a)}(E;e)$ satisfies the recurrence $e$.

By the above computation rule, the function $\dbl$ behaves as expected:
\begin{align*}
\dbl(\zw) & \equiv \rec_\natw(\natw; e; \supp(\true, \; \lambda(x : \zero), \rec_\zero(\natw;x))) \\
& \equiv e(\true, \big(\lambda(x : \zero), \; \rec_\zero(\natw;x)\big), \\
 & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \big(\lambda(x : \zero), \; \dbl(\rec_\zero(\natw;x))\big)) \\
 & \equiv e_t(\big(\lambda(x : \zero), \; \rec_\zero(\natw;x)\big), \big(\lambda(x : \zero), \; \dbl(\rec_\zero(\natw;x))\big)) \\
 & \equiv \zw \\
 \\
\dbl(1^\mathbf{w}) & \equiv \rec_\natw(\natw; e; \supp(\false, \; \lambda(x : \one), \zw)) \\
& \equiv e(\false, \big(\lambda(x : \one), \zw\big), \big(\lambda(x : \one), \; \dbl(\zw)\big)) \\
 & \equiv e_f(\big(\lambda(x : \one), \zw\big), \big(\lambda(x : \one), \; \dbl(\zw)\big)) \\
 & \equiv \supp(\false, (\lambda (x:\one), \; \supp(\false,(\lambda (y : \one), \; \dbl(\zw))))) \\
 & \equiv \supp(\false, (\lambda (x:\one), \; \supp(\false,(\lambda (y : \one), \; \zw)))) \\
 & \equiv 2^\mathbf{w}
\end{align*}
and so on.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Homotopy-inductive types}
In the previous section we showed how to encode natural numbers as W-types, with 
\begin{align*}
\natw & \defeq \wtype{b:\two} \rec_\two(\bbU;\zero;\one) \\
\zw & \defeq \supp(\true, (\lambda (x : \zero), \; \rec_\zero(\natw;x))) \\
\sw & \defeq \lambda (n : \natw), \; \supp(\false, (\lambda (x : \one), \; n))
\end{align*}
We also showed how one can define a $\dbl$ function on $\natw$ using simple elimination. When it comes to dependent elimination, however, this encoding is no longer satisfactory: given $E : \natw \to \type$ and recurrences $e_z : E(\zw)$ and $e_s : \prd{n : \natw}{y : E(n)} E(\sw(n))$, we can only construct a dependent function $r(E,e_z,e_s) : \prd{n : \natw} E(n)$ satisfying the given recurrences \emph{propositionally}. This means that the computation rules for natural numbers, which assert definitional equalities, cannot be derived from the rules for W-types.

This problem goes away if instead of the conventional inductive types we consider \emph{homotopy-inductive types}, where all computation rules are stated up to a path, i.e., the symbol $\equiv$ is replaced by $=$. For instance, the computation rule for the homotopy version of W-types $\mathbf{W^h}$ becomes:
\begin{itemize}
\item For any $x : A$ and $f : B(x) \to \wtypeh{a:A} B(a)$ we have 
\begin{align*}
& \rec_{\wtypeh{a:A} B(a)}(E;\supp(x,f)) = \\ & e(x,f,\big(\lambda (b : B(x)), \; \rec_{\wtypeh{a:A} B(a)}(E;f(b))\big))
\end{align*}
\end{itemize}

It is interesting to note that all the properties established so far for inductive types - e.g., the uniqueness principle, homotopy-initiality - hold for homotopy-inductive types as well. For instance, the type $\twoh$ with propositional computation rules for $\true$ and $\false$ gives rise to a homotopy-initial $\two$-algebra $(\nath, \true, \false)$. Furthermore, it can be shown that now we also have the converse: any homotopy-initial $2$-algebra $(\nat',t',f')$ necessarily satisfies the elimination and propositional computation rules for Booleans. We thus have the following stronger theorem:

\begin{thm}
The types satisfying the formation, introduction, elimination, and propositional computation rules are precisely the homotopy-initial $\two$-algebras.
\end{thm}

Finally, as desired, it can be shown that homotopy-natural numbers can be encoded as homotopy-W-types:

\begin{thm}
The rules for natural numbers with propositional computation rules can be derived from the rules for W-types with propositional computation rules.
\end{thm}

\section{Derivation of the Paulin-Mohring Rule}
\begin{thm}
Let $X : \type$, $x : X$, $P : \prd{y : X} (x = y) \to \type$, and $e : P(x, \refl{x})$ be given. Then for any $y : X$ and any $p : x = y$, we have $P(y,p)$.
\end{thm}
\begin{proof}
We can reformulate the above statement as follows: \medskip

\emph{Fix $x : X$, $y : X$, and $p : x = y$. Given any $P : \prd{z : X} (x = z) \to \type$ and $e : P(x, \refl{x})$, it follows that $P(y,p)$ is inhabited.}
\medskip

After applying the conventional identity elimination rule to $p$, the above statement reduces to the following:

\medskip

\emph{Fix $x : X$. Given any $P : \prd{z : X} (x = z) \to \type$ and $e : P(x, \refl{x})$, it follows that $P(x,\refl{x})$ is inhabited.}

\medskip
This is trivial, which finishes the proof.
\end{proof}

% Local Variables:
% TeX-master: "main"
% End:
