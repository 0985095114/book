\newcommand{\zero}{\ensuremath{\mathbf{0}}\xspace}
\newcommand{\one}{\ensuremath{\mathbf{1}}\xspace}
\newcommand{\two}{\ensuremath{\mathbf{2}}\xspace}
\newcommand{\three}{\ensuremath{\mathbf{3}}\xspace}
\newcommand{\nat}{\ensuremath{\mathbf{N}}\xspace}
\newcommand{\true}{\ensuremath{\mathbf{true}}\xspace}
\newcommand{\false}{\ensuremath{\mathbf{false}}\xspace}
\newcommand{\rec}{\ensuremath{\mathbf{rec}}\xspace}
\newcommand{\z}{\ensuremath{0}\xspace}
\newcommand{\wtype}[1]{\ensuremath{W}(#1)\xspace}
%\newcommand{\inl}{\ensuremath{\mathbf{inl}}\xspace}
%\newcommand{\inr}{\ensuremath{\mathbf{inr}}\xspace}
\newcommand{\s}{\ensuremath{\mathbf{s}}\xspace}
\newcommand{\alt}{\;|\;\;}
\newcommand{\disj}[2]{#1 + #2}
\newcommand{\der}{\vdash}
\newcommand{\dbl}{\ensuremath{\mathbf{double}}}

\chapter{Induction}
\label{cha:induction}

\section{Booleans and natural numbers}

% Local Variables:
% TeX-master: "main"
% End:

An \emph{inductive type} can be intuitively understood as a type generated by a certain finite collection of constructors. To specify an inductive type formally,
we will use a schematic definition, where we list the name and type of each constructor separately, e.g.:
\begin{align*}
  \two \defeq \; & \true : \two \\
         \alt & \false : \two
\end{align*}
The above definition declares the type $\two$ of Booleans to be the inductive type generated by two constant constructors $\true$ and $\false$. We can similarly define the types $\zero$ (aka Empty, Void), $\one$ (aka Unit), $\three$, and so on, with 0, 1, 3, or more constructors respectively. 

Another extremely important inductive type is the type $\nat$ of natural numbers:  
\begin{align*}
  \nat \defeq \; & \z : \nat \\
        \alt & \s : \nat \to \nat 
\end{align*}
As expected, $\nat$ is generated by a constant constructor $\z$ for the natural number zero and a unary constructor $\s$ taking a natural number $n : \nat$ to its successor $\s(n) : \nat$. It is understood that for any inductive type, different constructors construct different terms and each constructor itself is injective. This ensures that as desired, 0 is not a successor of any other natural number and that no two natural numbers have the same successor.

What can we do with such an inductive type? An intuitive understanding of an inductive type $A$ is that it \emph{behaves as if the only inhabitants of $A$ were the terms constructed solely by applying the constructors of $A$}; we refer to these particular terms as the \emph{canonical terms of $A$}. In an empty context,
each term of $A$ is canonical - for instance, any closed term of an identity type is necessarily the identity path. In the setting of nonempty contexts,  
interesting and often nontrivial behavior may occur, such as the one exhibited by identity types which behave much like paths in a space.

In the case of the type $\two$ of Booleans, assuming that each term is canonical simply means that each term $b : \two$ must be either $\true$ or $\false$. In particular, we have the principle of \emph{(dependent) elimination}:

\begin{itemize}
\item When proving a statement $\Gamma \der E : \two \to \type$ about \emph{all} Booleans, it suffices to prove it for $\true$ and $\false$, i.e., give proofs
$\Gamma \der e_t : E(\true)$ and $\Gamma \der e_f : E(\false)$.
\end{itemize}

Furthermore, the resulting proof $\Gamma \der \rec_\two(E,e_t,e_f): \prd{b : \two}E(b)$ behaves as expected when applied to the constructors $\true$ and $\false$; this principle is expressed by the \emph{computation rules}:
\begin{itemize}
\item The proof $\Gamma \der \rec_\two(E,e_t,e_f,\true) : E(\true)$ is identical to $e_t$.
\item The proof $\Gamma \der \rec_\two(E,e_t,e_f,\false) : E(\true)$ is identical to $e_f$.
\end{itemize}
For simplicity we often omit the ambient context $\Gamma$.

The rules for the type $\two$ of Booleans allow us to reason by \emph{case analysis}. Since neither of the two constructors takes any arguments, this is all we need for Booleans. However, for more complex types such as the type $\nat$ of natural numbers, true induction is often needed (hence the name \emph{inductive type}):

\begin{itemize}
\item When proving a statement $E : \nat \to \type$ about \emph{all} natural numbers, it suffices to prove it for $\z$ and for $\s(n)$, assuming it holds
for $n$. This entails giving the proofs $e_z : E(\z)$ and $e_s : \prd{n : \nat}{y : E(n)} E(\s(n))$.
\end{itemize}
The variable $y$ represents our inductive hypothesis. As for Booleans, we also have the associated computation rules for the function $\rec_\nat(E,e_z,e_s) : \prd{x:\nat} E(x)$:
\begin{itemize}
\item The proof $\rec_\nat(E,e_z,e_s,\z) : E(\z)$ is identical to $e_z$.
\item For any $n : \nat$, the proof $\rec_\nat(E,e_z,e_s,\s(n)) : E(\s(n))$ is identical to $e_s(n,\rec_\nat(E,e_z,e_s,n))$.
\end{itemize}
The dependent function $\rec_\nat(E,e_z,e_s)$ can thus be understood as being defined recursively on the argument $x : \nat$, via the recurrences $e_z$ and $e_s$: When $x$ is zero, the function simply returns $e_z$. When $x$ is the successor of another natural number $n$, the result is obtained by taking the recurrence $e_s$ and plugging in the specific predecessor $n$ and the recursive call value $\rec_\nat(E,e_z,e_s,n)$.

As an example we look at how to define a function on natural numbers which doubles its argument. We wish to apply dependent elimination with the constant type family $E \defeq \lambda(x : \nat), \nat$ since the intended type of $\dbl$ is $\nat \to \nat$. We first need to supply the value of $\dbl(\z)$, which is easy: we put $e_z \defeq \z$. Next, to compute the value of $\dbl(\s(n))$ for a natural number $n$, we first compute the value of $\dbl(n)$ and then perform the successor operation twice. This is captured by the recurrence $e_s(n,y) \defeq \s(\s(y))$; the variable $y$ stands for the result of the recursive call $\dbl(n)$. Thus, we define
\[ \dbl \defeq \rec_\nat(\lambda(x :\nat), \nat, \;  \z, \;  \lambda(n: \nat) \lambda (y:\nat), \s(\s(y))) \]
This indeed has the correct computational behavior: for example, we have 
\begin{align*}
\dbl(\s(\s(\z))) & \equiv e_s(\s(\z), \dbl(\s(\z))) \\
                 & \equiv \s(\s(\dbl(\s(\z)))) \\
                 & \equiv \s(\s(e_s(\z,\dbl(\z)))) \\
                 & \equiv \s(\s(\s(\s(\dbl(\z))))) \\
                 & \equiv \s(\s(\s(\s(e_z)))) \\
                 & \equiv \s(\s(\s(\s(\z))))
\end{align*}
as desired.

There are many similar cases when one wants to use dependent elimination with a constant family term. This is referred to as \emph{simple elimination}, which corresponds to recursion. In this case, we omit the $\lambda$-binder for simplicity and write $\dbl \defeq \rec_\nat(\nat, \z, \ldots)$.

The depndent elimination principle is quite strong and allows us to prove a variety of interesting theorems. For example, by specifying the terms $e_z$ and $e_s$, we  uniquely determine how the recursor behaves on canonical terms, and thus on all natural numbers. If we now have another function which obeys the same recurrence, then our intuition suggests these two functions should be equal. It turns out this is indeed the case and we have the following \emph{uniqueness principle}:

\begin{thm}
Let $f,g : \prd{x:\nat} E(x)$ be two functions which satisfy the recurrences $e_z : E(\z)$ and $e_s : \prd{n : \nat}{y : E(n)} E(\s(n))$ up to propositional equality, i.e., such that
\begin{align*}
\id{f(\z)}{e_z} \\ 
\id{g(\z)}{e_z}
\end{align*}
and 
\begin{align*}
\prd{n : \nat} \id{f(\s(n))}{e_s(n, f(n))} \\
\prd{n : \nat} \id{g(\s(n))}{e_s(n, g(n))}
\end{align*}
Then $f$ and $g$ are equal, i.e., we have $\alpha : \id[\prd{x :\nat} E(x)]{f}{g}$. 
\end{thm}

\begin{proof}
We use dependent elimination with the type $E(x) \defeq \id{f(x)}{g(x)}$. For the base case, we have \[f(\z) = e_z = g(\z)\]
For the inductive case, assume $n : \nat$ such that $f(n) = g(n)$. Then
\[ f(\s(n)) = e_s(n, f(n)) = e_s(n, g(n)) = g(\s(n)) \]
The first and last equality follow from the assumptions on $f$ and $g$. The middle equality follows from the inductive hypothesis and the fact that application preserves equality. This gives us pointwise equality between $f$ and $g$; invoking function extensionality finishes the proof.
\end{proof}
We note that the function $f$ is only required to satisfy the recurrences \emph{up to propositional equality}. The theorem itself only asserts propositional equality between functions - indeed, it is possible to construct functions which satisfy the same recurrence but are not definitionally equal (exercise). 

Similar uniqueness theorems can generally be formulated and shown for other inductive types as well. Such uniqueness results are a very useful tool; for instance, using uniqueness we can show that a certain class of inductive types gives rise to \emph{homotopy-initial algebras}. Taking Booleans as the simplest example, we define a \emph{$\two$-algebra} to be any type $C$ with two terms $c_0, c_1 : C$. Thus,

\begin{align*}
\mathtt{2Alg} \defeq \sm {C : \type} C \times C
\end{align*}
Given $\two$-algebras $(C,c_0,c_1)$ and $(D,d_0,d_1)$, we define a $\two$-homomorphism between them as a function $h : C \to D$ mapping $c_0$ to $d_0$ and
$c_1$ to $d_1$ (up to propositional equality). Thus,
\begin{align*}
\mathtt{2Hom \; (C,c_0,c_1) \; (D,d_0,d_1)} \defeq \sm {h : C \to D} \; \id{h(c_0)}{d_0} \times \id{h(c_1)}{d_1}
\end{align*}
A $\two$-algebra $\chi^*$ is then called \emph{homotopy-initial} if for any other $\two$-algebra $\chi$, the type of $\two$-homomorphisms from $\chi^*$ to $\chi$ is contractible. Thus,
\begin{align*}
\mathtt{is\_hinitial \; \chi^* \defeq \fall{\chi : 2Alg} \; is\_contr \; (2Hom \; \chi^*\; \chi)}
\end{align*}

We now have the following theorem; the proof is only sketched as it involves a large amount of technical detail.
\begin{thm}
The $\two$-algebra $(\two, \true, \false)$ is homotopy initial.
\end{thm}
\begin{proof}
(Rough sketch) Fix an arbitrary $\two$-algebra $(C,c_0,c_1)$. Using elimination on $\two$ it is easy to construct a $\two$-homomorphism into $C$. This will be our
center of contraction. To show that any other homomorphism is equal to the chosen one, we appeal to the uniqueness theorem for $\two$.
\end{proof}

\section{Disjoint unions and dependent sums}
A \emph{disjoint union} of two types $A$ and $B$ behaves much like a conjunction: we can choose to supply either a term of type $A$ or a term of type $B$. Schematically, we declare this as follows:

\begin{align*}
  \disj{A}{B} \defeq \; & \inl : A \to (\disj{A}{B}) \\
         \alt & \inr : B \to (\disj{A}{B})
\end{align*}

In this definition, $A$ and $B$ act as \emph{parameters}, i.e., they denote arbitrary types. In other words, the type constructor $+$ is really a function taking the types $A$ and $B$ as an argument and returning a new inductive type $\disj{A}{B}$. Although omitted from the above schema for clarity, the constructors $\inl$ and $\inr$ likewise take the types $A$ and $B$ as the first 2 arguments, followed by a term of type $A$ (for \inl) or a term of type $B$ (for \inr).

Elimination for a disjoint union amounts to case analysis:
\begin{itemize}
\item When proving a statement $E : (\disj{A}{B}) \to \type$ about \emph{all} terms of the disjoint union $\disj{A}{B}$, it suffices to prove it for $\inl(a)$ and $\inr(b)$, i.e., give proofs $e_l : \prd{a:A} E(\inl(a))$ and $e_r : \prd{b:B} E(\inr(b))$.
\end{itemize}
The associated computation rules for the function $\rec_{\disj{A}{B}}(E,e_l,e_r) : \prd{x:\disj{A}{B}} E(x)$ are as expected:
\begin{itemize}
\item For each $a : A$, the proof $\rec_{\disj{A}{B}}(E,e_l,e_r,\inl(a)) : E(\inl(a))$ is identical to $e_l(a)$.
\item For each $b : B$, the proof $\rec_{\disj{A}{B}}(E,e_l,e_r,\inr(b)) : E(\inr(b))$ is identical to $e_r(b)$.
\end{itemize}

A \emph{product} of two types $A$ and $B$ (denoted by $A \times B$) is the type of pairs $(a,b)$, where $a : A$ and $b : B$. A \emph{dependent sum} is a generalization of this concept, where we allow the type $B$ to depend on $A$. A simple example is the type of pairs $(n,(c_1,\ldots,c_n))$, where the first component is a natural number $n : \nat$ and the second component is a vector $(c_1,\ldots,c_n) : \mathbf{Vec}_C(n)$ of length $n$ over another type $C : \type$.
Given $A : \type$ and $B : A \to \type$, the dependent sum of $A$ and $B$ is given schematically as
\begin{align*}
  \sm{a:A} B(a) \defeq \; & \mathbf{pair} : \prd{a:A}{b:B(a)} \sm{a:A} B(a)
\end{align*}
Thus, the parameterized inductive type $\sm{a:A} B(a)$ has a single constructor $\mathbf{pair}$. Its first two arguments (not shown) are the parameters $A$ and $B$; the remaining two arguments are the respective components $a : A$ and $b : B(a)$. We will often denote $\mathbf{pair}(a,b)$ simply by $(a,b)$.

The elimination and computation rules are very simple:

\begin{itemize}
\item When proving a statement $E : \big(\sm{a:A} B(a)\big) \to \type$ about \emph{all} terms of the dependent sum $\sm{a:A} B(a)$, it suffices to prove it for a pair $(a,b)$, i.e., give a proof $e : \prd{a:A}{b:B(a)} E((a,b))$.
\end{itemize}

\begin{itemize}
\item For any terms $a : A$ and $b : B(a)$, the proof $\rec_{\sm{a:A} B(a)}(E,e,(a,b)) : E((a,b))$ is identical to $e(a,b)$.
\end{itemize}
Using the elimination operator, it is very easy to construct the well-know projection functions $\pi_1$ and $\pi_2$, extracting the first resp. the second component of a pair.

\section{W-types}
Martin-L{\"o}f's W-types, also known as the types of well-founded trees, are a generalization of such types as natural numbers, lists, and binary trees. A particular W-type is specified by giving two parameters $A : \type$ and $B : A \to \type$, written $\wtype{a:A} B(a)$.

The type $A$ represents the type of constructors for $\wtype{a :A} B(a)$. For instance, when defining natural numbers as a W-type the type $A$ would be the type $\two$ inhabited by the two terms $\true$ and $\false$, since there are precisely two ways how to construct a natural number - either it will be zero or a successor of another natural number. 

The dependent type family $B : A \to \type$ is used to record the arity of constructors: a constructor $a : A$ will take $B(a)$-many inductive arguments. These arguments are represented as a function $f : B(a) \to \wtype{a :A} B(a)$, with the understanding that for any $b : B(a)$, $f(b)$ is the $b$-th argument to the constructor $a$. 

In the case of natural numbers, the constructor $\true $ has arity 0, since it constucts the constant zero; the constructor $\false$ has arity 1, since it constructs the successor of its argument. We can capture this by using elimination on $\two$ to define a function into a universe of types:

\[ B \defeq \rec_\two(\lambda(b:\two), \; \bbU,\zero,\one) \]

 


The ``dependent`` type $E \defeq 

%\[\nat \stackrel{\text{def}}{=} \wtype{x}{\two}{\ifte{x}{\zero}{\one}} \]
%It is also possible to express parametrized types such as $\listt{\nat}$ as W-types. In this case we would have infinitely many constructors - one for the empty list plus a separate constructor for each $n : \nat$, corresponding to
%the $\mathbf{cons} \; n$ operation. Thus the type of constructors would be $\one + \nat$, with the arities defined in the obvious way.
%
%\subsection{Introduction Rule}
%We have the following introduction rule for W types:
%\begin{mathpar}
%\inferrule{ }{\entails{\Gamma, \term{x}{A}, \term{f}{B \to \wtype{x}{A}{B}}}{\term{\supp{x}{f}}{\wtype{x}{A}{B}}}}(\WI)
%\end{mathpar}
%By substitution, for any two terms $\entails{\Gamma}{\term{a}{A}}$ and $\entails{\Gamma}{\term{h}{\subst{B}{x}{a} \to {\wtype{x}{A}{B}}}}$ we get a term $\entails{\Gamma}{\term{\supp{a}{h}}{\wtype{x}{A}{B}}}$. Intuitively, $a$ determines a constructor for ${\wtype{x}{A}{B}}$ and $h$ determines the arguments for $a$ by mapping each argument index to an element of ${\wtype{x}{A}{B}}$. In the case of natural numbers, for instance, we would have $0 \stackrel{\text{def}}{=} \supp{\true}{\lam{x}{\zero}{\abort{x}}}$, $1 \stackrel{\text{def}}{=} \supp{\false}{\lam{x}{\one}{0}}$, $2 \stackrel{\text{def}}{=} \supp{\false}{\lam{x}{\one}{1}}$, etc.  
%
%\subsection{Elimination Rule}
%We have the following elimination rule for W types, referred to as dependent elimination:
%\begin{mathpar}
%\inferrule{\entails{\Gamma,\term{w}{\wtype{x}{A}{B}}}{\type{C}} \\
%           \entails{\Gamma,\term{x}{A},\term{f}{B \to \wtype{x}{A}{B}},\term{g}{\pitype{b}{B}{\subst{C}{w}{(\app{f}{b})}}}}{\term{H}{\subst{C}{w}{\supp{x}{f}}}}} 
%          {\entails{\Gamma,\term{w}{\wtype{x}{A}{B}}}{\term{\wrec{C}{H}{w}}{C}}}(\WDE)
%\end{mathpar}
%The above rule is the infinitary version of the elimination rule for inductively defined datatypes. If we view the dependent type $C$ as a predicate on $\wtype{x}{A}{B}$, the rule says that in order to show that $C$ holds for all  terms of $\wtype{x}{A}{B}$, it suffices to show that $C$ holds for all terms of $\wtype{x}{A}{B}$ having the form $\supp{x}{f}$ for some $x$ and $f$, thus capturing the notion that $\wtype{x}{A}{B}$ contains no other terms besides those created though one of its constructors. Moreover, when showing that $C$ holds for a term of the form $\supp{x}{f}$, we are allowed to use the induction hypothesis that $C$ holds for all arguments of the constructor $x$, capturing the notion of well-foundedness. In other words, $\wtype{x}{A}{B}$ can be seen as the type of trees of finite depth whose nodes are labeled by terms of $A$ and each node labeled by $a$ has precisely $|\subst{B}{x}{a}|$ children. The elimination rule then allows us to reason by induction on the depth of such a tree. In the case of natural numbers this reduces to ordinary induction.
%
%As an example we consider the function \emph{double} on natural numbers. To define the function recursively we must construct a suitable term 
%\[\entails{\term{x}{\two},\term{f}{B \to \nat},\term{g}{B \to \nat}}{\term{H}{\nat}}\]
%where $B$ denotes the type $\ifte{x}{\zero}{\one}$. The variable $x$ represents an arbitrary constructor, $f$ represents the predecessor (if applicable), and $g$ represents the double of the predecessor. The term $H$ itself then stands for the double of the natural number represented by $x$ and $f$. To construct $H$, we
%first define an auxiliary term $V$ as the case analysis
%\begin{align*}
%& \ifte{x \\ & \; }{\lam{f'}{\zero \to \nat}{\lam{g'}{\zero \to \nat}{0}} \\ & \;}
%  {\lam{f'}{\one \to \nat}{\lam{g'}{\one \to \nat}{\supp{\false}{\lam{-}{\one}{\supp{\false}{\lam{-}{\one}{(\app{g'}{\langle \rangle})}}}}}}}
%\end{align*}
%We then put
%\begin{align*}
%H & \stackrel{\text{def}}{=} V f \; g \\
%\textit{double} & \stackrel{\text{def}}{=} \lam{n}{\nat}{\wrec{\nat}{H}{n}}
%\end{align*}
%The variables $f'$ and $g'$ in the case analysis can be thought of as representing the functions $f$ and $g$ after the type refinement has been performed, i.e. after replacing the type $B$ by either $\zero$ or $\one$, depending on the value of $x$. If $x$ evaluates to $\true$, we are in the branch for $n = 0$ and we return 0. If $x$ evaluates to $\false$, we are in the branch for $n = s(n')$; the expression $g' \; \langle \rangle$ then gives us the result of the recursive call on the predecessor $n'$. The constructor $\false$ is then applied twice, corresponding to a double application of the successor function. This gives us the double of $n$.
% 
%\subsection{Computation Rule}
%We have the following computation rule for W types:
%\begin{align*}
%\inferrule{\entails{\Gamma, \term{w}{\wtype{x}{A}{B}}}{\type{C}} \\
%           \entails{\Gamma,\term{x}{A},\term{f}{B \to \wtype{x}{A}{B}},\term{g}{\pitype{b}{B}{\subst{C}{w}{(\app{f}{b})}}}}
%                    {\term{H}{\subst{C}{w}{\supp{x}{f}}}} \\%\\\\
%          u(w) \stackrel{\text{def}}{=} \wrec{C}{H}{w}}
%          {\entails{\Gamma, \term{x}{A}, \term{f}{B \to \wtype{x}{A}{B}}}
%                   {\term{u(\supp{x}{f}) = \subst{H}{g}{\big(\lam{b}{B}{u(\app{f}{b})}\big)}}{\subst{C}{w}{\supp{x}{f}}}}}(\WDB)      
%\end{align*}
%The rule is a form of $\beta$-reduction and states that the recursor obtained from the elimination rule behaves as specified by $H$. By substitution, we obtain the equality 
%\[\wrec{C}{H}{\supp{a}{h}} = H[a/x, h/f, \big(\lam{b}{\subst{B}{x}{a}}{\app{h}{b}}\big) / g]\]
%for any two terms $a$ and $h$ of suitable types. In the case of the function \textit{double} defined in the previous subsection, it is easy to see that by the computation rule we have $\textit{double} \; 0 = 0$ and $\textit{double} \; 1 = 2$, as expected.
%
%\section{Identity types}