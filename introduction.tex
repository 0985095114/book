\chapter*{Introduction}
\label{cha:introduction}

\addcontentsline{toc}{chapter}{Introduction}

%%% Outline
%%      - The idea of HoTT and the special role of Identity

%
%%      - The idea of informal type theory
%
%%      - Brief history - ok
%
%%      - Constructive math vs. general: no proof by contradiction/ proof relevance/ -- still need to do
%
%%      - How is type theory different from set theory? - ok
%
%%      - How is HoTT different from type theory? -ok
%
%%      - Different readers of these notes: -ok 
%%              - Working mathematicians
%%              - ... CS
%%              - ... Logicians
%%              - homotopy theorists and higher category theorists
%
%%      - weak points and work to do -- still needs to be done
%
%%      - Coq development, new proof assistant, etc. - ok
%%

\emph{Homotopy type theory} (HoTT) is a new field of mathematics, which combines aspects of several different areas in a surprising way.
It is based on a recently discovered connection between \emph{homotopy theory} and \emph{type theory}.
Homotopy theory is an outgrowth of algebraic topology and homological algebra, with connections to higher category theory; while type theory is a branch of mathematical logic and theoretical computer science.
Although these connections are currently the focus of intense investigation, it is increasingly clear that they are just the beginning of a subject that will take more time and more hard work to fully understand.

This new subject touches on topics as seemingly distant as the homotopy groups of  spheres and the decidability of type checking algorithms for computational proof assistants.  It includes a new definition of a weak $\infty$-groupoid, and a new axiom at the very foundation of mathematics --- Voevodsky's subtle and beautiful \emph{Univalence Axiom}.  One consequence of that axiom is that isomorphic structures can be identified: a principle that mathematicians have been happily using on workdays, despite its incompatibility with the ``official'' doctrines of conventional foundations.  Another new idea in this subject is \emph{higher inductive types}, which provide direct, logical descriptions of some of the basic spaces and constructions of homotopy theory: spheres, cylinders, truncations, localizations, etc., permitting an entirely new kind of ``logic of homotopy types''.

When we add in the ever-improving  implementations of type theory in computer proof assistants, we arrive at a new conception of foundations of mathematics, with intrinsic homotopical content, an ``invariant'' conception of the objects of mathematics, and a machine implementation which can serve as a practical aid to the working mathematician.  This is the \emph{univalent foundations program}.
The present book is intended as a first systematic exposition of the basics of univalent foundations, and a collection of examples of this new style of reasoning --- but without requiring the reader to know or learn any formal logic, or to use any computer proof assistant.
We contend that univalent foundations can replace set theory as the ``implicit foundation'' for the unformalized mathematics done by most mathematicians, independently of the success of its machine implementations.


\subsection*{Type theory}

Type theory was originally invented by Bertrand Russell in 1908 \cite{Russell:1908}, as a device for blocking the paradoxes in the logical foundations of mathematics  that were under investigation at the time. It was later developed as a rigorous formal system  in its own right (under the name ``$\lambda$-calculus'') by Alonzo Church \cite{Church:1933cl,Church:1940tu,Church:1941tc}.  Although it is not generally regarded as the foundation for classical mathematics, set theory being more customary, type theory still has numerous applications, especially in computer science and the theory of programming languages \cite{Pierce:2002tp}.   Per Martin-L\"{o}f \cite{MartinLof:1998tw,MartinLof:1975tb,MartinLof:1982bn,MartinLof:1984tr}, among others,
developed a generalization of Church's system which is now usually called dependent, constructive, or simply {\bf Martin\--L\"of type theory}; this is the basis of the system that we consider here. It was originally intended as a rigorous framework for the formalization of constructive mathematics.  

%Over the last 40 years it has also become clear that there are close connections between type theory and category theoretic approaches to logic and foundations, particularly topos theory \cite{elephant}

In type theory, unlike set theory, objects are classified using a primitive notion of \emph{type}, similar to the data-types used in programming languages.  These elaborately structured types can be used to express detailed specifications of the objects classified, giving rise to principles of reasoning about these objects.  To take a very simple example, the objects of a product type $A\times B$ are known to be of the form $\langle a, b\rangle$, and so one automatically knows how to construct them and how to decompose them. Similarly, an object of function type $A\to B$ can be acquired from an object of type $B$ parametrized by objects of type $A$, and can be evaluated at an argument of type $A$.  This rigidly predictable behavior of all objects (as opposed to set theory's more liberal formation principles, allowing inhomogeneous sets) is one aspect of type theory that has led to its extensive use in verifying the correctness of computer programs.  The clear reasoning principles associated with the construction of types also form the basis of modern {\bf computer proof assistants}, which are used for formalizing mathematics and verifying the correctness of formalized proofs.  We return to this aspect of type theory below.  

%(From the point of view of category theory, this is like saying that a product is determined by its universal property.)

%For example, the powerful Coq proof assistant \cite{coq} has recently been used to formalize and verify the correctness of the proof of the celebrated Feit-Thompson Odd-Order theorem \cite{gonthier}.

One problem with understanding type theory from a mathematical point of view, however, has always been that the basic concept of \emph{type} is unlike that of \emph{set} in ways that have been hard to make precise. This difficulty, we believe, has now been solved by the idea of regarding types, not as strange sets (perhaps constructed without using classical logic), but as spaces, viewed from the perspective of homotopy theory.

In homotopy theory one is concerned with spaces and continuous mappings between them, 
up to homotopy.  A \emph{homotopy} between a pair of continuous maps $f \colon X \to Y$
and  $g \colon X\to Y$ is 
a continuous map $H \colon X \times [0, 1] \to Y$ satisfying
$H(x, 0) = f (x)$  and $H(x, 1) = g(x)$. The homotopy $H$ may be thought of as a ``continuous deformation'' of $f$ into $g$. The spaces $X$ and $Y$ are said to be \emph{homotopy equivalent}, $X\simeq Y$, if there are continuous maps going back and forth, the composites of which are homotopical to the respective identity mappings, i.e., if they are isomorphic ``up to homotopy''.  Homotopy equivalent spaces have the same algebraic invariants (e.g., homology, or the fundamental group), and are said to have the same \emph{homotopy type}.

\subsection*{Homotopy type theory}

Homotopy type theory (``HoTT'') is a new field of mathematics which interprets type theory from a homotopical perspective.
In homotopy type theory, we regard the types as ``spaces'' (as studied in homotopy theory) or higher groupoids, and the logical constructions (such as the product $A\times B$) as homotopy-invariant constructions on these spaces.
In this way, we are able to manipulate spaces directly without first having to develop point-set topology (or any combinatorial replacement for it, such as the theory of simplicial sets).
To briefly explain this perspective, consider first the basic concept of type theory, namely that
the \emph{term} $a$ is of \emph{type} $A$, which is written:
$$
  a:A.
$$
This expression is traditionally thought of as akin to:
\begin{center}
``$a$ is an element of the set $A$.''
\end{center}
However, in HoTT we think of it instead as:
\begin{center}
``$a$ is a point of the space $A$.''
\end{center}
Similarly, every term $f : A\to B$ in type theory is regarded as a continuous function from the space $A$ to the space $B$.

We should stress that these ``spaces'' are treated purely homotopically, not topologically.
For instance, there is no notion of ``open subset'' of a type or of ``convergence'' of a sequence of elements of a type.
We only have ``homotopical'' notions, such as paths between points and homotopies betwen paths, which also make sense in other models of homotopy theory (such as simplicial sets).
Thus, it would be more accurate to say that we treat types as \emph{$\infty$-groupoids}; this is a name for the ``invariant objects'' of homotopy theory which can be presented by topological spaces, simplicial sets, or any other model for homotopy theory.
However, it is convenient to sometimes use topological words such as ``space'' and ``path'', as long as we remember that other topological concepts are not applicable.

(It is tempting to also use the phrase \emph{homotopy type} for these objects, suggesting the dual interpretation of ``a type (as in type theory) viewed homotopically'' and ``a space considered from the point of view of homotopy theory.''
The latter is a bit different from the classical meaning of ``homotopy type'' as an \emph{equivalence class} of spaces modulo homotopy equivalence, although it does preserve the meaning of phrases such as ``these two spaces have the same homotopy type''.)

The idea of interpreting types as structured objects, rather than sets, has a long pedigree, and is known to clarify various mysterious aspects of type theory.
For instance, interpreting types as sheaves helps explain the intuitionistic nature of type theoretic logic, while interpreting them as PERs or ``domains'' helps explain its computational aspects.
It also implies that we can use type-theoretic reasoning to study the structured objects, leading to the rich field of categorical logic.
The homotopical interpretation fits this same pattern: it clarifies the nature of \emph{identity} in type theory, and allows us to use type-theoretic reasoning in the study of homotopy theory.

The key new idea of the homotopy interpretation is that the logical notion of identity $a = b$ of two objects $a, b: A$ of the same type $A$ can be understood as the existence of a \emph{path} $p : a \leadsto b$ from point $a$ to point $b$ in the space $A$.  This also means that two functions $f, g: A\to B$ can be identified if they are homotopic, since a homotopy is just a (continuous) family of paths $p_x: f(x) \leadsto g(x)$ in $B$, one for each $x:A$.  In type theory, for every type $A$ there is a (formerly somewhat mysterious) type $\idtypevar{A}$ of identifications of two objects of $A$; in HoTT, this is just the \emph{path space} $A^I$ of all continuous maps $I\to A$ from the unit interval.  In this way, a term $p : \idtype[A]{a}{b}$ represents a path $p : a \leadsto b$ in $A$. 

The idea of homotopy type theory arose around 2006 in independent work by Awodey and Warren~\cite{AW} and Voevodsky~\cite{VV}, but it was inspired by 
Hofmann and Streicher's earlier groupoid interpretation~\cite{HofmannM:gromtt}.
Indeed, there is a precise sense in which HoTT relates type theory not only to homotopy theory, but also to higher-dimensional category theory, and in particular to weak $\infty$-groupoids.
The latter two subjects are now known to be intimately connected, as proposed by Grothendieck and now being studied intensely by both homotopy theorists and category theorists.
Moreover, the original semantic models of Awodey--Warren and Voevodsky use well-known notions and techniques from homotopy theory which are now also in use in higher category theory, such as Kan simplicial sets and Quillen model categories.

Voevodsky recognized that the simplicial interpretation of type theory satisfies a further crucial property, dubbed \emph{univalence}, which had not previously been considered in type theory (although Church's principle of extensionality for propositions turns out to be a very special case of it).
Adding univalence to type theory in the form of a new axiom has far-reaching consequences, many of which are natural, simplifying and compelling.
The Univalence Axiom also further strengthens the homotopical view of type theory, since it holds in the simplicial model and other related models, while failing under the view of types as sets.

\subsection*{Univalent Foundations}

Very briefly, the basic idea of the Univalence Axiom can be explained as follows.
In type theory, one can have a universe $\UU$, the terms of which are themselves types, $A : \UU$, etc.
Call those types that are terms of $\UU$ the \emph{small} types.
Like any type, $\UU$ has an identity type $\idtypevar{\UU}$ which expresses the identity relation $A = B$ between small types.
Thinking of types as spaces, $\UU$ is a space, the points of which are spaces; to understand its identity type, we must ask, what is a path $p : A \leadsto B$ between spaces in $\UU$?
The univalence axiom says that such paths correspond to homotopy equivalences $A\simeq B$, (roughly) as explained above.
A bit more precisely, given any (small) types $A$ and $B$, in addition to the primitive type $\idtype[\UU]AB$ of identifications of $A$ with $B$, there is the defined type $\texteqv AB$ of equivalences from $A$ to $B$.
Since the identity map on any object is an equivalence, there is a canonical map,
\[\idtype[\UU]AB\to\texteqv AB.\]
The univalence axiom states that this map is itself an equivalence.
At the risk of oversimplifying, we can state this succinctly as follows:

\begin{description}
\item[Univalence Axiom:]  $(A = B)\ \simeq\ (A\simeq B)$.
\end{description}
%
In other words, identity is equivalent to equivalence. 
In particular, one may say that ``equivalent types are identical''.
However, this phrase is somewhat misleading, since it may sound like a sort of ``skeletality'' condition which \emph{collapses} the notion of equivalence to coincide with identity, whereas in fact univalence is about \emph{expanding} the notion of identity so as to coincide with the (unchanged) notion of equivalence.

From the homotopical point of view, univalence implies that spaces of the same homotopy type are connected by a path in the universe $\UU$, in accord with the intuition for a classifying space for (small) spaces.
From the logical point of view, however, it is revolutionary: it says that isomorphic things can be identified!  Mathematicians are of course used to identifying isomorphic structures in practice, but they generally do so by ``abuse of notation'', or some other informal device, knowing that the objects involved are not ``really'' identical.  But in this new foundational scheme, such structures can be formally identified, in the logical sense that every property or construction involving one also applies to the other. Indeed, the identification is now made explicit, and properties and constructions can be systematically transported along it.  Moreover, the different ways in which such identifications may be made themselves form a structure that one can (and should!)\ take into account.

Thus in sum, for points $A$ and $B$ of the universe $\UU$ (i.e., small types), the Univalence Axiom identifies the following three notions:
\begin{itemize}
\item (logical) an identification $p:A=B$ of $A$ and $B$
\item (topological) a path $p:A \leadsto B$ from $A$ to $B$ in $\UU$
\item (homotopical) an equivalence $p:A\simeq B$ between $A$ and $B$.
\end{itemize}

%Voevodsky has christened this new foundational scheme, consisting of the combination of homotopy type theory with the univalence axiom, and possibly some further logical principles, together with an implementation in a computer proof assistant, the {\bf Univalent Foundations of Mathematics}.

\subsection*{Higher inductive types}

One of the classical advantages of type theory is its simple and effective techniques for working with inductively defined structures.
The simplest nontrivial inductively defined structure is the natural numbers, which is inductively generated by zero and the successor function.
From this statement one can automatically extract the principle of mathematical induction, which characterizes the natural numbers.
More general inductive definitions encompass lists and well-founded trees of all sorts, each of which is characterized by a corresponding ``induction principle''.
This includes most data structures used in computer programming; hence the great usefulness of type theory in formal reasoning about the latter.
If conceived in a very general sense, inductive definitions also include examples such as a disjoint union $A+B$, which may be regarded as ``inductively'' generated by the two injections $A\to A+B$ and $B\to A+B$.
The ``induction principle'' in this case is ``proof by case analysis'', which characterizes the disjoint union.

In homotopy theory, it is natural to consider also ``inductively defined spaces'' which are generated not merely by a collection of \emph{points}, but also by collections of \emph{paths} and higher paths.
Classically, such spaces are called \emph{CW complexes}.
For instance, the circle $S^1$ is generated by a single point and a single path from that point to itself.
Similarly, the 2-sphere $S^2$ is generated by a single point $b$ and a single two-dimensional path from the constant path at $b$ to itself, while the torus $T^2$ is generated by a single point, two paths $p$ and $q$ from that point to itself, and a two-dimensional path from $p\ct q$ to $q\ct p$.

By using the identification of paths with identities in homotopy type theory, these sort of ``inductively defined spaces'' can be characterized in type theory by ``induction principles'', entirely analogously to classical examples such as the natural numbers and the disjoint union.
The resulting \emph{higher inductive types} give a direct ``logical'' way to reason about familiar spaces such as spheres, which (in combination with univalence) can be used to perform familiar arguments from homotopy theory, such as calculating homotopy groups of spheres, in a purely formal way.
The resulting proofs are a marriage of classical homotopy-theoretic ideas with classical type-theoretic ones, yielding new insight into both disciplines.

Moreover, this is only the tip of the iceberg: many abstract constructions from homotopy theory, such as homotopy colimits, suspensions, Postnikov towers, localization, completion, and spectrification, can also be expressed as higher inductive types.
Many of these are classically constructed using Quillen's ``small object argument'', which can be regarded as a finite way of algorithmically describing an infinite CW complex presentation of a space, just as ``zero and successor'' is a finite algorithmic description of the infinite set of natural numbers.
Spaces produced by the small object argument are infamously complicated and difficult to understand; the type-theoretic approach is much simpler, bypassing the need for any explicit construction by giving direct access to the appropriate ``induction principle''.
Thus, the combination of univalence and higher inductive types suggests the possibility of a true revolution in the practice of homotopy theory.


\subsection*{Informal type theory}

One difficulty often encountered by the classical mathematician when faced with learning about type theory is that it is usually presented as a fully or partially formalized deductive system.
This style, which is very useful for proof-theoretic investigations, is not particularly convenient for use in applied, informal reasoning.
Nor is it even familiar to most working mathematicians, even those who might be interested in foundations of mathematics.
One objective of the present work is to develop an informal style of working \emph{in} HoTT that is at once rigorous and precise, but is also closer to the language and style of presentation of everyday mathematics.

In present-day mathematics, one usually constructs and reasons about mathematical objects in a way that could in principle, one presumes, be formalized in a system of elementary set theory like ZFC --- at least given enough ingenuity and patience.
For the most part, one does not even need to be aware of this possibility, since it largely coincides with the condition that a proof be ``fully rigorous'' (in the sense that all mathematicians have come to understand intuitively through education and experience).
But there are a few aspects of working in ``informal set theory'' that one does need to learn to be careful about: the use of collections too large or inchoate to  be sets, the axiom of choice and its equivalents,  the method of proof by contradiction, and so on.
Adopting a new foundational system such as HoTT as the \emph{implicit formal basis} of informal reasoning will require adjusting some of one's instincts and practices, even when reasoning informally.
The present text is intended to serve as an example of this ``new kind of mathematics'', which is still informal, but could now in principle be formalized in HoTT, rather than ZFC, again given enough ingenuity and patience.

It is worth emphasizing that in the new system, however, such formalization has a real practical benefit, increasing the likelihood that one will actually want to carry it out.
Namely, the formal system can be (and has been) implemented in a computer proof assistant.
In practical terms, this means that it is possible to use currently available proof assistants based on type theory to develop mathematics, to verify the correctness of proofs, to provide some degree of automation, and even to extract numerical algorithms from formal proofs.  We believe that this aspect of the univalent foundations program distinguishes it from other approaches to foundations, by providing a real practical utility for the working mathematician.

Indeed, many of the results to be described in this book were actually \emph{first} done in a fully formalized form in a proof assistant, and are only now being ``unformalized'' for the first time --- a reversal of the usual relation between formal and informal mathematics.   One can imagine a not too distant future when it will be possible for mathematicians  to verify the correctness of their own papers by working within the system of univalent foundations, formalized in a proof assistant, and that doing so will become as natural as typesetting their own papers in \TeX. (Whether this proves to be the publishers' dream or their nightmare  remains to be seen.) 
In principle, this could be equally true for any other foundational system based on type theory; but we believe it to be more practically achievable using univalent foundations, because of the many independent advantages of the latter system for the working mathematician.

%We refer the reader to \cite{Simpson:2004bt,Hales:2008ud} for two accounts of the use of computer proof assistants in general.   


\subsection*{Constructivity} 


One of the most striking differences between classical foundations and type theory is the idea of \emph{proof relevance}, according to which mathematical statements, and even their proofs, become first-class mathematical objects.
In type theory, we represent mathematical statements by types, which can be regarded simultaneously as both mathematical constructions and mathematical assertions, a conception also known as \emph{propositions as types}.
Accordingly, we can regard a term $a : A$ as both an element of the type $A$ (or in HoTT, a point of the space $A$), and at the same time, a proof of the proposition $A$.
To take an example, suppose we have sets $A$ and $B$ (discrete spaces), and consider the statement ``$A$ is isomorphic to $B$.''
In type theory, this can be rendered as:
\[
\mathsf{Iso}(A,B)\ :=\ \Sigma_{f : A\to B}\Sigma_{g : B\to A}\big((\Pi_{x:A}\, gf(x) = x) \times (\Pi_{y:B}\, fg(y) = y)\big)
\]
%
Reading the type constructors $\Sigma, \Pi, \times$  here  as ``there exists'', ``for all'', and ``and'' respectively yields the usual formulation of ``$A$ and $B$ are isomorphic''; on the other hand, reading them as sums and products yields the \emph{type of all isomorphisms} between $A$ and $B$!  To prove that $A$ and $B$ are isomorphic, one  constructs a proof $p : \mathsf{Iso}(A,B)$, which is therefore the same  as constructing an isomorphism between $A$ and $B$, i.e., exhibiting a pair of functions $f, g$ together with \emph{proofs} that their composites are the respective identity maps.  The latter proofs, in turn, are nothing but homotopies of the appropriate sorts.  In this way, \emph{proving a proposition is the same as constructing a term of some particular type.}

In particular, to prove a statement of the form ``$A$ and $B$'' is just to prove $A$ and to prove $B$, i.e., to give a term of type $A\times B$.
And to prove that $A$ implies $B$ is just to find a term of type $A\to B$, i.e., a function from $A$ to $B$ (determining a mapping of proofs of $A$ to proofs of $B$).
This ``constructive'' conception (for more on which, see~\cite{kolmogorov,BHK}) is what gives type theory its good computational character.
For instance, every proof that something exists carries with it enough information to actually find such an object; and from a proof that  ``$A$ or $B$'' holds, one can extract either a proof that $A$ holds or one that $B$ holds.
Thus, from every proof we can automatically extract an algorithm; this is very useful in applications to computer programming.

However, this conception of logic does behave in ways unfamiliar to most mathematicians.
For instance, a naive translation of the \textbf{axiom of choice} yields a statement that is a provable principle of reasoning.
Essentially, the type theoretic notion of ``there exists'' is strong enough to ensure that, if for every $x: A$ there exists a $y:B$ such that $R(x,y)$, then there is a function $f : A\to B$ such that, for all $x:A$, we have $R(x, f(x))$.
On the other hand, the conception of ``or'' is so strong that a naive translation of the \textbf{law of excluded middle} yields a statement that cannot be established.
For if we always had a proof of ``$A$ or not $A$'', then for \emph{every} proposition $A$ we would either have a proof of it or a refutation --- which would of course be nice, but it would also allow us to construct the Halting Oracle and other non-computable functions.
(In fact, the univalence axiom implies the stronger statement that this naive form of excluded middle is actually false; see \autoref{thm:not-lem}.)

Thus, the logic of ``proposition as types'' suggested by traditional type theory is not the ``classical'' logic familiar to most mathematicians.
But it is also different from the logic often called ``intuitionistic'', which lacks \emph{both} the law of excluded middle and the axiom of choice.
It is sometimes called ``constructive'' logic, but one should be aware that intuitionistic logic is also sometimes given that name; for emphasis we may call it \emph{pure constructive logic}.

The computational advantages of pure constructive logic imply that we should not lightly discard it, but for the purposes of mathematics, its nonclassicality can be problematic.
Many mathematicians are, of course, accustomed to rely on the law of excluded middle; while the ``axiom of choice'' that is available in pure constructive logic looks superficially similar to its classical namesake, but does not have any of its strong consequences.
Fortunately, homotopy type theory gives a finer analysis of this question, which allows all kinds of logic to coexist and intermix.

The idea is that types, just like spaces in classical homotopy theory, can be ``stratified'' according to in which dimensions their higher homotopy structure exists.
In particular, Voevodsky has found a type-theoretic definition of \emph{homotopy $n$-types}: spaces with no nontrivial homotopy information above dimension $n$.
Moreover, with higher inductive types, we can universally ``truncate'' a type into an $n$-type; in classical homotopy theory this would be its $n^{\mathrm{th}}$ Postnikov section.

With these notions in hand, the homotopy $(-1)$-types, which we call \emph{mere propositions}, support a logic that is much more like traditional ``intuitionistic'' logic.
(Classically, every $(-1)$-type is empty or contractible; we interpret these possibilities as the truth values ``false'' and ``true'' respectively.)
The ``$(-1)$-truncated axiom of choice'' is not automatically true, but is a strong assumption with the same sorts of consequences as its counterpart in set theory.
Similarly, the ``$(-1)$-truncated law of excluded middle'' may be assumed, with many of the same consequences as in classical mathematics.
Thus, the homotopical perspective reveals that classical/intuitionistic logic and pure constructive logic can coexist as opposite ends of a sliding scale, with an infinite number of possibilities in between (the homotopy $n$-types for $-1 < n < \infty$).
We may speak of ``LEM$_n$'' and ``AC$_n$'', with AC$_\infty$ being provable and LEM$_\infty$ inconsistent with univalence, while AC$_{-1}$ and LEM$_{-1}$ are the versions familiar to classical mathematicians --- hence in most cases it is appropriate to assume the subscript $(-1)$ when none is given.

It is worth emphasizing that univalent foundations does not \emph{require} the use of constructive or intuitionistic logic.
Most of classical mathematics which depends on the law of excluded middle and the axiom of choice can be performed in univalent foundations, simply by assuming that these two principles hold (in their proper, $(-1)$-truncated, form).
However, type theory does encourage avoiding these principles when they are unnecessary, for several reasons.

First of all, every mathematician knows that a theorem is more powerful when proven using fewer assumptions, since it applies to more examples; the situation with AC and LEM is no different.
Type theory admits many interesting ``nonstandard'' models, such as in sheaf toposes, and likewise homotopy type theory admits models in higher toposes such as studied in~\cite{lurie:higher-topoi}.
Classicality principles such as AC and LEM tend to fail in such models.
But if we avoid using these principles, then the theorems we prove will be valid internally to all such models.

Secondly, one of the additional virtues of type theory is its computable character.
In addition to a foundation for mathematics, type theory is a formal theory of computation, and can be treated as a powerful programming language.
From this perspective, the rules of the system cannot be chosen arbitrarily the way set-theoretic axioms can: there must be a harmony between them which allows all proofs to be ``executed'' as programs.
We do not yet fully understood the new principles introduced by homotopy type theory, such as univalence and higher inductive types, from this point of view, but the basic outlines are clear; see e.g.~\autoref{sec:computational} and~\cite{lh:canonicity}.
However, principles such as AC and LEM are fundamentally antithetical to computability, since they assert baldly that certain things exist without giving any way to compute them.
Thus, avoiding them is also necessary to maintain the character of type theory as a theory of computation.

Fortunately, constructive reasoning is not as hard as it may seem.
Quite often, simply by rephrasing some definitions, a theorem can be made constructive and its proof more elegant at the same time.
Moreover, univalent foundations makes this even more true than it used to be.
For instance:
\begin{enumerate}
\item In set-theoretic foundations, various constructions in homotopy theory and category theory rely on the axiom of choice to perform transfinite constructions.
  But with higher inductive types, we can encode these constructions directly and constructively.
  In particular, none of the ``synthetic'' homotopy theory we will do in \autoref{cha:homotopy} requires LEM or AC.
\item In set-theoretic foundations, the statement ``every fully faithful and essentially surjective functor is an equivalence of categories'' is equivalent to the axiom of choice.
  But with the univalence axiom, it is just \emph{true}; see \autoref{cha:category-theory}.
\item In set theory, various circumlocutions are required to obtain notions of ``cardinal number'' and ``ordinal number'' which canonically represent isomorphism classes of sets and well-ordered sets, respectively --- possibly involving the axiom of choice or the axiom of foundation.
  But with univalence and higher inductive types, we can obtain such representatives directly by truncating the universe; see \autoref{cha:set-math}.
\item In set-theoretic foundations, the definition of the real numbers as equivalence classes of Cauchy sequences requires the axiom of (countable) choice to be well-behaved.
  But with higher inductive types, we can give a version of this definition which is well-behaved and avoids any choice principles; see \autoref{cha:real-numbers}.
\end{enumerate}
However, we emphasize again that the reader does not have to care about, or worry about, constructivity to read this book.
The point is that in all of the above examples, the version of the theory we present is the \emph{best} way to to it, whether or not LEM and AC are available.
Constructivity is an added bonus.

Given this discussion of adding new principles such as univalence, higher inductive types, AC, and LEM, one may wonder whether the resulting system remains consistent.
(One of the original virtues of type theory, relative to set theory, was that it can be seen to be consistent by proof-theoretic means).
As with any foundational system, consistency is a relative question: ``consistent with respect to what?''
The short answer is that all of the constructions and axioms considered in this book have a model in the category of Kan complexes, due mostly to Voevodsky~\cite{klv:ssetmodel} (see~\cite{ls:hits} for HITs).
Thus, they are known to be formally consistent relative to ZFC (with as many inaccessible cardinals as we need nested univalent universes).
Giving a more traditionally type-theoretic account of this consistency is work in progress (see, e.g.,~\cite{lh:canonicity}).

We summarize the different points of view of the type-theoretic operations in Table~\ref{tab:pov}.

\begin{table}\centering
  \begin{tabular}{c||c|c|c}
       Type Theory & Logic & Set Theory & HoTT\\\hline
       $A$ & proposition & set & space\\
       $a:A$ & proof & element & point \\
       $x:A \vdash B(x)$ & predicate & family of sets & fibration \\
       $x:A \vdash b(x) : B(x)$ & conditional proof & family of elements & section\\
       $0, 1$ & $\bot, \top$ & $\emptyset, \{ \emptyset \}$ & $\emptyset, *$\\
       $A + B$ & $A\vee B$ & disjoint union & coproduct\\
       $A\times B$ & $A\wedge B$ & set of pairs & product space\\
       $A\to B$ & $A\Rightarrow B$ & set of functions & function space\\
       $\Sigma_{x:A}B(x)$ &  $\exists_{x:A}B(x)$ & disjoint sum & total space\\
       $\Pi_{x:A}B(x)$ &  $\forall_{x:A}B(x)$ & product & space of sections\\
       $\mathsf{Id}_{A}$ & identity relation & $\{\langle x,x\rangle\, |\, x\in A\}$ & path space $A^I$
  \end{tabular}
  \caption{Comparing points of view on type-theoretic operations}\label{tab:pov}
\end{table}

% TODO: write something about reasons to be constructive

\subsection*{Open problems} 

For those interested in contributing to this new branch of mathematics, it may be encouraging to know that there are many interesting open questions.

Perhaps the most pressing of them is the ``constructivity'' of the Univalence Axiom, posed by Voevodsky in \cite{Vo2012}.
The basic system of type theory follows the structure of Gentzen's natural deduction. Logical connectives are defined by their introduction rules, and have elimination rules justified by computation rules. Following this pattern, and using Tait's computability method, originally designed to analyse G\"odel Dialectica interpretation, one can show the property of {\em normalization} for type theory. This in turn implies important properties such as decidability of type-checking (a crucial property since type-checking corresponds to proof-checking, and one can argue that we should be able to "recognize a proof when we see one"), and the so-called "canonicity property" that any closed term of the type of natural numbers reduces to a numeral. This last property, and the uniform structure of introduction/elimination for rules, are lost when one extends type theory with an axiom, such as the axiom of function extensionality, or the Univalence Axiom. Voevodsky has formulated a precise mathematical conjecture connected to this question of canonicity for type theory extended with the axiom of Univalence: given a closed term of the type of natural numbers, is it always possible to find a numeral and a proof that this term is equal to this numeral, where this proof of equality may itself use the Univalence Axiom? More generally, an important issue is if it is possible to provide a constructive justification of the Univalence Axiom.
What about if one adds other homotopically motivated constructions, like higher inductive types?
These questions remain open at the present time, although methods are currently being developed to try to find answers.

Another basic issue is the difficulty of working with types, like the natural numbers, which are essentially sets (i.e., discrete spaces), containing only trivial paths.
At present, homotopy type theory can really only characterize spaces up to homotopy equivalence, which means that these ``discrete spaces'' may only be \emph{homotopy equivalent} to discrete spaces.
Type-theoretically, this means there are many paths that are equal to reflexivity, but not \emph{judgmentally} equal to it (see \cref{sec:types-vs-sets} for the meaning of ``judgmentally'').
While this homotopy-invariance has advantages, these ``meaningless'' identity terms do introduce needless complications into arguments and constructions, so it would be convenient to have a systematic way of eliminating or collapsing them.
In some cases, the proliferation of such superfluous identity terms makes it very difficult or impossible to formulate what should be a straightforward concept, such as the definition of a (semi-)simplicial type.

A more specialized, but no less important, problem is the relation between HoTT and the research  on \emph{higher toposes} currently happening at the intersection of higher category theory and homotopy theory.
There is a growing conviction among those familiar with both subjects that they are, at root, one.
For instance, the notion of a univalent universe should coincide with that of an object classifier, while higher inductive types should be an ``elementary'' reflection of local presentability.
More generally, we expect that HoTT should be the internal language of $(\infty,1)$-toposes.
Despite this general consensus, however, the details remain to be worked out, and doing so will undoubtedly lead to further insights into both concepts.

But by far the largest field of work to be done is in the ongoing formalization of everyday mathematics in this new system.
Recent successes in formalizing some facts from basic homotopy theory and category theory have been encouraging; some of these are described in \cref{cha:homotopy,cha:category-theory}.
Obviously, however, much work remains to be done.

%\subsection*{Some references?}
%
%Some other surveys and introductions?


% Local Variables:
% TeX-master: "main"
% End:
