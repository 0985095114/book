\chapter{Homotopy type theory}
\label{cha:basics}

\section{Types are higher groupoids}
\label{sec:equality}

Recall that for any type $A$, and any $x,y:A$, we have a identity type $\id[A]{x}{y}$, also written $\idtype[A]{x}{y}$ or just $x=y$.
We can think of inhabitants of $x=y$ as either
\begin{enumerate}
\item proofs or evidence that $x$ and $y$ are equal,
\item identifications of $x$ with $y$,
\item paths from $x$ to $y$, or
\item isomorphisms/equivalences from $x$ to $y$.
\end{enumerate}
The first is more traditional in type theory; but in homotopy type theory we often take the latter perspectives as well.
It turns out that the defining rules of identity types, as described in the previous chapter, give them structure which corresponds precisely to that of a space or a higher groupoid.

Recall that the induction principle for the identity types $\id[A]{x}{y}$ (with $A$ a fixed type) says that if
\begin{itemize}
\item for every $x,y:A$ and every $p:\id[A]xy$ we have a type $D(x,y,p)$, and
\item for every $a:A$ we have an element $d(a):D(a,a,\refl a)$, 
\end{itemize}
then
\begin{itemize}
\item there exists an element $J_{D,d}(x,y,p):D(x,y,p)$ for \emph{every} two elements $x,y:A$ and $p:\id[A]xy$, such that $J_{D,d}(a,a,\refl a) \jdeq d(a)$.
\end{itemize}
In other words, given dependent functions
\begin{align*}
D & :\prd{x,y:A}{p:\id{x}{y}} \type\\
d & :\prd{a:A} D(a,a,\refl{a})
\end{align*}
there is a dependent function
\[J_{D,d}:\prd{x,y:A}{p:\id{x}{y}} D(x,y,p)\]
such that 
\begin{equation}\label{eq:Jconv}
J_{D,d}(a,a,\refl{a})\jdeq d(a)
\end{equation}
for every $a:A$.
The notation $J$ is traditional for this function, but we will not use it very much.
Usually, every time we apply this induction rule we will either not care about the specific function being defined, or we will immediately give it a different name.

Informally, the induction principle for identity types says that if we want to construct an object (or prove a statement) which depends on an inhabitant $p:\id[A]xy$ of an identity type, then it suffices to perform the construction (or the proof) in the special case when $x$ and $y$ are the same (judgmentally) and $p$ is a reflexivity term $\refl{x}$ (judgmentally).
When writing informally, we may express this with a phrase such as ``by induction, it suffices to assume\dots''.
This reduction to the ``reflexivity case'' is analogous to the reduction to the ``base case'' and ``inductive step'' in an ordinary proof by induction on the natural numbers, and also to the ``left case'' and ``right case'' in a proof by case analysis on a disjoint union or disjunction.

The ``conversion rule''~\eqref{eq:Jconv} is less familiar in the context of proof by induction on natural numbers, but there is an analogous notion in the related concept of definition by recursion.
If a sequence $(a_n)_{n\in \mathbb{N}}$ is defined by giving $a_0$ and specifying $a_{n+1}$ in terms of $a_n$, then in fact the $0^{\mathrm{th}}$ term of the resulting sequence \emph{is} the given one, and the given recurrence relation relating $a_{n+1}$ to $a_n$ holds for the resulting sequence.
(This may seem so obvious as to not be worth saying, but if we view a definition by recursion as an algorithm for calculating values of a sequence, then it is precisely the process of executing that algorithm.)
The rule~\eqref{eq:Jconv} is analogous: it says that if we define an object $f(p)$ for all $p:x=y$ by specifying what the value should be when $p$ is $\refl{x}:x=x$, then the value we specified is in fact the value of $f(\refl{x})$.

We now derive from this induction principle all the structure of a higher groupoid.
We begin with symmetry of equality, which, in topological language, means that ``paths can be reversed''.

\begin{lem}\label{lem:opp}
  For every type $A$ and every $x,y:A$ there is a function
  \begin{equation*}
    (x= y)\to(y= x)
  \end{equation*}
  denoted $p\mapsto \opp{p}$, such that $\opp{\refl{x}}\jdeq\refl{x}$ for each $x:A$.
\end{lem}
\begin{proof}[First proof]
  Let $D:\prd{x,y:A}{p:x= y} \type$ be the type family defined by $D(x,y,p)\defeq (y= x)$.
  In other words, $D$ is a function assigning to any $x,y:A$ and $p:x=y$ a type, namely the type $y=x$.
  Then we have
  \begin{equation*}
    d\defeq \lambda x.\refl{x}:\prd{x:A} D(x,x,\refl{x}).
  \end{equation*}
  Thus, the eliminator $J$ for identity types gives us a term $J_{D,d}(x,y,p): (y= x)$ for each $p:(x= y)$.
  We can now define the desired function $\opp{(-)}$ to be $\lambda p. J_{D,d}(x,y,p)$, i.e.\ we set $\opp{p} \defeq J_{D,d}(x,y,p)$.
  The conversion rule~\eqref{eq:Jconv} gives $\opp{\refl{x}}\jdeq \refl{x}$.
\end{proof}

We have written out this proof in a very formal style, which may be helpful while the induction rule on identity types is unfamiliar.
However, eventually we prefer to use more natural language, such as in the following equivalent proof.

\begin{proof}[Second proof]
  We want to construct, for each $x,y:A$ and $p:x=y$, an element $\opp{p}:y=x$.
  By induction, it suffices to do this in the case when $y$ is $x$ and $p$ is $\refl{x}$.
  But in this case, the type $x=y$ of $p$ and the type $y=x$ in which we are trying to construct $\opp{p}$ are both simply $x=x$.
  Thus, in the ``reflexivity case'', we can define $\opp{\refl{x}}$ to be simply $\refl{x}$.
  The general case then follows by the induction principle, and the conversion rule $\opp{\refl{x}}\jdeq\refl{x}$ is precisely the proof in the reflexivity case that we gave.
\end{proof}

We will write out the next few proofs in both styles, to help the reader become accustomed to the latter one.
Next we prove the transitivity of equality, or equivalently we ``concatenate paths''.

\begin{lem}\label{lem:concat}
  For every type $A$ and every $x,y,z:A$ there is a function
  \begin{equation*}
    (x= y) \to (y= z)\to (x=  z)
  \end{equation*}
  written $(p,q)\mapsto p\ct q$, such that $\refl{x}\ct \refl{x}\jdeq \refl{x}$ for any $x:A$.
\end{lem}

\begin{proof}[First proof]
  Let $D:\prd{x,y:A}{p:x=y} \type$ be the type family
  \begin{equation*}
    D(x,y,p)\defeq \prd{z:A}{q:y=z} (x=z).
  \end{equation*}
  Note that $D(x,x,\refl x) \jdeq \prd{z:A}{q:x=z} (x=z)$.
  Thus, in order to apply the induction principle for identity types to this $D$, we need a function of type
  \begin{equation}\label{eq:concatD}
    \prd{x:A} D(x,x,\refl{x})
    \jdeq \prd{x,z:A}{q:x=z} (x=z).
  \end{equation}
  Now let $E:\prd{x,z:A}{q:x=z}\type$ be the type family $E(x,z,q)\defeq (x=z)$.
  Note that $E(x,x,\refl x) \jdeq (x=x)$.
  Thus, we have the function
  \begin{equation*}
    e(x) \defeq \refl{x} : E(x,x,\refl{x}).
  \end{equation*}
  By the induction principle for identity types applied to $E$, we obtain a function
  \begin{equation*}
    d(x,z,q) : \prd{x,z:A}{q:x=z} E(x,z,q) \jdeq \prd{x,z:A}{q:x=z} (x=z)
  \end{equation*}
  which is~\eqref{eq:concatD}.
  Thus, we can use this function $d$ and apply the induction principle for identity types to $D$, to obtain our desired function of type
  \begin{equation*}
    \prd{x,y,z:A}{q:y=z}{p:x=y} (x=z).
  \end{equation*}
  The conversion rules for the two induction principles give us $\refl{x}\ct \refl{x}\jdeq \refl{x}$ for any $x:A$.
\end{proof}

\begin{proof}[Second proof]
  We want to construct, for every $x,y,z:A$ and every $p:x=y$ and $q:y=z$, an element of $x=z$.
  By induction on $p$, it suffices to assume that $y$ is $x$ and $p$ is $\refl{x}$.
  In this case, the type $y=z$ of $q$ is $x=z$.
  Now by induction on $q$, it suffices to assume also that $z$ is $x$ and $q$ is $\refl{x}$.
  But in this case, $x=z$ is $x=x$, and we have $\refl{x}:(x=x)$.
\end{proof}

The reader may well feel that we have given an overly convoluted proof of this lemma.
In fact, we could stop after the induction on $p$, since at that point what we want to produce is an equality $x=z$, and we already have such an equality, namely $q$.
Why do we go on to do another induction on $q$?

The answer is that, as described in the introduction, we are doing \emph{proof-relevant} mathematics.
When we prove a lemma, we are defining an inhabitant of some type, and it can matter what \emph{specific} element we defined in the course of the proof, not merely the type that that element inhabits (that is, the \emph{statement} of the lemma).
\autoref{lem:concat} has three obvious proofs: we could do induction over $p$, induction over $q$, or induction over both of them.
If we proved it three different ways, we would have three different elements of the same type.
It's not hard to show that these three elements are equal (see \autoref{ex:basics:concat}), but as they are not \emph{definitionally} equal, there can still be reasons to prefer one over another.

In the case of \autoref{lem:concat}, the difference hinges on the computation rule.
If we proved the lemma using a single induction over $p$, then we would end up with a computation rule of the form $\refl{y} \ct q \jdeq q$.
If we proved it with a single induction over $q$, we would have instead $p\ct\refl{x}\jdeq p$, while proving it with a double induction (as we did) gives only $\refl{x}\ct\refl{x} \jdeq \refl{x}$.

The asymmetrical computation rules can sometimes be convenient when doing formalized mathematics, as they allow the computer to simplify more things automatically.
However, in informal mathematics, and arguably even in the formalized case, it can be confusing to have a concatenation operation which behaves asymmetrically and to have to remember which side is the ``special'' one.
Treating both sides symmetrically makes for more robust proofs; this is why we have given the proof that we did.
(However, this is admittedly a stylistic choice.)

The table below summarizes the ``equality'' and ``homotopical'' points of view on what we have done so far.
\begin{center}
  \begin{tabular}{c|c}
    Equality & Homotopy \\\hline
    reflexivity & constant path\\
    symmetry & inversion of paths\\
    transitivity & concatenation of paths
  \end{tabular}
\end{center}

However, proof-relevance also means that we can't stop after proving ``symmetry'' and  ``transitivity'' of equality: we need to know that these \emph{operations} on equalities are well-behaved.
(This issue is invisible to set-level mathematics, where symmetry and transitivity are mere \emph{properties} of equality, rather than structure on paths.)
For instance, we need to know that concatenation is \emph{associative}, and that inversion provides \emph{inverses} with respect to concatenation.
This is to be expected from the topological point of view, where these are regarded as \emph{operations} on paths.

\begin{lem}\label{thm:omg}%[The $\omega$-groupoid structure of types]
  Suppose $A:\type$, that $x,y,z,w:A$ and that $p:x= y$ and $q:y = z$ and $r:z=w$.
  We have the following:
  \begin{enumerate}
  \item $p= p\ct \refl{y}$ and $p = \refl{x} \ct p$.\label{item:omg1}
  \item $\opp{p}\ct p=  \refl{y}$ and $p\ct \opp{p}= \refl{x}$.
  \item $\opp{(\opp{p})}= p$.
  \item $p\ct (q\ct r)=  (p\ct q)\ct r$.\label{item:omg4}
  \end{enumerate}
\end{lem}

Note, in particular, that~\ref{item:omg1}--\ref{item:omg4} are themselves propositional equalities, living in the identity types of the identity types $x=y$.
Topologically, they are \emph{paths of paths}, and we are familiar topologically with the idea that concatenating a path with the reversed path only gives a constant path \emph{up to homotopy}, i.e.\ up to a higher path.
The paths~\ref{item:omg1}--\ref{item:omg4} also satisfy their own higher coherence laws, which are themselves higher paths, and so on all the way up.

However, for most purposes it is unnecessary to make the whole infinite-dimensional structure explicit.
One of the nice things about homotopy type theory is that all of this structure can be \emph{proven} starting from only the inductive property of identity types, so we can make explicit as much or as little of it as we need.
In particular, often we don't need the complicated combinatorics involved in making precise notions such as ``coherent structure at all higher levels''.

\begin{proof}[Proof of~\autoref{thm:omg}]
  All the proofs use the induction principle for equalities.
  \begin{enumerate}
  \item \emph{(First proof)} Let $D:\prd{x,y:A}{p:x=y} \type$ be the type family given by 
    \begin{equation*}
      D(x,y,p)\defeq (p= p\ct \refl{y}).
    \end{equation*}
    Then $D(x,x,\refl{x})$ is $\refl{x}=\refl{x}\ct\refl{x}$.
    Since $\refl{x}\ct\refl{x}\jdeq\refl{x}$, it follows that $D(x,x,\refl{x})\jdeq (\refl{x}=\refl{x})$.
    Thus, there is a term
    \begin{equation*}
      d\defeq\lambda x.\refl{\refl{x}}:\prd{x:A} D(x,x,\refl{x}).
    \end{equation*}
    Now $J$ gives a term $J(D,d,p):(p= p\ct\refl{y})$ for each $p:x= y$.
    The other equality is proven similarly.

    \noindent
    \emph{(Second proof)} By induction on $p$, it suffices to assume that $y$ is $x$ and that $p$ is $\refl x$.
    But in this case, we have $\refl{x}\ct\refl{x}\jdeq\refl{x}$.
  \item \emph{(First proof)} Let $D:\prd{x,y:A}{p:x=y} \type$ be the type family given by 
    \begin{equation*}
      D(x,y,p)\defeq (\opp{p}\ct p=  \refl{y}).
    \end{equation*}
    Then $D(x,x,\refl{x})$ is $\opp{\refl{x}}\ct\refl{x}=\refl{x}$.
    Since $\opp{\refl{x}}\jdeq\refl{x}$ and $\refl{x}\ct\refl{x}\jdeq\refl{x}$, we get that $D(x,x,\refl{x})\jdeq (\refl{x}=\refl{x})$.
    Hence we find the function
    \begin{equation*}
      d\defeq\lambda x.\refl{\refl{x}}:\prd{x:A} D(x,x,\refl{x}).
    \end{equation*}
    Now $J$ gives a term $J(D,d,p):\opp{p}\ct p=\refl{y}$ for each $p:x= y$ in $A$.
    The other equality is similar.

    \noindent \emph{(Second proof)} By induction, it suffices to assume $p$ is $\refl x$.
    But in this case, we have $\opp{p} \ct p \jdeq \opp{\refl x} \ct \refl x \jdeq \refl x$.
  \item \emph{(First proof)} Let $D:\prd{x,y:A}{p:x=y} \type$ be the type family given by
    \begin{equation*}
      D(x,y,p)\defeq (\opp{\opp{p}}= p).
    \end{equation*}
    Then $D(x,x,\refl{x})$ is the type $(\opp{\opp{\refl x}}=\refl{x})$.
    But since $\opp{\refl{x}}\jdeq \refl{x}$ for each $x:A$, we have $\opp{\opp{\refl{x}}}\jdeq \opp{\refl{x}} \jdeq\refl{x}$, and thus $D(x,x,\refl{x})\jdeq(\refl{x}=\refl{x})$.
    Hence we find the function
    \begin{equation*}
      d\defeq\lambda x.\refl{\refl{x}}:\prd{x:A} D(x,x,\refl{x}).
    \end{equation*}
    Now $J$ gives a term $J(D,d,p):\opp{\opp{p}}= p$ for each $p:x= y$.

    \noindent \emph{(Second proof)} By induction, it suffices to assume $p$ is $\refl x$.
    But in this case, we have $\opp{\opp{p}}\jdeq \opp{\opp{\refl x}} \jdeq \refl x$.
  \item \emph{(First proof)} Let $D_1:\prd{x,y:A}{p:x=y} \type$ be the type family given by
    \begin{equation*}
      D_1(x,y,p)\defeq\prd{z,w:A}{q:y= z}{r:z= w} \big(p\ct (q\ct r)=  (p\ct q)\ct r\big).
    \end{equation*}
    Then $D_1(x,x,\refl{x})$ is
    \begin{equation*}
      \prd{z,w:A}{q:x= z}{r:z= w} \big(\refl{x}\ct(q\ct r)= (\refl{x}\ct q)\ct r\big).
    \end{equation*}
    To construct a term in this type, let $D_2:\prd{x,z:A}{q:x=z} \type$ be the type family
    \begin{equation*}
      D_2 (x,z,q) \defeq \prd{w:A}{r:z=w} \big(\refl{x}\ct(q\ct r)= (\refl{x}\ct q)\ct r\big).
    \end{equation*}
    Then $D_2(x,x,\refl{x})$ is
    \begin{equation*}
      \prd{w:A}{r:x=w} \big(\refl{x}\ct(\refl{x}\ct r)= (\refl{x}\ct \refl{x})\ct r\big).
    \end{equation*}
    To construct a term in \emph{this} type, let $D_3:\prd{x,w:A}{r:x=w} \type$ be the type family
    \begin{equation*}
      D_3(x,w,r) \defeq \big(\refl{x}\ct(\refl{x}\ct r)= (\refl{x}\ct \refl{x})\ct r\big).
    \end{equation*}
    Then $D_3(x,x,\refl{x})$ is
    \begin{equation*}
      \big(\refl{x}\ct(\refl{x}\ct \refl{x})= (\refl{x}\ct \refl{x})\ct \refl{x}\big)
    \end{equation*}
    which is definitionally equal to the type $(\refl{x} = \refl{x})$, and is therefore inhabited by $\refl{\refl{x}}$.
    Applying the identity elimination rule three times, therefore, we obtain a term of the overall desired type.

    \noindent \emph{(Second proof)} By induction, it suffices to assume $p$, $q$, and $r$ are all $\refl x$.
    But in this case, we have
    \begin{align*}
      p\ct (q\ct r)
      &\jdeq \refl{x}\ct(\refl{x}\ct \refl{x})\\
      &\jdeq \refl{x}\\
      &\jdeq (\refl{x}\ct \refl x)\ct \refl x\\
      &\jdeq (p\ct q)\ct r.
    \end{align*}
    Thus, we have $\refl{\refl{x}}$ inhabiting this type.\qedhere
  \end{enumerate}
\end{proof}

\begin{rmk}
  There are other ways to define all of these higher paths.
  For instance, in \autoref{thm:omg}\ref{item:omg4} we might do induction only over one or two paths rather than all three.
  Each possibility will produce a \emph{definitionally} different proof, but they will all be equal to each other.
  Such an equality between any two particular proofs can, again, be proven by induction, reducing all the paths in question to reflexivities and then observing that both proofs reduce themselves to reflexivities.
\end{rmk}

\section{Functions are functors}
\label{sec:functors}

Now we wish to establish that functions $f:A\to B$ behave functorially on paths.
In traditional type theory, this is equivalently the statement that functions respect equality.
Topologically, this corresponds to saying that every function is ``continuous'', i.e.\ preserves paths.

\begin{lem}\label{lem:map}
  Suppose that $f:A\to B$ is a function and that $p:(\id[A]xy)$.
  Then for any $x,y:A$ there is an operation
  \begin{equation*}
    \apfunc f : (x=y) \to (f(x)= f(y)).
  \end{equation*}
  Moreover, for each $x:A$ we have $\apfunc{f}(\refl{x})\jdeq \refl{f(x)}$.
\end{lem}

The notation $\apfunc f$ can be read either as the \underline{ap}plication of $f$ to a path, or as the \underline{a}ction on \underline{p}aths of $f$.

\begin{proof}[First proof]
  Let $D:\prd{x,y:A}{p:x=y}\type$ be the type family defined by
  \[D(x,y,p)\defeq (f(x)= f(y)).\]
  Then we have
  \begin{equation*}
    d\defeq\lambda x.\refl{f(x)}:\prd{x:A} D(x,x,\refl{x}).
  \end{equation*}
  Applying $J$, we obtain $\apfunc f : \prd{x,y:A}{p:x=y}(f(x)=g(x))$.
  The conversion rule implies $\apfunc f({\refl{x}})\jdeq\refl{f(x)}$ for each $x:A$.
\end{proof}

\begin{proof}[Second proof]
  By induction, it suffices to assume $p$ is $\refl{x}$.
  In this case, we may define $\apfunc f(p) \defeq \refl{f(x)}:f(x)\jdeq f(x)$.
\end{proof}

We will often write $\apfunc f (p)$ as simply $\ap f p$.
This is strictly speaking ambiguous, but generally no confusion arises.
It matches the common convention in category theory of using the same symbol for the application of a functor to objects and to morphisms.

We note that $\apfunc{}$ behaves functorially, in all the ways that one might expect.

\begin{lem}\label{lem:ap-functor}
  For functions $f:A\to B$ and $g:B\to C$ and paths $p:\id[A]xy$ and $q:\id[b]yz$, we have:
  \begin{enumerate}
  \item $\apfunc f(p\ct q) = \apfunc f(p) \ct \apfunc f(q)$.\label{item:apfunctor-ct}
  \item $\apfunc f(\opp p) = \opp{\apfunc f (p)}$.\label{item:apfunctor-opp}
  \item $\apfunc g (\apfunc f(p)) = \apfunc{g\circ f} (p)$.\label{item:apfunctor-compose}
  \end{enumerate}
\end{lem}
\begin{proof}
  Left to the reader.
\end{proof}

Now, since \emph{dependently typed} functions are very important in type theory, we will also need a version of \autoref{lem:map} for these.
However, this is not quite so simple to state, because if $f:\prd{x:A} B(x)$ and $p:x=y$, then $f(x):B(x)$ and $f(y):B(y)$ are elements of distinct types, so that \emph{a priori} we cannot even ask whether they are equal.
The missing ingredient is that $p$ itself gives us a way to relate the types $B(x)$ and $B(y)$.

\begin{lem}[Transport]\label{lem:transport}
  Suppose that $P$ is a type family over $A$ and that $p:\id[A]xy$.
  Then there is a function $\transf{p}:P(x)\to P(y)$.
\end{lem}

\begin{proof}[First proof]
  Let $D:\prd{x,y:A}{p:\id{x}{y}} \type$ be the type family defined by
  \[D(x,y,p)\defeq P(x)\to P(y).\]
  Then we have the function
  \begin{equation*}
    d\defeq\lambda x.\idfunc[P(x)]:\prd{x:A} D(x,x,\refl{x}),
  \end{equation*}
  so that the induction principle gives us $J_{D,d}(x,y,p):P(x)\to P(y)$ for $p:x= y$, which we define to be $\transf p$.
\end{proof}

\begin{proof}[Second proof]
  By induction, it suffices to assume $p$ is $\refl x$.
  But in this case, we can take $\transf{(\refl x)}:P(x)\to P(x)$ to be the identity function.
\end{proof}

Sometimes, it is necessary to notate the type family $P$ in which the transport operation happens.
In this case, we may write
\[\transfib P p - : P(x) \to P(y).\]

Recall that a type family $P$ over a type $A$ can be seen as a property of elements of $A$, which holds at $x$ in $A$ if $P(x)$ is inhabited.
Then the transportation lemma says that if $x$ is equal to $y$, then $P(x)$ holds if and only if $P(y)$ holds.
In fact, we will see later on that if $x=y$ then actually $P(x)$ and $P(y)$ are \emph{equivalent}.

Topologically, the transportation lemma can be viewed as a ``path lifting'' operation in a fibration.
Given a path $p:x=y$ in the base space $A$ and a point $u:P(x)$ in the fiber over $x$, we may lift the path $p$ to a path in the total space starting at $u$.
The point $\trans p u$ is the other endpoint of this lifted path.
We can also define the path itself in type theory:

\begin{thm}[Path lifting property]\label{thm:path-lifting}
Let $P:A\to\type$ be a type family over $A$ and assume we have $u:P(x)$ for $x:A$. Then we have the identity
\begin{equation*}
\mathsf{lift}(u,p):(x,u)=(y,\trans{p}{u})
\end{equation*}
in $\sm{x:A}P(x)$ for any $p:x=y$.
\end{thm}

\begin{proof}[First proof]
Let $D:\prd{x,y:A}{p:x=y}\type$ be defined by
\begin{equation*}
D(x,y,p)\defeq (x,u)=(y,\trans{p}{u}).
\end{equation*}
Then $D(x,x,\refl{x})\defeq (x,u)=(x,\trans{\refl{x}}{u})$. By the conversion rule we have $\trans{\refl{x}}{u}\defeq u$, so we see that $D(x,x,\refl{x})\defeq (x,u)=(x,u)$. Therefore we find $d(x)\defeq\refl{(x,u)}:D(x,x,\refl{x})$. Now $J$ gives a term of type $\prd{x,y:A}{p:x=y}(x,u)=(y,\trans{p}{u})$.
\end{proof}
\begin{proof}[Second proof] 
  By induction, it suffices to find a term of type $(x,u)=(x,\trans{\refl{x}}{u})$.
  Note that $\trans{\refl{x}}{u}\jdeq u$, so we really need to find a term of type $(x,u)=(x,u)$.
  But here we can take the reflexivity term.
\end{proof}

Now we can prove the dependent version of \autoref{lem:map}.
The topological intuition is that given $f:\prd{x:A} P(x)$ and a path $p:\id[A]xy$, we ought to be able to apply $f$ to $p$ and obtain a path in the total space of $P$ which ``lies over'' $p$, as shown below.

\begin{center}
  \begin{tikzpicture}[yscale=.5,xscale=2]
    \draw (0,0) arc (-90:170:1cm) node[anchor=south east] {$A$} arc (170:270:1cm);
    \draw (0,4) arc (-90:170:1cm) node[anchor=south east] {$\Sigma P$} arc (170:270:1cm);
    \draw[->] (0,3.8) -- node[auto] {$\proj1$} (0,2.2);
    \node[circle,fill,inner sep=1pt,label=left:{$x$}] (b1) at (-.5,1) {};
    \node[circle,fill,inner sep=1pt,label=right:{$y$}] (b2) at (.5,1) {};
    \draw[decorate,decoration={snake,amplitude=1}] (b1) -- node[auto,swap] {$p$} (b2);
    \node[circle,fill,inner sep=1pt,label=left:{$f(x)$}] (b1) at (-.5,5) {};
    \node[circle,fill,inner sep=1pt,label=right:{$f(y)$}] (b2) at (.5,5) {};
    \draw[decorate,decoration={snake,amplitude=1}] (b1) -- node[auto] {$f(p)$} (b2);
  \end{tikzpicture}
\end{center}

We \emph{can} obtain such a thing from \autoref{lem:map}.
Given $f:\prd{x:A} P(x)$, we can define a non-dependent function $f':A\to \sm{x:A} P(x)$ by setting $f'(x)\defeq (x,f(x))$, and then consider $\ap{f'}{p} : f'(x) = f'(y)$.
However, it is not obvious from the type of such a path that it lies over a specific path in $A$ (in this case, $p$), which is sometimes important.

The solution is to use the transport lemma.
Since there is a canonical path from $u:P(x)$ to $\trans p u :P(y)$ which (at least intuitively) lies over $p$, any path from $u$ to $v:P(y)$ lying over $p$ should factor through this path, essentially uniquely, by a path from $\trans p u$ to $v$ lying entirely in the fiber $P(y)$.
Thus, up to equivalence, it makes sense to define ``a path from $u$ to $v$ lying over $p:x=y$'' to mean a path $\trans p u = v$ in $P(y)$.
And, indeed, we can show that dependent functions produce such paths.

\begin{lem}[Dependent map]\label{lem:mapdep}
  Suppose $f:\prd{x: A} P(x)$; then we have
  \[\apdfunc f :(x=y) \to \big(\id[P(y)]{\trans p{f(x)}}{f(y)}\big).\]
\end{lem}

\begin{proof}[First proof]
  Let $D:\prd{x,y:A}{p:\id{x}{y}} \type$ be the type family defined by
  \begin{equation*}
    D(x,y,p)\defeq \trans p {f(x)}= f(y).
  \end{equation*}
  Then $D(x,x,\refl{x})$ is $\trans{(\refl{x})}{f(x)}= f(x)$.
  But since $\trans{(\refl{x})}{f(x)}\jdeq f(x)$, we get that $D(x,x,\refl{x})\jdeq (f(x)= f(x))$.
  Thus, we find the term
  \begin{equation*}
    d\defeq\lambda x.\refl{f(x)}:\prd{x:A} D(x,x,\refl{x})
  \end{equation*}
  and now $J$ gives us $\apdfunc f(p):\trans p{f(x)}= f(y)$ for each $p:x= y$.
\end{proof}

\begin{proof}[Second proof]
  By induction, it suffices to assume $p$ is $\refl x$.
  But in this case, the desired equation is $\trans{(\refl{x})}{f(x)}\jdeq f(x)$, which holds judgmentally.
\end{proof}

Recall that a non-dependently typed function $f:A\to B$ is just the special case of a dependently typed function $f:\prd{x:A} P(x)$ when $P$ is a constant type family, $P(x) \defeq B$.
In this case, $\apdfunc{f}$ and $\apfunc{f}$ are closely related, because of the following lemma:

\begin{lem}\label{thm:trans-trivial}
  If $P:A\to\type$ is defined by $P(x) \defeq B$ for a fixed $B:\type$, then for any $x,y:A$ and $p:x=y$ and $b:B$ we have $\transfib P p b = b$.
\end{lem}
\begin{proof}[First proof]
  Fix a $b:B$, and let $D:\prd{x,y:A}{p:\id{x}{y}} \type$ be the type family defined by
  \[ D(x,y,p) \defeq (\transfib P p b = b). \]
  Then $D(x,x,\refl x)$ is $(\transfib P{\refl{x}}{b} = b)$, which is judgmentally equal to $(b=b)$ by the computation rule for transporting.
  Thus, we have the term
  \[ d \defeq \lambda x.\refl{b} : \prd{x:A} D(x,x,\refl x). \]
  Now $J$ gives us a term in $\prd{x,y:A}{p:x=y}(\transfib P p b = b)$, as desired.
\end{proof}
\begin{proof}[Second proof]
  By induction, it suffices to assume $y$ is $x$ and $p$ is $\refl x$.
  But $\transfib P {\refl x} b \jdeq b$, so in this case what we have to prove is $b=b$, and we have $\refl{b}$ for this.
\end{proof}

Thus, by concatenating with the path defined in \autoref{thm:trans-trivial}, for any $x,y:A$ and $p:x=y$ and $f:A\to B$ we have functions
\begin{align}
  \big(f(x) = f(y)\big) &\to \big(\trans{p}{f(x)} = f(y)\big)\label{eq:ap-to-apd}
  \qquad\text{and} \\
  \big(\trans{p}{f(x)} = f(y)\big) &\to \big(f(x) = f(y)\big).\label{eq:apd-to-ap}
\end{align}
In fact, these functions are inverse equivalences (in the sense to be introduced in \S\ref{sec:basics-equivalences}), and they relate $\apfunc f (p)$  to $\apdfunc f (p)$.
Specifically, if we write $\transconst Bpb$ for the path defined in \autoref{thm:trans-trivial}, we have the following.

\begin{lem}\label{thm:apd-const}
  For $f:A\to B$ and $p:\id[A]xy$, we have
  \[ \apdfunc f(p) = \transconst B p{f(x)} \ct \apfunc f (p) \]
\end{lem}
\begin{proof}[First proof]
  Let $D:\prd{x,y:A}{p:\id xy} \type$ be the type family defined by
  \[ D(x,y,p) \defeq \big(\apdfunc f (p) = \transconst Bp{f(x)} \ct \apfunc f (p)\big). \]
  Thus, we have
  \[D(x,x,\refl x) \jdeq \big(\apdfunc f (\refl x) = \transconst B{\refl x}{f(x)} \ct \apfunc f ({\refl x})\big).\]
  But by definition, all three paths appearing in this type are $\refl{f(x)}$, so we have
  \[ \refl{\refl{f(x)}} : D(x,x,\refl x). \]
  Thus, path induction gives us an element of $\prd{x,y:A}{p:x=y} D(x,y,p)$, which is what we wanted.
\end{proof}
\begin{proof}[Second proof]
  By induction, it suffices to assume $y$ is $x$ and $p$ is $\refl x$.
  In this case, what we have to prove is $\refl{f(x)} = \refl{f(x)} \ct \refl{f(x)}$, which is true judgmentally.
\end{proof}

But because the types of $\apdfunc{f}$ and $\apfunc{f}$ are different, it is often clearer to use different notations for them.
We may sometimes use a notation $\apd f p$ for $\apdfunc{f}(p)$, which is similar to the notation $\ap f p$ for $\apfunc{f}$.

\medskip

At this point, we hope the reader is starting to get a feel for proofs by induction on identity types.
From now on we desist from giving both styles of proofs, allowing ourselves to use whatever is most clear and convenient (and often the second, more concise one).


\section{Summary of the basic higher structure}
\label{sec:basics-summary}

Here we summarize the basic definitions made in the previous two sections.

\begin{itemize}
\item $\opp{p} : y=x$, for $p:x=y$, defined by
  \[\opp{\;\refl{x}}\jdeq \refl{x}.\]
\item $p\ct q :y=z$, for $p:x=y$ and $q:y=z$, defined by
  \[ \refl{x}\ct\refl{x}\jdeq\refl{x}.\]
\item If $P$ is a type family over $A$ then $\transf{p}:P(x)\ra P(y)$, for $p:x=y$, defined by
  \[\transf{(\refl{x})}\jdeq \idfunc[P(x)].\]
\item If $f:A\ra B$ then $\map{f}{p}:f(x)=f(y)$, for $p:x=y$, defined by
  \[\map{f}{\refl{x}}\jdeq \refl{f(x)}.\]
\item If $f:\prod_{x:A}P(x)$ then $\mapdep{f}{p}:\trans{p}{f(x)}=f(y)$, for $p:x=y$, defined by
  \[\mapdep{f}{\refl{x}}\jdeq \refl{f(x)}.\]
\end{itemize}


\input{basics-equivalences}

\input{computational}


\section{Universal properties}
\label{sec:universal-properties}

By combining the path computation rules described in \S\ref{sec:computational}, we can show that various type forming operations satisfy the expected universal properties.
For instance, given types $X,A,B$, we have a function
\begin{equation}
  (X\to A\times B) \;\to \; (X\to A)\times (X\to B)\label{eq:prod-ump-map}
\end{equation}
defined by $f \mapsto (\proj1 \circ f, \proj2\circ f)$.

\begin{thm}\label{thm:prod-ump}
  \eqref{eq:prod-ump-map} is an equivalence.
\end{thm}
\begin{proof}
  We define a quasi-inverse to send $(g,h)$ to the function $x\mapsto (g(x),h(x))$.
  (Technically, we have used the induction principle for the cartesian product $(X\to A)\times (X\to B)$, to reduce to the case of a pair.)

  Now given $f:X\to A\times B$, the round-trip composite yields the function
  \begin{equation}
    x\mapsto (\proj1(f(x)),\proj2(f(x))).\label{eq:prod-ump-rt1}
  \end{equation}
  By \autoref{thm:path-prod}, for any $x:X$ we have $(\proj1(f(x)),\proj2(f(x))) = f(x)$.
  Thus, by function extensionality, the function~\eqref{eq:prod-ump-rt1} is equal to $f$.

  On the other hand, given $(g,h)$, the round-trip composite yields the pair $(x\mapsto g(x),x\mapsto h(x))$.
  By function extensionaility, the two components of this are equal to $g$ and $h$ respectively, so by \autoref{thm:path-prod}, the pair is equal to $(g,h)$.
\end{proof}

In fact, we also have a dependently typed version of this universal property.
Suppose given a type $X$ and type families $A,B:X\to \type$.
Then we have a function
\begin{equation}\label{eq:prod-umpd-map}
  \Big(\prd{x:X} (A(x)\times B(x))\Big) \;\to\; \Big(\prd{x:X} A(x)\Big) \times \Big(\prd{x:X} B(x)\Big)
\end{equation}
defined as before by $f \mapsto (\proj1 \circ f, \proj2\circ f)$.

\begin{thm}\label{thm:prod-umpd}
  \eqref{eq:prod-umpd-map} is an equivalence.
\end{thm}
\begin{proof}
  Left to the reader.
\end{proof}

Just as $\Sigma$-types are a generalization of cartesian products, they satisfy a generalized version of this universal property.
Jumping right to the dependently typed version, suppose we have a type $X$ and type families $A:X\to \type$ and $P:\prd{x:X} A(x)\to\type$.
Then we have a function
\begin{equation}
  \label{eq:sigma-ump-map}
  \Big(\prd{x:X} \textstyle\sum_{a:A(x)} P(x,a)\Big)  \;\to\;
  \Big(\sm{g:\textstyle\prod_{x:X} A(x)} \textstyle\prod_{x:X} P(x,g(x))\Big)
\end{equation}
Note that if we have $P(x,a) \defeq B(x)$ for some $B:X\to\type$, then~\eqref{eq:sigma-ump-map} reduces to~\eqref{eq:prod-umpd-map}.

\begin{thm}\label{thm:ttac}
  \eqref{eq:sigma-ump-map} is an equivalence.
\end{thm}
\begin{proof}
  As before, we define a quasi-inverse to send $(g,h)$ to the function $x\mapsto (g(x),h(x))$.
  Now given $f:\prd{x:X} \sm{a:A(x)} P(x,a)$, the round-trip composite yields the function
  \begin{equation}
    x\mapsto (\proj1(f(x)),\proj2(f(x))).\label{eq:prod-ump-rt1}
  \end{equation}
  Now for any $x:X$, by \autoref{thm:eta-sigma} ($\eta$-equivalence for $\Sigma$-types) we have $(\proj1(f(x)),\proj2(f(x))) = f(x)$.
  Thus, by function extensionality,~\eqref{eq:prod-ump-rt1} is equal to $f$.

  On the other hand, given $(g,h)$, the round-trip composite yields the pair $(x\mapsto g(x),x\mapsto h(x))$.
  But $x\mapsto g(x)$ and $x\mapsto h(x)$ are judgmentally equal to $g$ and $h$, respectively, and hence this pair of functions is also equal to $(g,h)$.
\end{proof}

This is noteworthy because the propositions-as-types interpretation of~\eqref{eq:sigma-ump-map} is ``the axiom of choice''.
If we read $\Sigma$ as ``there exists'' and $\Pi$ (sometimes) as ``for all'', we can pronounce:
\begin{itemize}
\item $\prd{x:X} \sm{a:A(x)} P(x,a)$ as ``for all $x:X$ there exists an $a:A(x)$ such that $P(x,a)$'', and
\item $\sm{g:\prd{x:X} A(x)} \prd{x:X} P(x,g(x))$ as ``there exists a choice function $g:\prd{x:X} A(x)$ such that for all $x:X$ we have $P(x,g(x))$''.
\end{itemize}
Thus, \autoref{thm:ttac} says that not only is the axiom of choice ``true'', it hypotheses are equivalent to its conclusion.
(On the other hand, it should also be clear to the classical mathematician that~\eqref{eq:sigma-ump-map} does not carry the intended meaning of the axiom of choice, since we have specified the values of $g$ already and there are no choices left to be made.
We will return to this point in \S\ref{sec:logic}.

The above universal property for pair types is for ``mapping in'', which is familiar from the category-theoretic notion of products.
However, pair types also have a universal property for ``mapping out'', which may look less familiar.
In the case of cartesian products, the non-dependent version simply expresses the cartesian closedness adjunction:
\[ (A\times B) \to C \;\simeq\; A\to (B\to C).\]
The dependent version of this is formulated for a type family $C:A\times B\to \type$:
\[ \prd{w:A\times B} C(w) \;\simeq\; \prd{x:A}{y:B} C(x,y). \]
Here the left-to-right function is simply the induction principle for $A\times B$, while the right-to-left is evaluation at a pair.
We leave it to the reader to prove that these are quasi-inverses.
There is also a version for $\Sigma$-types:
\begin{equation}
  \prd{w:\textstyle\sum_{x:A} B(x)} C(w) \;\simeq\; \prd{x:A}{y:B(x)} C(x,y)\label{eq:sigma-lump}
\end{equation}
Again, the left-to-right function is the induction principle.

Some other induction principles are also part of universal properties of this sort.
For instance, path induction is the right-to-left direction of an equivalence as follows:
\begin{equation}
  \label{eq:path-lump}
  \prd{x:A}{p:a=x} B(x,p) \;\simeq\; B(a,\refl a)
\end{equation}
for any $a:A$ and type family $B:\prd{x:A} (a=x) \to\type$.
However, inductive types with recursion, like the natural numbers, have more complicated universal properties; see Chapter~\ref{cha:induction}.


\section{Sets}
\label{sec:basics-sets}

While types in general behave like higher groupoids, there is a subclass of them that behave more like the sets in a traditional set-theoretic system.
Categorically, we may consider \emph{discrete} groupoids, which are determined by a set of objects and only identity morphisms and higher morphisms, while topologically we may consider sets with the discrete topology.
More generally, we may consider groupoids or spaces that are \emph{equivalent} to ones of this sort; since everything we do in type theory is up to homotopy, we can't expect to tell the difference.

Intuitively, we would expect a type to ``be a set'' in this sense if it has no higher homotopy information: any two parallel paths are equal (up to homotopy), and similarly for parallel higher paths at all dimensions.
Fortunately, because everything in homotopy type theory is automatically functorial/continuous, it turns out to be sufficient to ask this at the bottom level.

\begin{defn}\label{defn:set}
  A type $A$ is a \textbf{set} if for all $x,y:A$ and all $p,q:x=y$, we have $p=q$.
\end{defn}

More precisely, the proposition $\isset(A)$ is defined to be the type
\[ \isset(A) \defeq \prd{x,y:A}{p,q:x=y} (p=q). \]
In Chapter~\ref{cha:hlevels} we will make precise the sense in which this ``suffices for all higher levels'', but as an example, we observe that it suffices for the next level up.

\begin{lem}\label{thm:isset-is1type}
  If $A$ is a set (that is, $\isset(A)$ is inhabited), then for any $x,y:A$ and $p,q:x=y$ and $r,s:p=q$, we have $r=s$.
\end{lem}
\begin{proof}
  Suppose $f:\isset(A)$; then for any $x,y:A$ and $p,q:x=y$ we have $f(x,y,p,q):p=q$.
  Fix $x$, $y$, and $p$, and define $g: \prd{q:x=y} (p=q)$ by $g(q) \defeq f (x,y,p,q)$.
  Then for any $r:q=q'$, we have $\apdfunc{g}(r) : \trans{r}{g(q)} = g(q')$.
  By \autoref{cor:transport-path-prepost}, therefore, we have $g(q) \ct r = g(q')$.

  In particular, suppose given $x,y,p,q,r,s$ as in the lemma statement, and define $g$ as above.
  Then $g(p) \ct r = g(q)$ and also $g(p) \ct s = g(q)$, hence by cancellation $r=s$.
\end{proof}

As mentioned in \S\ref{sec:types-vs-sets},
the sets in homotopy type theory are not like the sets in ZF set theory, in that there is no global ``membership predicate'' $\in$.
They are more like the sets used in structural mathematics and in category theory, whose elements are ``abstract points'' to which we give structure with functions and relations.
This is all we need in order to use them as a foundational system for most set-based mathematics; we will see some examples in Chapter~\ref{cha:set-math}.

Which types are sets?
In Chapter~\ref{cha:hlevels} we will study a more general form of this question in depth, but for now we can observe some easy examples.

\begin{eg}
  The type \unit is a set.
  For by \autoref{thm:path-unit}, for any $x,y:\unit$ we have $\eqv{(x=y)}{\unit}$.
  Since any two elements of \unit are equal, this implies that any two elements of $x=y$ are equal.
\end{eg}

\begin{eg}
  The type $\emptyt$ is a set, for given any $x,y:\emptyt$ we may deduce anything we like by contradiction.
\end{eg}

\begin{eg}
  The type \nat of natural numbers is also a set.
  This follows from \autoref{thm:path-nat}, since all equality types $\id[\nat]xy$ are equivalent to either \unit or \emptyt, and any two inhabitants of \unit or \emptyt are equal.
  We will see another proof of this fact in Chapter~\ref{cha:hlevels}.
\end{eg}

Most of the type forming operations we have considered so far also preserve sets.

\begin{eg}\label{thm:isset-prod}
  If $A$ and $B$ are sets, then so is $A\times B$.
  For given $x,y:A\times B$ and $p,q:x=y$, by \autoref{thm:path-prod} we have $p= \pairpath(\projpath1(p),\projpath2(p))$ and $q= \pairpath(\projpath1(q),\projpath2(q))$.
  But $\projpath1(p)=\projpath1(q)$ since $A$ is a set, and $\projpath2(p)=\projpath2(q)$ since $B$ is a set; hence $p=q$.

  Similarly, if $A$ is a set and $B:A\to\type$ is such that each $B(x)$ is a set, then $\sm{x:A} B(x)$ is a set.
\end{eg}

\begin{eg}\label{thm:isset-forall}
  If $A$ is \emph{any} type and $B:A\to \type$ is such that each $B(x)$ is a set, then the type $\prd{x:A} B(x)$ is a set.
  For suppose $f,g:\prd{x:A} B(x)$ and $p,q:f=g$.
  By function extensionality, we have $p = {\funext (x \mapsto \happly(p,x))}$ and likewise $q = {\funext (x \mapsto \happly(q,x))}$.
  But for any $x:A$, we have $\happly(p,x):f(x)=g(x)$ and also $\happly(q,x):f(x)=g(x)$, so since $B(x)$ is a set we have $\happly(p,x) = \happly(q,x)$.
  Now using function extensionality again, the dependent functions $(x \mapsto \happly(p,x))$ and $(x \mapsto \happly(q,x))$ are equal, and hence (applying $\apfunc{\funext}$) so are $p$ and $q$.
\end{eg}

For more examples, see Exercises~\ref{ex:isset-coprod} and~\ref{ex:isset-sigma}.
However, not all types are sets.

\begin{eg}\label{thm:type-is-not-a-set}
  The universe \type is not a set.
  To prove this, it suffices to exhibit a type $A$ and a path $p:A=A$ which is not equal to $\refl A$.
  Take $A=\bool$, and let $f:A\to A$ be defined by $f(\btrue)\defeq \bfalse$ and $f(\bfalse)\defeq \btrue$.
  Then $f(f(x))=x$ for all $x$ (by an easy case analysis), so $f$ is an equivalence.
  Hence, by univalence, $f$ gives rise to a path $p:A=A$.

  If $p$ were equal to $\refl A$, then (again by univalence) $f$ would equal the identity function of $A$.
  But this would imply that $\btrue=\bfalse$, contradicting \autoref{rmk:true-neq-false}.
\end{eg}

We will study other types that are not sets in more detail starting in Chapter~\ref{cha:hits}.
An even more special kind of type than a set is a set with at most one element.
Such types may be regarded as \emph{propositions in a narrow sense}, and their study is just what is usually called "logic".

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Logic}
\label{sec:logic}

% This implies in particular that the usual method of \emph{proof by contradiction} cannot in general be used to establish a positive statement, but only one of the form ``it's not the case that \dots''.

Type theory, formal or informal, is a collection of rules for manipulating types and their elements.
But when writing mathematics informally in human language, we generally use familiar words, particularly logical connectives such as ``and'' and ``or'', and logical quantifiers such as ``for all'' and ``there exists''.
In contrast to set theory, type theory offers us more than one choice for how to regard these English phrases as operations on types.
This potential ambiguity needs to be resolved, by setting out local or global conventions, by introducing new annotations to informal mathematics, or both.
This requires some getting used to, but is offset by the fact that because type theory permits this finer analysis of logic, we can represent mathematics more faithfully, with fewer ``abuses of language'' than in set-theoretic foundations.
In this section we will explain the issues involved, and justify the choices we have made.


\subsection{Propositions as types?}
\label{subsec:pat?}

Until now, we have been following the straightforward ``propositions as types'' philosophy described in \S\ref{sec:pat}, according to which English phrases such as ``there exists an $x:A$ such that $P(x)$'' are interpreted by corresponding types such as $\sm{x:A} P(x)$, with the proof of a statement being regarded as judging some specific term to inhabit that type.
However, we have also seen some ways in which the ``logic'' resulting from this reading seems unfamiliar to a classical mathematician.
For instance, in \autoref{thm:ttac} we saw that the statement
\begin{equation}\label{eq:english-ac}
  \parbox{\textwidth-2cm}{``If for all $x:X$ there exists an $a:A(x)$ such that $P(x,a)$, then there exists a function $g:\prd{x:A} A(x)$ such that for all $x:X$ we have $P(x,g(x))$,''}
\end{equation}
which looks like the classical \emph{axiom of choice}, is always true under this reading.  That's a good thing, but rather unexpected, since the axiom of choice is not generally thought to be a law of logic.

On the other hand, we can now show that corresponding statements looking like the classical \emph{law of double negation} and \emph{law of excluded middle} are incompatible with the univalence axiom.

\begin{thm}\label{thm:not-dneg}
  It is not the case that for all $A:\UU$ we have $\neg(\neg A) \to A$.
\end{thm}
\begin{proof}
  Recall that $\neg A \jdeq (A\to\emptyt)$.
  We also read ``it is not the case that \dots'' as the operator $\neg$.
  Thus, in order to prove this statement, it suffices to assume given some $f:\prd{A:\UU} (\neg\neg A \to A)$ and construct an element of \emptyt.

  The idea of the following proof is to observe that $f$, like any function, is automatically ``continuous'' with respect to paths in its domain.
  By univalence, this implies that $f$ is \emph{natural} with respect to equivalences of types.
  From this, and a fixed-point-free autoequivalence, we will be able to extract a contradiction.

  Let $e:\eqv\bool\bool$ be the equivalence defined by $e(\bfalse)\defeq\btrue$ and $e(\btrue)\defeq\bfalse$, as in \autoref{thm:type-is-not-a-set}.
  Let $p:\bool=\bool$ be the path corresponding to $e$ by univalence, i.e.\ $p\defeq \ua(e)$.
  Then we have $f(\bool) : \neg\neg\bool \to\bool$ and
  \[\apd f p : \transfib{A\mapsto (\neg\neg A \to A)}{p}{f(\bool)} = f(\bool).\]
  Hence, for any $u:\neg\neg\bool$, we have
  \[\happly(\apd f p,u) : \transfib{A\mapsto (\neg\neg A \to A)}{p}{f(\bool)}(u) = f(\bool)(u).\]

  Now by~\eqref{eq:transport-arrow}, transporting $f(\bool):\neg\neg\bool\to\bool$ along $p$ in the type family ${A\mapsto (\neg\neg A \to A)}$ is equal to the function which transports its argument along $\opp p$ in the type family $A\mapsto \neg\neg A$, applies $f(\bool)$, then transports the result along $p$ in the type family $A\mapsto A$:
  \[ \transfib{A\mapsto (\neg\neg A \to A)}{p}{f(\bool)}(u) =
  \transfib{A\mapsto A}{p}{f(\bool) (\transfib{A\mapsto \neg\neg A}{\opp{p}}{u})}
  \]
  However, any two points $u,v:\neg\neg\bool$ are equal by function extensionality, since for any $x:\neg\bool$ we have $u(x):\emptyt$ and thus we can derive any conclusion, in particular $u(x)=v(x)$.
  Thus, we have $\transfib{A\mapsto \neg\neg A}{\opp{p}}{u} = u$, and so from $\happly(\apd f p,u)$ we obtain an equality
  \[ \transfib{A\mapsto A}{p}{f(\bool)(u)} = f(\bool)(u).\]
  Finally, as discussed in \S\ref{sec:compute-universe}, transporting in the type family $A\mapsto A$ along the path $p\jdeq \ua(e)$ is equivalent to applying the equivalence $e$; thus we have
  \begin{equation}
    e(f(\bool)(u)) = f(\bool)(u).\label{eq:fpaut}
  \end{equation}

  However, we can also prove that
  \begin{equation}
    \prd{x:\bool} \neg(e(x)=x).\label{eq:fpfaut}
  \end{equation}
  This follows from a case analysis on $x$: both cases are immediate from the definition of $e$ and the injectivity of $\inl$ and $\inr$ which we proved in \S\ref{sec:compute-coprod}.
  Thus, applying~\eqref{eq:fpfaut} to $f(\bool)(u)$ and~\eqref{eq:fpaut}, we obtain an element of $\emptyt$.
\end{proof}

\begin{rmk}
  In particular, this implies that there can be no Hilbert-style ``choice operator'' which selects an element of every nonempty type.
  The point is that no such operator can be \emph{natural}, and under the univalence axiom, all functions acting on types must be natural with respect to equivalences.
\end{rmk}

\begin{cor}\label{thm:not-lem}
  It is not the case that for all $A:\UU$ we have $A+(\neg A)$.
\end{cor}
\begin{proof}
  Suppose we had $g:\prd{A:\UU} (A+(\neg A))$.
  We will show that then $\prd{A:\UU} (\neg\neg A \to A)$, so that we can apply \autoref{thm:not-dneg}.
  Thus, suppose $A:\UU$ and $u:\neg\neg A$; we want to construct an element of $A$.

  Now $g(A):A+(\neg A)$, so by case analysis, we may assume either $g(A)\jdeq \inl(a)$ for some $a:A$, or $g(A)\jdeq \inr(w)$ for some $w:\neg A$.
  In the first case, we have $a:A$, while in the second case we have $u(w):\emptyt$ and so we can obtain anything we wish (such as $A$).
  Thus, in both cases we have an element of $A$, as desired.
\end{proof}

Thus, if we want to assume the univalence axiom (which, of course, we do) and still leave ourselves the option of classical reasoning (which is also desirable), we cannot use the unmodified propositions-as-types principle to interpret \emph{all} informal mathematical statements into type theory, since then the law of excluded middle would be false.
However, neither do we want to discard propositions-as-types entirely, because of its many good properties (such as simplicity, constructivity, and computability).
We now discuss a modification of propositions-as-types which resolves these problems; in \S\ref{subsec:when-trunc} we will return to the question of which logic to use when.


\subsection{Mere propositions}
\label{subsec:hprops}

The good and bad things about propositions-as-types logic have a common cause: when types are viewed as propositions, they can contain more information than mere truth or falsity, and all ``logical'' constructions on them must respect this additional information.
This suggests that we could obtain a more conventional logic by restricting attention to types that do \emph{not} contain any more information than truth or falsity, and only regarding these as logical propositions,.

Such a type $A$ will be ``true'' if it is inhabited, and ``false'' if its inhabitation yields a contradiction (i.e.\ if $\neg A \jdeq (A\to\emptyt)$ is inhabited).
What we want to avoid, in order to obtain a more traditional sort of logic, is treating as logical propositions those types for which giving an element of them gives more information than simply knowing that the type is inhabited.
For instance, if we are given an element of \bool, then we receive more information than the mere fact that \bool contains some element.
Indeed, we receive exactly \emph{one bit} more information: we know \emph{which} element of \bool we were given.
By contrast, if we are given an element of \unit, then we receive no more information than the mere fact that \unit contains an element, since any two elements of \unit are equal to each other.
This suggests the following definition.

\begin{defn}
  A type $P$ is a \textbf{mere proposition} if for all $x,y:P$ we have $x=y$.
\end{defn}

Note that since we are still doing mathematics \emph{in} type theory, this is a definition \emph{in} type theory, which means it is a type --- or, rather, a type family.
Specifically, for any $P:\type$, the type $\isprop(P)$ is defined to be
\[ \isprop(P) \defeq \prd{x,y:P} (x=y). \]
Thus, to assert that ``$P$ is a mere proposition'' means to exhibit an inhabitant of $\isprop(P)$, which is a dependent function connecting any two elements of $P$ by a path.
The continuity/naturality of this function implies that not only are any two elements of $P$ equal, but $P$ contains no higher homotopy either.

\begin{lem}\label{thm:inhabprop-eqvunit}
  If $P$ is a mere proposition and $x_0:P$, then $\eqv P \unit$.
\end{lem}
\begin{proof}
  Define $f:P\to\unit$ by $f(x)\defeq \ttt$, and $g:\unit\to P$ by $g(u)\defeq x_0$.
  Then for any $u:\unit$ we have $\eqv{(f(g(u))=u)}{\unit}$, hence $f(g(u))=u$, while for any $x:P$ we have $g(f(x))=x$ since $P$ is a mere proposition.
\end{proof}

In homotopy theory, a space that is homotopy equivalent to \unit is said to be \emph{contractible}.
Thus, any mere proposition which is inhabited is contractible (see also \S\ref{sec:contractibility}).
On the other hand, the uninhabited type \emptyt is also (vacuously) a mere proposition.
In classical mathematics, at least, these are the only two possibilities.

Mere propositions are also called \emph{subterminal objects} (if thinking categorically), \emph{subsingletons} (if thinking set-theoretically), or \emph{h-propositions}.
In Chapter~\ref{cha:hlevels} we will learn to also call them \emph{$(-1)$-truncated types}.
The adjective ``mere'' emphasizes that although any type may be regarded as a proposition (which we prove by giving an inhabitant of it), a type that is a mere proposition cannot usefully be regarded as any \emph{more} than a proposition: there is no additional information contained in a witness of its truth.

Note that a type $A$ is a set if and only if for all $x,y:A$, the identity type $\id[A]xy$ is a mere proposition.
On the other hand, by copying (and simplifying) the proof of \autoref{thm:isset-is1type}, we see that every mere proposition is a set.
We have seen one other example so far: condition~\ref{item:be3} in \S\ref{sec:basics-equivalences} asserts that for any function $f$, the type $\isequiv (f)$ should be a mere proposition.


\subsection{Classical vs.\ intuitionistic logic}
\label{sec:intuitionism}

With the notion of mere proposition in hand, we can now give the proper formulation of the \emph{law of excluded middle} in homotopy type theory:
\begin{equation}
  \label{eq:lem}
  \mathsf{LEM}\;\defeq\;
  \prd{A:\UU} \Big(\isprop(A) \to (A + \neg A)\Big).
\end{equation}
Similarly, the \emph{law of double negation} is
\begin{equation}
  \label{eq:ldn}
  \mathsf{DN}\;\defeq\;
  \prd{A:\UU} \Big(\isprop(A) \to (\neg\neg A \to A)\Big).
\end{equation}
These formulations avoid the paradoxes of \autoref{thm:not-dneg} and \autoref{thm:not-lem}, since \bool is not a mere proposition.
Although they are not consequences of the basic type theory described in Chapter~\ref{cha:typetheory}, they may be consistently assumed as axioms.
For instance, we will assume them in \S\ref{sec:wellorderings}.
(The two are also easily seen to be equivalent to each other; see \autoref{ex:lem-ldn}.)

However, it can be surprising how far we can get without using such axioms.
Quite often, a simple reformulation of a definition or theorem enables us to avoid invoking excluded middle or double negation.
While this takes a little getting used to sometimes, it is often worth the hassle, resulting in more elegant and more general proofs.

For instance, in classical mathematics, double negations are frequently used unnecessarily.
A very simple example is the common assumption that a set $A$ is ``nonempty'', which literally means it is \emph{not} the case that $A$ contains \emph{no} elements.
Almost always what is really meant is the positive assertion that $A$ \emph{does} contain at least one element, and by removing the double negation we make the statement less dependent on LEM.
We will say that a type $A$ is \textbf{inhabited} to mean that we assert $A$ itself as a proposition (i.e.\ we construct a term belonging to $A$, usually unnamed).\footnote{We've been using this already, so probably it should be mentioned in Chapter\S\ref{cha:typetheory}.}

Similarly, it is not uncommon in classical mathematics to find unnecessary proofs by contradiction.
Of course, proof by contradiction proceeds by way of the law of double negation: we assume $\neg A$ and derive a contradiction, thereby deducing $\neg \neg A$, and thus by DN we obtain $A$.
However, often the derivation of a contradiction from $\neg A$ can be rephrased slightly so as to yield a direct proof of $A$, avoiding the need for DN.

It is also important to note that if the goal is to prove a \emph{negation}, then ``proof by contradiction'' does not involve DN.
In fact, since $\neg A$ is by definition the type $A\to\emptyt$, by definition to prove $\neg A$ is to prove a contradiction (\emptyt) under the assumption of $A$.
Similarly, the law of double negation does hold for negated propositions: $\neg\neg\neg A \to \neg A$.
With practice, one learns to distinguish more carefully between negated and un-negated propositions and to notice when LEM and DN are being used and when they are not.

Thus, contrary to how it may appear on the surface, doing mathematics ``constructively'' does not usually involve giving up important theorems, but rather finding the best way to state the definitions so as to make the important theorems constructively provable.
That is, the LEM is a crutch on which we may rely when first investigating a subject, but once that subject is better understood, we can refine the definitions and proofs so as to do without that crutch.
For instance, the theory of ordinal numbers, which classically makes heavy use of LEM, works quite well constructively once we choose the correct definition of ``ordinal''; see \S\ref{sec:ordinals}.

Finally, this sort of observation is even more pronounced in \emph{homotopy} type theory, where the powerful tools of univalence and higher inductive types allow us to constructively attack many problems that traditionally would require classical reasoning.
For instance, none of the ``synthetic'' homotopy theory we will develop in Chapter~\ref{cha:homotopy} requires LEM or DN --- despite the fact that classical homotopy theory (formulated using topological spaces or simplicial sets) makes heavy use of them (as well as the axiom of choice).



\subsection{Subsets}
\label{subsec:prop-subsets}

As another example of the usefulness of mere propositions, we discuss subsets (and more generally subtypes).
Suppose $P:A\to\type$ is a type family, with each type $P(x)$ regarded as a proposition.
Then $P$ itself is a \emph{predicate} on $A$, or a \emph{property} of elements of $A$.

In set theory, whenever we have a predicate on $P$ on a set $A$, we may form the subset $\setof{x\in A | P(x)}$.
In type theory, the obvious analogue is the $\Sigma$-type $\sm{x:A} P(x)$.
An inhabitant of $\sm{x:A} P(x)$ is, of course, a pair $(x,p)$ where $x:A$ and $p$ is a proof of $P(x)$.
However, for general $P$, an element $a:A$ might give rise to more than one distinct element of $\sm{x:A} P(x)$, if the proposition $P(a)$ has more than one distinct proof.
This is counter to the usual intuition of a \emph{subset}.
But if $P$ is a \emph{mere} proposition, then this cannot happen.

\begin{lem}
  Suppose $P:A\to\type$ is a type family such that $P(x)$ is a mere proposition for all $x:A$.
  If $u,v:\sm{x:A} P(x)$ are such that $\proj1(u) = \proj1(v)$, then $u=v$.
\end{lem}
\begin{proof}
  Suppose $p:\proj1(u) = \proj1(v)$.
  By \autoref{thm:path-sigma}, to show $u=v$ it suffices to show $\trans{p}{\proj2(u)} = \proj2(v)$.
  But $\trans{p}{\proj2(u)}$ and $\proj2(v)$ are both elements of $P(\proj1(v))$, which is a mere proposition; hence they are equal.
\end{proof}

For instance, recall that in \S\ref{sec:basics-equivalences} we defined
\[(\eqv A B) \;\defeq\; \sm{f:A\to B} \isequiv (f),\]
where each type $\isequiv (f)$ was supposed to be a mere proposition.
It follows that if two equivalences have equal underlying functions, then they are equal as equivalences.

Henceforth, if $P:A\to \type$ is a family of mere propositions, we will allow ourselves to write $\setof{x:A | P(x)}$ as an alternative notation for $\sm{x:A} P(x)$.


\subsection{The logic of mere propositions}
\label{subsec:logic-hprop}

We mentioned in \S\ref{sec:types-vs-sets} that in contrast to type theory, which has only one basic notion (types), set-theoretic foundations have two basic notions: sets and propositions.
Thus, a classical mathematician is accustomed to manipulating these two kinds of objects separately.

It is possible to recover a similar dichotomy in type theory, with the role of the set-theoretic propositions being played by the types (and type families) that are \emph{mere} propositions.
In many cases, the logical connectives and quantifiers can be represented in this logic by simply restricting the corresponding type-former to the mere propositions.
Of course, this requires knowing that the type-former in question preserves mere propositions.

\begin{eg}
  If $A$ and $B$ are mere propositions, so is $A\times B$.
  This is easy to show using the characterization of paths in products, just like \autoref{thm:isset-prod} but simpler.
  Thus, the connective ``and'' preserves mere propositions.
\end{eg}

\begin{eg}\label{thm:isprop-forall}
  If $A$ is any type and $B:A\to \type$ is such that for all $x:A$, the type $B(x)$ is a mere proposition, then $\prd{x:A} B(x)$ is a mere proposition.
  The proof is just like \autoref{thm:isset-forall} but simpler: given $f,g:\prd{x:A} B(x)$, for any $x:A$ we have $f(x)=g(x)$ since $B(x)$ is a mere proposition.
  But then by function extensionality, we have $f=g$.

  In particular, if $B$ is a mere proposition, then so is $A\to B$ regardless of what $A$ is.
  In even more particular, since \emptyt is a mere proposition, so is $\neg A \jdeq (A\to\emptyt)$.
  Thus, the connectives ``implies'' and ``not'' preserve mere propositions, as does the quantifier ``for all''.
\end{eg}

On the other hand, some type formers do not preserve mere propositions.
Even if $A$ and $B$ are mere propositions, $A+B$ will not in general be.
For instance, \unit is a mere proposition, but $\bool=\unit+\unit$ is not.
Logically speaking, $A+B$ is a ``purely constructive'' sort of ``or'': a witness of it contains the additional information of \emph{which} disjunct is true.
Sometimes this is very useful, but if we want a more classical sort of ``or'' that preserves mere propositions, we need a way to ``truncate'' this type into a mere proposition by forgetting this additional information.

The same issue arises with the $\Sigma$-type $\sm{x:A} P(x)$.
This is a purely constructive interpretation of ``there exists an $x:A$ such that $P(x)$'' which remembers the witness $x$, and hence is not generally a mere proposition even if each type $P(x)$ is.
(Recall that we observed in \S\ref{subsec:prop-subsets} that $\sm{x:A} P(x)$ can also be regarded as ``the subset of those $x:A$ such that $P(x)$''.
The tension between these two interpretations is exactly the point.)


\subsection{Propositional truncation}
\label{subsec:prop-trunc}

The \emph{propositional truncation}, also called the \emph{$(-1)$-truncation}, \emph{bracket type}, or \emph{squash type}, is an additional type former which ``truncates'' or ``squashes'' a type down to a mere proposition, forgetting all information contained in inhabitants of that type other than their existence.

More precisely, for any type $A$, there is a type $\brck{A}$.
It has two constructors:
\begin{itemize}
\item For any $a:A$ we have $\bproj a : \brck A$.
  Thus, if $A$ is inhabited, so is $\brck A$.
\item For any $x,y:\brck A$, we have $x=y$.
  In other words, $\brck A$ is a mere proposition; usually we leave the witness of this fact nameless.
\end{itemize}
The induction principle of $\brck A$ says that:
\begin{itemize}
\item If $B$ is a mere proposition and we have $f:A\to B$, then there is an induced $g:\brck A \to B$ such that $g(\bproj a) \jdeq f(a)$ for all $a:A$.
\end{itemize}
Thus, $\brck A$, as a mere proposition, contains no more information than the inhabitedness of $A$, since any mere proposition which follows from the inhabitedness of $A$ already follows from $\brck A$.

With the propositional truncation, we can extend the ``logic of mere propositions'' to cover disjunction and the existential quantifier.
Specifically, $\brck{A+B}$ is a mere propositional version of ``$A$ or $B$'', which does not ``remember'' the information of which disjunct is true.

The induction principle of truncation implies that we can still do a case analysis on $\brck{A+B}$ \emph{when attempting to prove a mere proposition}.
That is, suppose we have an assumption $u:\brck{A+B}$ and we are trying to prove a mere proposition $Q$.
In other words, we are trying to define an element of $\brck{A+B} \to Q$.
Since $Q$ is a mere propositon, by the induction principle for propositional truncation, it suffices to construct a function $A+B\to Q$.
But now we can use case analysis on $A+B$.

Similarly, for a type family $P:A\to\type$, we can consider $\brck{\sm{x:A} P(x)}$, which is a mere propositional version of ``there exists an $x:A$ such that $P(x)$''.
As for disjunction, by combining the induction principles of truncation and $\Sigma$-types, if we have an assumption of type $\brck{\sm{x:A} P(x)}$, we may introduce new assumptions $x:A$ and $y:P(x)$ \emph{when attempting to prove a mere proposition}.
In other words, if we know that there exists some $x:A$ such that $P(x)$, but we don't have a particular such $x$ in hand, then we are free to make use of such an $x$ as long as we aren't trying to construct anything which might depend on the particular value of $x$.
Requiring the codomain to be a mere proposition expresses this independence of the result on the witness, since all possible inhabitants of such a type must be equal.

We can now properly formulate the \emph{axiom of choice} in homotopy type theory.
Assume a type $X$ and type families $A:X\to\type$ and $P:\prd{x:X} A(x)\to\type$, and moreover that
\begin{itemize}
\item $X$ is a set,
\item $A(x)$ is a set for all $x:X$, and
\item $P(x,a)$ is a mere proposition for all $x:X$ and $a:A(x)$.
\end{itemize}
The \textbf{axiom of choice} asserts that under these assumptions,
\begin{multline}\label{eq:ac}
  \left(\prd{x:X} \Brck{\sm{a:A(x)} P(x,a)}\right)
  \to\\
  \Brck{\sm{g:\prd{x:X} A(x)} \prd{x:X} P(x,g(x))}
\end{multline}
Of course, this is a direct translation of~\eqref{eq:english-ac} where we read ``there exists $x:A$ such that $B(x)$'' as $\brck{\sm{x:A}B(x)}$.
In particular, therefore, the propositional truncation appears twice.
The truncation in the domain means we assume that for every $x$ there exists some $a:A(x)$ such that $P(x,a)$, but that these values are not chosen or specified in any known way.
The truncation in the codomain means we conclude that there exists some function $g$, but this function is not determined or specified in any known way.

As with LEM and DN,~\eqref{eq:ac} is not a consequence of our basic type theory, but it may consistently be assumed as an axiom.
Note the restriction to types that are sets; because of the continuity/functoriality of all functions, it is unreasonable to assert such a statement without some such restriction.
However, they can be relaxed to a certain extent.
For instance, we may allow $A$ and $P$ to be arbitrary type families as long as we maintain the assumption that $X$ is a set; this results in a seemingly stronger statement that is equally consistent.
We may also replace the propositional truncation by the more general $n$-truncations to be considered in Chapter~\ref{cha:hlevels}, obtaining a spectrum of axioms interpolating between~\eqref{eq:ac} and \autoref{thm:ttac}.

We also observe that \autoref{thm:ttac} implies a simplification of~\eqref{eq:ac}.

\begin{lem}
  The axiom of choice~\eqref{eq:ac} is equivalent to the statement that for any set $X$ and any $Y:X\to\type$ such that each $Y(x)$ is a set, we have
  \begin{equation}
    \big(\prd{x:X} \brck{Y(x)}\big)
    \to
    \brck{\prd{x:X} Y(x)}.\label{eq:epis-split}
  \end{equation}
\end{lem}
\begin{proof}
  By \autoref{thm:ttac}, the codomain of~\eqref{eq:ac} is equivalent to
  \[\brck{\prd{x:X} \sm{a:A(x)} P(x,a)}.\]
  Thus,~\eqref{eq:ac} is equivalent to the instance of~\eqref{eq:epis-split} where $Y(x) \defeq \sm{a:A(x)} P(x,a)$.
  Conversely,~\eqref{eq:epis-split} is equivalent to the instance of~\eqref{eq:ac} where $A(x)\defeq Y(x)$ and $P(x,a)\defeq\unit$.
  Thus, the two are logically equivalent; so since both are mere propositions, they are equivalent types.
\end{proof}

\begin{rmk}
  A word of caution about a common pitfall: although dependent function types preserve mere propositions (\autoref{thm:isprop-forall}), they do not commute with truncation: $\brck{\prd{x:A} P(x)}$ is not generally equivalent to $\prd{x:A} \brck{P(x)}$.
  Indeed, while the version of the ``axiom of choice'' which is provable in type theory is really just a statement about how $\Sigma$'s and $\Pi$'s commute, the proper axiom of choice~\eqref{eq:ac} is arguably a statement about how $\Pi$'s commute with truncation.
\end{rmk}

\subsection{All surjectives split implies decidability of propositions}\label{sec:surj_split}
In this subsection we derive that if all surjective functions between sets have
a section, then for all propositions $A$ we can decide whether $A$ or $\neg A$
holds. This is an adaptation of an old, famous theorem by
Diaconescu and Bishop to homotopy type theory. %; for more information on surjective functions see~\cite{RijkeSpitters}.

We make use of the more general construction of suspension which will be defined in section~\ref{sec:suspension}, but we only need
to apply the suspension to propositions. In the case where $A$ is a proposition,
it turns out that the
suspension $\susp(A)$ is equivalent to the quotient $\mathsf{bool}/R$,
where $R$ is the equivalence relation we get by setting
$R(\mathsf{true},\mathsf{false}):= A$. This shall become clear in the proofs
below.

\begin{lem}\label{prop:trunc_of_prop_is_set}
The suspension of a proposition is a set and the path space 
$0=1$ in $\susp(A)$ is equivalent to $A$, for any proposition $A$. 
\end{lem}

\begin{proof}
Let $A$ be a proposition. Using the univalence axiom, we will define a 
dependent type $P:\susp(A)\to\susp(A)\to\type$ with the 
property that $P(x,y)$ is a proposition for each $x,y:\susp(A)$ 
and which turns out to be equivalent to the dependent type 
$\mathsf{Id}_{\susp(A)}$.

We make the following definitions:
\begin{align*}
P(0,0) & \equiv \mathsf{unit} & P(1,0) & \equiv A\\
P(0,1) & \equiv A & P(1,1) & \equiv \mathsf{unit}.
\end{align*}
To show that this gives a dependent type we need to verify that there 
is an equivalence $P(0,0)\simeq P(0,1)$ and an eqivalence 
$P(1,0)\simeq P(1,1)$ for every $a:A$. Since $A$ is assumed to 
be a proposition, this is indeed the case.

To finish the proof we need to show that $P(x,y)\simeq x=y$ 
for every $x,y:\susp(A)$. We can find such a fiberwise equivalence 
by finding a transformation
\begin{equation*}
\tau(y):\prod(x:\susp(A)),\ P(x,y)\to x=y
\end{equation*}
which induces an equivalence 
\begin{equation*}
\big(\sum(x:A),\ P(x,y)\big)\simeq\big(\sum(x:\susp(A)),\ x=y\big)
\end{equation*}
of total spaces, for every $y:\susp(A)$. The latter type 
is contractible, so it suffices to find the mentioned transformation 
and proof that the total space on the left is contractible. We may do 
the first of these tasks by applying the induction principle for 
$\susp(A)$. We make the definitions
\begin{align*}
\tau(0,0) & := \lambda t.\idfunc{0} & \tau(1,0) &:= \lambda a.\alpha(a)^{-1}\\
\tau(0,1) & := \alpha & \tau(1,1) &:= \lambda t.\idfunc{1}.
\end{align*}
To find a path $\alpha(a)\cdot\tau(0,0)=\tau(0,1)$ for $a:A$, 
note that because $A$ is a proposition there are paths
\begin{equation*}
(\alpha(a)\cdot\tau(0,0))(x)=\tau(0,0,x)\bullet\alpha(a)^{-1}
=\alpha(a)^{-1}= \alpha(x)^{-1}. 
\end{equation*}
Thus we get the requested path from function extensionality. Likewise, 
we obtain a path $\tau(0)=\tau(1)$ for every $a:A$, 
which finishes the construction of $\tau$.

The last thing to do to finish the proof is to show that 
$\sum(x:A),\ P(x,y)$ is contractible for every $y:\susp(A)$. 
Since $\mathsf{isContr}(X)$ is a proposition for every type $X$, 
we only need to show that $\sum(x:A),\ P(x,0)$ and 
$\sum(x:\susp(A)),\ P(x,1)$ are contractible. 
They are clearly equivalent, so we only verify the contractibility 
of the latter. The obvious candidate for the center of contraction 
is $\pair{1,\mathsf{tt}}$, thus we need to show that there is a 
function $c$ of type
\begin{equation*}
\prod(x:\susp(A))(u:P(x,1)),\ \langle{x,u}\rangle=\langle{1,\mathsf{tt}}\rangle.
\end{equation*}
We do this again by induction on $x$. To define $c(0)$, note that 
for any $a:A$, we have the path $\alpha(a):0= 1$. Since 
$\mathsf{unit}$ is contractible we will automatically get a path 
$\alpha(a)\cdot u=\mathsf{tt}$. The definition of $c(1)$ is obvious. 
Then we get, for $a:A$, a path 
$(\alpha(a)\cdot c(0))(\mathsf{tt})= c(1,\mathsf{tt})$ 
because $\susp(A)$ is contractible when $A$ is contractible. 
This gives us a path $\alpha(a)\cdot c(0)= c(1)$ and finishes 
the proof by induction that $\sum(x:A),\ P(x,1)$ is contractible.
\end{proof}

\begin{defn}
Suppose $A$ is a type. We define
\begin{equation*}
\mathsf{isDecidable}(A)\equiv  A+\neg A
\end{equation*}
\end{defn}

\begin{thm}[Bishop/Diaconescu]\label{prop:1surj_to_surj_to_pem}
Suppose that all surjections between sets split. Then for all $A:\prop$,
$\mathsf{isDecidable}(A)$. 
\end{thm}

\begin{proof}
Consider the function $\pi:\mathsf{bool}\to\susp(A)$ defined by 
$\pi(\mathsf{true})\equiv 1$ and $\pi(\mathsf{false})\equiv 0$. 
The first thing we will do is show that $\pi$ is surjective, 
i.e.\ that there is a function of type
\begin{equation*}
\prod(x:\susp(A)),\ \|\hfiber\pi{x}\|.
\end{equation*}
Note that we have the terms 
$\langle{\mathsf{false},\idfunc{0}}\rangle:\hfiber\pi0$ 
and $\langle{\mathsf{true},\idfunc{1}}\rangle:\hfiber\pi1$. 
Since $\|\hfiber\pi{x}\|$ is a proposition for each 
$x:\susp(A)$, this gives us the proof by induction of 
the surjectivity of $\pi$.

By proposition~\ref{prop:trunc_of_prop_is_set} the suspension of 
a proposition is always a set, so our assumption gives us a 
section $g:\susp(A)\to\mathsf{bool}$ of $\pi$. 
Since $\mathsf{bool}$ has decidable equality, there is a term of type 
\begin{equation*}
\mathsf{isDecidable}(g(\pi(\mathsf{true}))= g(\pi(\mathsf{false})))
\end{equation*}
and since $g$ is a section of $\pi$ it follows that there is a term of type
\begin{equation*}
\mathsf{isDecidable}(\pi(\mathsf{true})=\pi(\mathsf{false}))
\end{equation*}
Now we see that it is enough to show that 
$\pi(\mathsf{true})= \pi(\mathsf{false})$ is equivalent to $A$. 
This we also obtained in proposition~\ref{prop:trunc_of_prop_is_set}.
\end{proof}

\subsection{When are propositions truncated?}
\label{subsec:when-trunc}

At first glance, it may seem that the truncated versions of $+$ and $\Sigma$ are actually closer to the informal mathematical meaning of ``or'' and ``there exists'' than the untruncated ones.
Certainly, they are closer to the \emph{precise} meaning of ``or'' and ``there exists'' in the first-order logic which underlies formal set theory, since the latter makes no attempt to remember any witnesses to the truth of propositions.
However, it may come as a surprise to realize that the practice of \emph{informal} mathematics is often more accurately described by the untruncated forms.

For example, consider a statement like ``every prime number is either $2$ or odd.''
The working mathematician feels no compunction about using this fact not only to prove \emph{theorems} about prime numbers, but also to perform \emph{constructions} on prime numbers, perhaps doing one thing in the case of $2$ and another in the case of an odd prime.
Since the end result of the construction is not merely the truth of some statement, but a piece of data which may depend on the parity of the prime number, from a type-theoretic perspective such a construction is naturally phrased using the induction principle for the coproduct type ``$(p=2)+(p\text{ is odd})$'', not its propositional truncation.

Admittedly, this is not an ideal example, since ``$p=2$'' and ``$p$ is odd'' are mutually exclusive, so that $(p=2)+(p\text{ is odd})$ is in fact already a mere proposition and hence equivalent to its truncation (see Exercises~\ref{ex:disjoint-or} and~\ref{ex:prop-eqvtrunc}).
More compelling examples come from the existential quantifier.
It is not uncommon to prove a theorem of the form ``there exists an $x$ such that \dots'' and then refer later on to ``the $x$ constructed in Theorem Y'' (note the definite article).
Moreover, when deriving further properties of this $x$, one may use phrases such as ``by the construction of $x$ in the proof of Theorem Y''.

A very common example is ``$A$ is isomorphic to $B$'', which strictly speaking means only that there exists \emph{some} isomorphism between $A$ and $B$.
But almost invariably, when proving such a statement one exhibits a specific isomorphism, or proves that some previously known map is an isomorphism, and it often matters later on what particular isomorphism was given.

Honest, set-theoretically trained mathematicians generally feel a twinge of guilt at such ``abuses of language''.
We may attempt to apologize for them, expunge them from final drafts, or weasel out of them with vague words like ``canonical''.
The problem is exacerbated by the fact that in formalized set theory, there is technically no way to ``construct'' objects at all --- we can only prove that an object with certain properties exists.
Untruncated logic in type theory thus captures some common practices of informal mathematics that the set theoretic reconstruction obscures.
(This is similar to how the univalence axiom validates the common, but formally unjustified, practice of identifying isomorphic objects.)

On the other hand, sometimes truncated logic is essential.
We have seen this in the statements of LEM and AC; some other examples will appear later on in the book.
Thus, we are faced with the problem: when writing informal type theory, what should we mean by the words ``or'' and ``there exists'' (along with common synonyms such as ``there is'' and ``we have'')?

A universal consensus may not be possible.
Perhaps depending on the sort of mathematics being done, one convention or the other may be more useful --- or, perhaps, the choice of convention may be irrelevant.
In this case, a remark at the beginning of a mathematical paper may suffice to inform the reader of the linguistic conventions in use therein.
However, even after one overall convention is chosen, the other sort of logic will usually arise at least occasionally, so we need a way to refer to it.
More generally, one may consider replacing the propositional truncation with another operation on types that behaves similarly, such as the double negation $A\mapsto \neg\neg A$, or the $n$-truncations to be considered in Chapter~\ref{cha:hlevels}.
As an experiment in exposition,  in what follows we will occasionally use \emph{adverbs} to denote the application of such ``modalities'' as propositional truncation.

For instance, if untruncated logic is the default convention, we may use the adverb \textbf{merely} to denote propositional truncation.
Thus the phrase
\begin{center}
  ``there merely exists an $x:A$ such that $P(x)$''
\end{center}
indicates the type $\brck{\sm{x:A} P(x)}$.
Similarly, we will say that a type $A$ is \textbf{merely inhabited} to mean that its propositional truncation $\brck A$ is inhabited (i.e.\ that we have an unnamed element of it).

On the other hand, if truncated logic is the current default convention, we may use an adverb such as \textbf{purely} or \textbf{constructively} to indicate its absence, so that
\begin{center}
``there purely exists an $x:A$ such that $P(x)$''
\end{center}
would denote the type $\sm{x:A} P(x)$.
We may also use ``purely'' or ``actually'' just to emphasize the absence of truncation, even when that is the default convention.

In this book we will continue using untruncated logic as the default convention, for a number of reasons.
\begin{enumerate}[label=(\arabic*)]
\item We want to encourage the newcomer to experiment with it, rather than sticking to truncated logic simply because it is more familiar.
\item Using truncated logic as the default in type theory suffers from the same sort of ``abuse of language'' problems as set-theoretic foundations, which untruncated logic avoids.
  For instance, our definition of ``$\eqv A B$'' as the type of equivalences between $A$ and $B$, rather than its propositional truncation, means that to prove a theorem of the form ``$\eqv A B$'' is literally to construct a particular such equivalence.
  This specific equivalence can then be referred to later on.
\item We want to emphasize that the notion of ``mere proposition'' is not a fundamental part of type theory.
  As we will see in Chapter~\ref{cha:hlevels}, mere propositions are just the second rung on an infinite ladder, and there are also many other modalities not lying on this ladder at all.
\item Many statements that classically are mere propositions are no longer so in homotopy type theory.
  Of course, foremost among these is equality.
\item On the other hand, one of the most interesting observations of homotopy type theory is that a surprising number of types are \emph{automatically} mere propositions, or can be slightly modified to become so, without the need for any truncation.
  (See \autoref{ex:isprop-isprop} and Chapters~\ref{cha:equivalences}, \ref{cha:hlevels}, \ref{cha:category-theory}, and~\ref{cha:set-math}.)
  Thus, although these types contain no data beyond a truth value, we can nevertheless use them to construct untruncated objects, since there is no need to use the induction principle of propositional truncation.
  This useful fact is more clumsy to express if propositional truncation is applied to all statements by default.
\item Finally, truncations are not very useful for most of the mathematics we will be doing in this book, so it is simpler to notate them explicitly when they occur.
\end{enumerate}

\section{Contractibility}
\label{sec:contractibility}

In \autoref{thm:inhabprop-eqvunit} we observed that a mere proposition which is inhabited must be equivalent to unit, and it is not hard to see that the converse also holds.
Another equivalent definition, which is also sometimes convenient, is the following.

\begin{defn}\label{defn:contractible}
  A type $A$ is \textbf{contractible}, or a \textbf{singleton}, if there is $a:A$, called the \textbf{center of contraction}, such that $a=x$ for all $x:A$.
\end{defn}

In other words, the type $\iscontr(A)$ is defined to be
\[ \iscontr(A) \defeq \sm{a:A} \prd{x:A}(a=x). \]

\begin{lem}\label{thm:contr-paths}
  For a type $A$, the following are logically equivalent.
  \begin{enumerate}
  \item $A$ is contractible.\label{item:contr}
  \item $A$ is a mere proposition, and there is a point $a:A$.\label{item:contr-inhabited-prop}
  \item $A$ is equivalent to \unit.\label{item:contr-eqv-unit}
  \end{enumerate}
\end{lem}
\begin{proof}
  If $A$ is contractible, then it certainly has a point $a:A$ (the center of contraction), while for any $x,y:A$ we have $x=a=y$; thus $A$ is a mere proposition.
  Conversely, if we have $a:A$ and $A$ is a mere proposition, then for any $x:A$ we have $x=a$; thus $A$ is contractible.
  And we showed~\ref{item:contr-inhabited-prop}$\Rightarrow$\ref{item:contr-eqv-unit} in \autoref{thm:inhabprop-eqvunit}, while the converse follows since \unit easily has properties~\ref{item:contr} and~\ref{item:contr-inhabited-prop}.
\end{proof}

\begin{lem}\label{thm:isprop-iscontr}
  For any type $A$, the type $\iscontr(A)$ is a mere proposition.
\end{lem}
\begin{proof}
  Suppose given $c,c':\iscontr(A)$.
  We may assume $c\jdeq(a,p)$ and $c'\jdeq(a',p')$ for $a,a':A$ and $p:\prd{x:A} (a=x)$ and $p':\prd{x:A} (a'=x)$.
  By the characterization of paths in $\Sigma$-types, to show $c=c'$ it suffices to exhibit $q:a=a'$ such that $\trans{q}{p}=p'$.

  We choose $q\defeq p(a')$.
  For the other equality, by function extensionality we must show that $(\trans q p)(x)=p'(x)$ for any $x:A$.
  For this, it will suffice to show that for any $x,y:A$ and $u:x=y$ we have $u= \opp{p(x)} \ct p(y)$, since then we would have $(\trans q p)(x) = \opp{p(x)} \ct p(y) = p'(x)$.
  But now we can invoke path induction to assume that $x\jdeq y$ and $u\jdeq \refl{x}$.
  In this case our goal is to show that $\refl x = \opp{p(x)} \ct p(x)$, which is just the inversion law for paths.
\end{proof}

\begin{cor}\label{thm:contr-contr}
  If $A$ is contractible, then so is $\iscontr(A)$.
\end{cor}
\begin{proof}
  By \autoref{thm:isprop-iscontr} and \autoref{thm:contr-paths}\ref{item:contr-inhabited-prop}.
\end{proof}

\begin{lem}\label{thm:contr-forall}
  If $P:A\to\type$ is a type family such that each $P(a)$ is contractible, then $\prd{x:A} P(x)$ is contractible.
\end{lem}
\begin{proof}
  By \autoref{thm:isprop-forall}, $\prd{x:A} P(x)$ is a mere proposition since each $P(x)$ is.
  But it also has an element, namely the function sending each $x:A$ to the center of contraction of $P(x)$.
  Thus by \autoref{thm:contr-paths}\ref{item:contr-inhabited-prop}, $\prd{x:A} P(x)$ is contractible.
\end{proof}

(In fact, the statement of \autoref{thm:contr-forall} is equivalent to the function extensionality axiom.
See Appendix~[?].)


%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Notes}
\label{sec:notes}

The definition of identity types and the elimination rule $J$ are due to Martin-L\"of (what is the best reference?).
Our identity types are generally called \emph{intensional}, by contrast with the \emph{extensional} case which would have an additional ``reflection rule'' saying that if $p:x=y$, then in fact $x\jdeq y$.
This reflection rule implies that all the higher groupoid structure collapses, so for nontrivial homotopy we must use the intensional version. 
One may argue, however, that homotopy type theory is more ``extensional'' than traditional extensional type theory, because of the function extensionality and univalence rules.  

The proofs of symmetry (inversion) and transitivity (concatenation) for equalities are well-known in type theory.
The fact that these make each type into a 1-groupoid (up to homotopy) is also folklore, and was exploited in~\cite{hs:gpd-typethy} to give the first homotopical semantics for type theory.  The general homotopical interpretation, with identity types as path spaces, is due to \cite{aw:hiit}.
For a construction of \emph{all} the higher operations and coherences of an $\infty$-groupoid in type theory, see~\cite{pll:wkom-type} and~\cite{bg:type-wkom}.

Operations such as $\transfib{P}{p}{-}$ and $\apfunc{f}$ and one good notion of equivalence were first studied extensively in type theory by Voevodsky, using the proof assistant Coq.
Subsequent researchers have found many other equivalent definitions of equivalence, which we will compare in Chapter~\ref{cha:equivalences}.

The ``computational'' interpretation of identity types, transport, and so on described in \S\ref{sec:computational} has been emphasized by~\cite{lh:canonicity}.
They also described a ``1-truncated'' type theory (see Chapter~\ref{cha:hlevels}) in which these rules really are computation steps (that is, definitional equalities which a computer can ``evaluate'').
The possibility of extending this to the full untruncated theory is a subject of current research.

The naive form of function extensionality which says that ``if two functions are pointwise equal, then they are equal'' is a common axiom in type theory.
Some stronger forms of function extensionality were considered in~\cite{garner:depprod}.
The version we have used, which identifies the identity types of function types up to equivalence, was first studied by Voevodsky, who also proved that it is implied by the naive version.

The univalence axiom is also due to Voevodsky.
It was originally motivated by semantic considerations; see~\cite{klv:ssetmodel} [and possibly an appendix, if we include one about semantics].

The simple conclusions in \S\S\ref{sec:compute-coprod}--\ref{sec:compute-nat} such as ``coproduct injections are injective and disjoint'' are well-known in type theory, and the construction of the function \encode is the usual way to prove them.
The more refined approach we have described, which characterizing the entire identity type of a positive type (up to equivalence), is a more recent development; see e.g.~\cite{ls:pi1s1}.

The type-theoretic axiom of choice~\eqref{eq:sigma-ump-map} was noticed in William Howard's original paper~\cite{howard:pat} on the propositions-as-types correspondence, and was studied further by Martin-L\"of with the introduction of his dependent type theory.

The fact that it is possible to define sets in type theory using finitely many data, with all higher homotopies automatically taken care of as in \S\ref{sec:basics-sets}, was first observed by Voevodsky.
His original definition of $\isset$ was a bit more complicated than ours, but has the advantage of fitting neatly into an infinite hierarchy; see Chapter~\ref{cha:hlevels}.

\autoref{thm:not-dneg} and \autoref{thm:not-lem} rely in essense on a classical theorem of Hedberg, which we will prove in Chapter~\ref{cha:hlevels}.
The implication that the propositions-as-types form of LEM contradicts univalence was observed by Martin Escardo on the Agda mailing list.
The proof we have given of \autoref{thm:not-dneg} is due to Thierry Coquand.

Mere propositions were first defined in type theory by Voevodsky.
His original definition was slightly more complicated than ours, but fits into the more general framework of Chapter~\ref{cha:hlevels}.

The propositional truncation was introduced, in extensional type theory, by~\cite{ab:bracket-types}.
The intensional version was constructed by Voevodsky using an impredicative quantification, and later by Lumsdaine using higher inductive types (see Chapter~\ref{cha:hits}).

The adverb ``purely'' as used to refer to untruncated logic is a reference to the use of monadic modalities to model effects in programming languages.
A computation is said to be \emph{pure} if its execution results in no side effects (such as printing a message to the screen, playing music, or sending data over the Internet).
There exist ``purely functional'' programming languages, such as Haskell, in which it is technically only possible to write pure functions: side effects are represented by applying ``monads'' to output types.
For instance, a function of type $\mathsf{Int}\to\mathsf{Int}$ is pure, while a function of type $\mathsf{Int}\to \mathsf{IO}(\mathsf{Int})$ may perform input and output along the way to computing its result.
Inside of type theory, the propositional truncation $\brck-$ is also a monad, as are the more general modalities one might consider replacing it with; thus it makes sense to call a type \emph{pure} when no such modality is present.


\section*{Exercises}
\label{basics:exercises}

\begin{ex}\label{ex:basics:concat}
  Show that the three obvious proofs of \autoref{lem:concat} are pairwise equal.
\end{ex}

\begin{ex}
  Show that the three equalities of proofs constructed in the previous exercise form a commutative triangle.
\end{ex}

\begin{ex}
  Give a fourth, different, proof of \autoref{lem:concat}, and prove that it is equal to the others.
\end{ex}

\begin{ex}
  Prove that the functions~\eqref{eq:ap-to-apd} and~\eqref{eq:apd-to-ap} are inverse equivalences, and that they take $\apfunc f(p)$ to $\apdfunc f (p)$ and vice versa.
\end{ex}

\begin{ex}\label{ex:ap-sigma}
  State and prove a generalization of \autoref{thm:ap-prod} from cartesian products to $\Sigma$-types.
\end{ex}

\begin{ex}
  State and prove an analogue of \autoref{thm:ap-prod} for coproducts.
\end{ex}

\begin{ex}\label{ex:coprod-ump}
  Prove that coproducts have the expected universal property:
  \[ \eqv{(A+B \to X)}{(A\to X)\times (B\to X)} \]
  Can you generalize this to an equivalence involving dependent functions?
\end{ex}

\begin{ex}
  Prove that if $\eqv A B$ and $A$ is a set, then so is $B$.
\end{ex}

\begin{ex}\label{ex:isset-coprod}
  Prove that if $A$ and $B$ are sets, then so is $A+B$.
\end{ex}

\begin{ex}\label{ex:isset-sigma}
  Prove that if $A$ is a set and $B:A\to \type$ is a type family such that $B(x)$ is a set for all $x:A$, then $\sm{x:A} B(x)$ is a set.
\end{ex}

\begin{ex}\label{ex:prop-endocontr}
  Show that $A$ is a mere proposition if and only if $A\to A$ is contractible.
\end{ex}

\begin{ex}
  Show that if $A$ is a mere proposition, then so is $A+(\neg A)$.
  Thus, there is no need to insert a propositional truncation in~\eqref{eq:lem}.
\end{ex}

\begin{ex}\label{ex:disjoint-or}
  More generally, show that if $A$ and $B$ are mere propositions and $\neg(A\times B)$, then $A+B$ is also a mere proposition.
\end{ex}

\begin{ex}\label{ex:hprop-iff-equiv}
  Show that if $A$ and $B$ are mere propositions such that $A\to B$ and $B\to A$, then $\eqv A B$.
\end{ex}

\begin{ex}\label{ex:isprop-isprop}
  Show that for any type $A$, the types $\isprop(A)$ and $\isset(A)$ are mere propositions.
\end{ex}

\begin{ex}\label{ex:prop-eqvtrunc}
  Show that if $A$ is already a mere proposition, then $\eqv A{\brck{A}}$.
\end{ex}

\begin{ex}\label{ex:brck-qinv}
  Assuming that some type $\isequiv(f)$ satisfies conditions~\ref{item:be1}--\ref{item:be3} of \S\ref{sec:basics-equivalences}, show that the type $\brck{\qinv(f)}$ satisfies the same conditions and is equivalent to $\isequiv(f)$.
\end{ex}

\begin{ex}
  Show that it is not the case that for all $A:\type$ we have $\brck{A} \to A$.
  (However, there can be particular types for which $\brck{A}\to A$.
  \autoref{ex:brck-qinv} implies that $\qinv(f)$ is such.)
\end{ex}

\begin{ex}
  Show that the rules for the propositional truncation given in \S\ref{sec:prop-trunc} are sufficient to imply a dependent version of the induction principle: for any type family $B:\brck A \to \type$ such that each $B(x)$ is a mere proposition, if for every $a:A$ we have $B(\bproj a)$, then for every $x:\brck A$ we have $B(x)$.
\end{ex}

\begin{ex}\label{ex:lem-ldn}
  Show that the law of excluded middle~\eqref{eq:lem} and the law of double negation~\eqref{eq:ldn} are logically equivalent.
\end{ex}

% Local Variables:
% TeX-master: "main"
% End:
