\newcommand{\zero}{\ensuremath{\mathbf{0}}\xspace}
\newcommand{\one}{\ensuremath{\mathbf{1}}\xspace}
\newcommand{\two}{\ensuremath{\mathbf{2}}\xspace}
\newcommand{\three}{\ensuremath{\mathbf{3}}\xspace}
\newcommand{\nat}{\ensuremath{\mathbf{N}}\xspace}
\newcommand{\true}{\ensuremath{\mathbf{true}}\xspace}
\newcommand{\false}{\ensuremath{\mathbf{false}}\xspace}
\newcommand{\rec}{\ensuremath{\mathbf{rec}}\xspace}
\newcommand{\z}{\ensuremath{0}\xspace}
\newcommand{\s}{\ensuremath{\mathbf{s}}\xspace}
\newcommand{\alt}{\;|\;\;}
\newcommand{\der}{\vdash}

\chapter{Induction}
\label{cha:induction}

\section{Booleans and natural numbers}

% Local Variables:
% TeX-master: "main"
% End:

An \emph{inductive type} can be intuitively understood as a type generated by a certain finite collection of constructors. To specify an inductive type formally,
we will use a schematic definition, where we list the name and type of each constructor separately, e.g.:
\begin{align*}
  \two \defeq \; & \true : \two \\
         \alt & \false : \two
\end{align*}
The above definition declares the type $\two$ of Booleans to be the inductive type generated by two constant constructors $\true$ and $\false$. We can similarly define the types $\zero$ (aka Empty, Void), $\one$ (aka Unit), $\three$, and so on, with 0, 1, 3, or more constructors respectively. 

Another extremely important inductive type is the type $\nat$ of natural numbers:  
\begin{align*}
  \nat \defeq \; & \z : \nat \\
        \alt & \s : \nat \to \nat 
\end{align*}
As expected, $\nat$ is generated by a constant constructor $\z$ for the natural number zero and a unary constructor $\s$ taking a natural number $n : \nat$ to its successor $\s(n) : \nat$. It is understood that for any inductive type, different constructors construct different terms and each constructor itself is injective. This ensures that as desired, 0 is not a successor of any other natural number and that no two natural numbers have the same successor.

What can we do with such an inductive type? An intuitive understanding of an inductive type $A$ is that it \emph{behaves as if the only inhabitants of $A$ were the terms constructed solely by applying the constructors of $A$}; we refer to these particular terms as the \emph{canonical terms of $A$}. In an empty context,
each term of $A$ is canonical - for instance, any closed term of an identity type is necessarily the identity path. In the setting of nonempty contexts,  
interesting and often nontrivial behavior may occur, such as the one exhibited by identity types which behave much like paths in a space.

In the case of the type $\two$ of Booleans, assuming that each term is canonical simply means that each term $b : \two$ must be either $\true$ or $\false$. In particular, we have the principle of \emph{(dependent) elimination}:

\begin{itemize}
\item When proving a statement $\Gamma \der E : \prd{b : \two} \type$ about \emph{all} Booleans, it suffices to prove it for $\true$ and $\false$, i.e., give proofs
$\Gamma \der e_t : E(\true)$ and $\Gamma \der e_f : E(\false)$.
\end{itemize}

Furthermore, the resulting proof $\Gamma \der \rec_\two(E,e_t,e_f): \prd{b : \two}E(b)$ behaves as expected when applied to the constructors $\true$ and $\false$; this principle is expressed by the \emph{computation rules}:
\begin{itemize}
\item The proof $\Gamma \der \rec_\two(E,e_t,e_f,\true) : E(\true)$ is identical to $e_t$.
\item The proof $\Gamma \der \rec_\two(E,e_t,e_f,\false) : E(\true)$ is identical to $e_f$.
\end{itemize}
For simplicity we often omit the ambient context $\Gamma$.

The rules for the type $\two$ of Booleans allow us to reason by \emph{case analysis}. Since neither of the two constructors takes any arguments, this is all we need for Booleans. However, for more complex types such as the type $\nat$ of natural numbers, true induction is often needed (hence the name \emph{inductive type}):

\begin{itemize}
\item When proving a statement $E : \prd{x : \nat} \type$ about \emph{all} natural numbers, it suffices to prove it for $\z$ and for $\s(n)$, assuming it holds
for $n$. This entails giving the proofs $e_z : E(\z)$ and $e_s : \prd{n : \nat}{y : E(n)} E(\s(n))$.
\end{itemize}
The variable $y$ represents our inductive hypothesis. As for Booleans, we also have the associated computation rules for the function $\rec_\nat(E,e_z,e_s) : \prd{x:\nat} E(x)$:
\begin{itemize}
\item The proof $\rec_\nat(E,e_z,e_s,\z) : E(\z)$ is identical to $e_z$.
\item For any $n : \nat$, the proof $\rec_\nat(E,e_z,e_s,\s(n)) : E(\s(n))$ is identical to $e_s(n,\rec_\nat(E,e_z,e_s,n))$.
\end{itemize}
The dependent function $\rec_\nat(E,e_z,e_s)$ can thus be understood as being defined recursively on the argument $x : \nat$, via the recurrences $e_z$ and $e_s$: When $x$ is zero, the function simply returns $e_z$. When $x$ is the successor of another natural number $n$, the result is obtained by taking the recurrence $e_s$ and plugging in the specific predecessor $n$ and the recursive call value $\rec_\nat(E,e_z,e_s,n)$.

This induction principle is rather strong and allows us to prove a variety of interesting theorems. For example, by specifying the terms $e_z$ and $e_s$, we  uniquely determine how the recursor behaves on canonical terms, and thus on all natural numbers. If we now have another function which obeys the same recurrence, then our intuition suggests these two functions should be equal. It turns out this is indeed the case and we have the following \emph{uniqueness principle}:

\begin{thm}
Let $f,g : \prd{x:\nat} E(x)$ be two functions which satisfy the recurrences $e_z : E(\z)$ and $e_s : \prd{n : \nat}{y : E(n)} E(\s(n))$ up to propositional equality, i.e., such that
\begin{align*}
\id{f(\z)}{e_z} \\ 
\id{g(\z)}{e_z}
\end{align*}
and 
\begin{align*}
\prd{n : \nat} \id{f(\s(n))}{e_s(n, f(n))} \\
\prd{n : \nat} \id{g(\s(n))}{e_s(n, g(n))}
\end{align*}
Then $f$ and $g$ are equal, i.e., we have $\alpha : \id[\prd{x :\nat} E(x)]{f}{g}$. 
\end{thm}

\begin{proof}
We use dependent elimination with the type $E(x) \defeq \id{f(x)}{g(x)}$. For the base case, we have \[f(\z) = e_z = g(\z)\]
For the inductive case, assume $n : \nat$ such that $f(n) = g(n)$. Then
\[ f(\s(n)) = e_s(n, f(n)) = e_s(n, g(n)) = g(\s(n)) \]
The first and last equality follow from the assumptions on $f$ and $g$. The middle equality follows from the inductive hypothesis and the fact that application preserves equality. This gives us pointwise equality between $f$ and $g$; invoking function extensionality finishes the proof.
\end{proof}
We note that the function $f$ is only required to satisfy the recurrences \emph{up to propositional equality}. The theorem itself only asserts propositional equality between functions - indeed, it is possible to construct functions which satisfy the same recurrence but are not definitionally equal (exercise). 

Similar uniqueness theorems can generally be formulated and shown for other inductive types as well. Such uniqueness results are a very useful tool; for instance, using uniqueness we can show that a certain class of inductive types gives rise to \emph{homotopy-initial algebras}. Taking Booleans as the simplest example, we define a \emph{$\two$-algebra} to be any type $C$ with two terms $c_0, c_1 : C$. Thus,

\begin{align*}
\mathtt{2Alg} \defeq \sm {C : \type} C \times C
\end{align*}
Given $\two$-algebras $(C,c_0,c_1)$ and $(D,d_0,d_1)$, we define a $\two$-homomorphism between them as a function $h : C \to D$ mapping $c_0$ to $d_0$ and
$c_1$ to $d_1$ (up to propositional equality). Thus,
\begin{align*}
\mathtt{2Hom \; (C,c_0,c_1) \; (D,d_0,d_1)} \defeq \sm {h : C \to D} \; \id{h(c_0)}{d_0} \times \id{h(c_1)}{d_1}
\end{align*}
A $\two$-algebra $\chi^*$ is then called \emph{homotopy-initial} if for any other $\two$-algebra $\chi$, the type of $\two$-homomorphisms from $\chi^*$ to $\chi$ is contractible. Thus,
\begin{align*}
\mathtt{is\_hinitial \; \chi^* \defeq \fall{\chi : 2Alg} \; is\_contr \; (2Hom \; \chi^*\; \chi)}
\end{align*}

We now have the following theorem; the proof is only sketched as it involves a large amount of technical detail.
\begin{thm}
The $\two$-algebra $(\two, \true, \false)$ is homotopy initial.
\end{thm}
\begin{proof}
(Rough sketch) Fix an arbitrary $\two$-algebra $(C,c_0,c_1)$. Using elimination on $\two$ it is easy to construct a $\two$-homomorphism into $C$. This will be our
center of contraction. To show that any other homomorphism is equal to the chosen one, we appeal to the uniqueness theorem for $\two$.
\end{proof}

\section{Dependent and disjoint sums; W-types}