\chapter{Homotopy type theory}
\label{cha:basics}

\section{Types are higher groupoids}
\label{sec:equality}

Recall that for any type $A$, and any $x,y:A$, we have a identity type $\id[A]{x}{y}$, also written $\idtype[A]{x}{y}$ or just $x=y$.
We can think of inhabitants of $x=y$ as either
\begin{enumerate}
\item evidence that $x$ and $y$ are equal,
\item identifications of $x$ with $y$,
\item paths from $x$ to $y$, or
\item isomorphisms/equivalences from $x$ to $y$.
\end{enumerate}
The first is more traditional in type theory; but in homotopy type theory we often take the latter perspectives as well.
It turns out that the defining rules of identity types, as described in the previous chapter, give them structure which corresponds precisely to that of the continuous paths and (higher) homotopies between them in a space, or a (weak) higher-dimensional groupoid.

This interpretation depends on the ability to \emph{iterate} the identity types.
Recall that type theory (unlike, say, first-order logic) does not distinguish between individuals (e.g.\ $x:A$), which may be the subjects of propositions, and proofs of propositions (e.g.\ evidence for $\id[A]{x}{y}$).
Thus, the identity type can be iterated: we can form the type $\id[{\id[A]{x}{y}}]{p}{q}$ of identifications between identifications $p,q$, and the type $\id[{\id[{\id[A]{x}{y}}]{p}{q}}]{r}{s}$, and so on.
(Obviously, the notation ``$\id[A]{x}{y}$'' has its limitations here.
The style $\idtype[A]{x}{y}$ is only slightly better in iterations: $\idtype[{\idtype[{\idtype[A]{x}{y}}]{p}{q}}]{r}{s}$.)

If the  type $A$ is ``set-like'', such as \nat, these iterated identity types will be uninteresting (see \autoref{sec:basics-sets}).
However, under the interpretation of $p,q : \id[A]{x}{y}$ as paths from $x$ to $y$, an element $r : \id[{\id[A]{x}{y}}]{p}{q}$ can be
thought of as a \emph{path between paths}, or a \emph{homotopy}, and visualized as a 2-dimensional path that fills in the space between $p$ and $q$.
Similarly, $\id[{\id[{\id[A]{x}{y}}]{p}{q}}]{r}{s}$ is the type of 3-dimensional paths between two 2-dimensional paths, and so on.
This ``tower of identity types'' is what we claim has the structure of a higher groupoid.

One of the amazing things about homotopy type theory is that all the higher groupoid structure arises automatically from the induction principle for identity types, as stated in \autoref{sec:identity-types}.
Recall that this says that if
\begin{itemize}
\item for every $x,y:A$ and every $p:\id[A]xy$ we have a type $D(x,y,p)$, and
\item for every $a:A$ we have an element $d(a):D(a,a,\refl a)$, 
\end{itemize}
then
\begin{itemize}
\item there exists an element $J_{D,d}(x,y,p):D(x,y,p)$ for \emph{every} two elements $x,y:A$ and $p:\id[A]xy$, such that $J_{D,d}(a,a,\refl a) \jdeq d(a)$.
\end{itemize}
In other words, given dependent functions
\begin{align*}
D & :\prd{x,y:A}{p:\id{x}{y}} \type\\
d & :\prd{a:A} D(a,a,\refl{a})
\end{align*}
there is a dependent function
\[J_{D,d}:\prd{x,y:A}{p:\id{x}{y}} D(x,y,p)\]
such that 
\begin{equation}\label{eq:Jconv}
J_{D,d}(a,a,\refl{a})\jdeq d(a)
\end{equation}
for every $a:A$.
The notation $J$ is traditional for this function, but we will not use it very much.
Usually, every time we apply this induction rule we will either not care about the specific function being defined, or we will immediately give it a different name.

Informally, the induction principle for identity types says that if we want to construct an object (or prove a statement) which depends on an inhabitant $p:\id[A]xy$ of an identity type, then it suffices to perform the construction (or the proof) in the special case when $x$ and $y$ are the same (judgmentally) and $p$ is the reflexivity element $\refl{x}:x=x$ (judgmentally).
When writing informally, we may express this with a phrase such as ``by induction, it suffices to assume\dots''.
This reduction to the ``reflexivity case'' is analogous to the reduction to the ``base case'' and ``inductive step'' in an ordinary proof by induction on the natural numbers, and also to the ``left case'' and ``right case'' in a proof by case analysis on a disjoint union or disjunction.

The ``conversion rule''~\eqref{eq:Jconv} is less familiar in the context of proof by induction on natural numbers, but there is an analogous notion in the related concept of definition by recursion.
If a sequence $(a_n)_{n\in \mathbb{N}}$ is defined by giving $a_0$ and specifying $a_{n+1}$ in terms of $a_n$, then in fact the $0^{\mathrm{th}}$ term of the resulting sequence \emph{is} the given one, and the given recurrence relation relating $a_{n+1}$ to $a_n$ holds for the resulting sequence.
(This may seem so obvious as to not be worth saying, but if we view a definition by recursion as an algorithm for calculating values of a sequence, then it is precisely the process of executing that algorithm.)
The rule~\eqref{eq:Jconv} is analogous: it says that if we define an object $f(p)$ for all $p:x=y$ by specifying what the value should be when $p$ is $\refl{x}:x=x$, then the value we specified is in fact the value of $f(\refl{x})$.

We now derive from this induction principle the beginnings of the structure of a higher groupoid.
We begin with symmetry of equality, which, in topological language, means that ``paths can be reversed''.

\begin{lem}\label{lem:opp}
  For every type $A$ and every $x,y:A$ there is a function
  \begin{equation*}
    (x= y)\to(y= x)
  \end{equation*}
  denoted $p\mapsto \opp{p}$, such that $\opp{\refl{x}}\jdeq\refl{x}$ for each $x:A$.
\end{lem}
\begin{proof}[First proof]
  Let $D:\prd{x,y:A}{p:x= y} \type$ be the type family defined by $D(x,y,p)\defeq (y= x)$.
  In other words, $D$ is a function assigning to any $x,y:A$ and $p:x=y$ a type, namely the type $y=x$.
  Then we have an element
  \begin{equation*}
    d\defeq \lam{x} \refl{x}:\prd{x:A} D(x,x,\refl{x}).
  \end{equation*}
  Thus, the eliminator $J$ for identity types gives us an element $J_{D,d}(x,y,p): (y= x)$ for each $p:(x= y)$.
  We can now define the desired function $\opp{(-)}$ to be $\lam{p} J_{D,d}(x,y,p)$, i.e.\ we set $\opp{p} \defeq J_{D,d}(x,y,p)$.
  The conversion rule~\eqref{eq:Jconv} gives $\opp{\refl{x}}\jdeq \refl{x}$, as required.
\end{proof}

We have written out this proof in a very formal style, which may be helpful while the induction rule on identity types is unfamiliar.
However, eventually we prefer to use more natural language, such as in the following equivalent proof.

\begin{proof}[Second proof]
  We want to construct, for each $x,y:A$ and $p:x=y$, an element $\opp{p}:y=x$.
  By induction, it suffices to do this in the case when $y$ is $x$ and $p$ is $\refl{x}$.
  But in this case, the type $x=y$ of $p$ and the type $y=x$ in which we are trying to construct $\opp{p}$ are both simply $x=x$.
  Thus, in the ``reflexivity case'', we can define $\opp{\refl{x}}$ to be simply $\refl{x}$.
  The general case then follows by the induction principle, and the conversion rule $\opp{\refl{x}}\jdeq\refl{x}$ is precisely the proof in the reflexivity case that we gave.
\end{proof}

We will write out the next few proofs in both styles, to help the reader become accustomed to the latter one.
Next we prove the transitivity of equality, or equivalently we ``concatenate paths''.

\begin{lem}\label{lem:concat}
  For every type $A$ and every $x,y,z:A$ there is a function
  \begin{equation*}
  (x= y) \to   \big((y= z)\to (x=  z)\big)
  \end{equation*}
  written $(p,q)\mapsto p\ct q$, such that $\refl{x}\ct \refl{x}\jdeq \refl{x}$ for any $x:A$.
\end{lem}

\begin{proof}[First proof]
  Let $D:\prd{x,y:A}{p:x=y} \type$ be the type family
  \begin{equation*}
    D(x,y,p)\defeq \prd{z:A}{q:y=z} (x=z).
  \end{equation*}
  Note that $D(x,x,\refl x) \jdeq \prd{z:A}{q:x=z} (x=z)$.
  Thus, in order to apply the induction principle for identity types to this $D$, we need a function of type
  \begin{equation}\label{eq:concatD}
    \prd{x:A} D(x,x,\refl{x})
  \end{equation}
  which is to say, of type
  \[ \prd{x,z:A}{q:x=z} (x=z). \]
  Now let $E:\prd{x,z:A}{q:x=z}\type$ be the type family $E(x,z,q)\defeq (x=z)$.
  Note that $E(x,x,\refl x) \jdeq (x=x)$.
  Thus, we have the function
  \begin{equation*}
    e(x) \defeq \refl{x} : E(x,x,\refl{x}).
  \end{equation*}
  By the induction principle for identity types applied to $E$, we obtain a function
  \begin{equation*}
    d(x,z,q) : \prd{x,z:A}{q:x=z} E(x,z,q).
  \end{equation*}
  But $E(x,z,q)\jdeq (x=z)$, so this is~\eqref{eq:concatD}.
  Thus, we can use this function $d$ and apply the induction principle for identity types to $D$, to obtain our desired function of type
  \begin{equation*}
    \prd{x,y,z:A}{q:y=z}{p:x=y} (x=z).
  \end{equation*}
  The conversion rules for the two induction principles give us $\refl{x}\ct \refl{x}\jdeq \refl{x}$ for any $x:A$.
\end{proof}

\begin{proof}[Second proof]
  We want to construct, for every $x,y,z:A$ and every $p:x=y$ and $q:y=z$, an element of $x=z$.
  By induction on $p$, it suffices to assume that $y$ is $x$ and $p$ is $\refl{x}$.
  In this case, the type $y=z$ of $q$ is $x=z$.
  Now by induction on $q$, it suffices to assume also that $z$ is $x$ and $q$ is $\refl{x}$.
  But in this case, $x=z$ is $x=x$, and we have $\refl{x}:(x=x)$.
\end{proof}

The reader may well feel that we have given an overly convoluted proof of this lemma.
In fact, we could stop after the induction on $p$, since at that point what we want to produce is an equality $x=z$, and we already have such an equality, namely $q$.
Why do we go on to do another induction on $q$?

The answer is that, as described in the introduction, we are doing \emph{proof-relevant} mathematics.
When we prove a lemma, we are defining an inhabitant of some type, and it can matter what \emph{specific} element we defined in the course of the proof, not merely the type that that element inhabits (that is, the \emph{statement} of the lemma).
\autoref{lem:concat} has three obvious proofs: we could do induction over $p$, induction over $q$, or induction over both of them.
If we proved it three different ways, we would have three different elements of the same type.
It's not hard to show that these three elements are equal (see \autoref{ex:basics:concat}), but as they are not \emph{definitionally} equal, there can still be reasons to prefer one over another.

In the case of \autoref{lem:concat}, the difference hinges on the computation rule.
If we proved the lemma using a single induction over $p$, then we would end up with a computation rule of the form $\refl{y} \ct q \jdeq q$.
If we proved it with a single induction over $q$, we would have instead $p\ct\refl{x}\jdeq p$, while proving it with a double induction (as we did) gives only $\refl{x}\ct\refl{x} \jdeq \refl{x}$.

The asymmetrical computation rules can sometimes be convenient when doing formalized mathematics, as they allow the computer to simplify more things automatically.
However, in informal mathematics, and arguably even in the formalized case, it can be confusing to have a concatenation operation which behaves asymmetrically and to have to remember which side is the ``special'' one.
Treating both sides symmetrically makes for more robust proofs; this is why we have given the proof that we did.
(However, this is admittedly a stylistic choice.)

The table below summarizes the ``equality'' and ``homotopical'' points of view on what we have done so far.
\begin{center}
  \begin{tabular}{c|c}
    Equality & Homotopy \\\hline
    reflexivity & constant path\\
    symmetry & inversion of paths\\
    transitivity & concatenation of paths
  \end{tabular}
\end{center}

Because of proof-relevance, we can't stop after proving ``symmetry'' and ``transitivity'' of equality: we need to know that these \emph{operations} on equalities are well-behaved.
(This issue is invisible in set theory, where symmetry and transitivity are mere \emph{properties} of equality, rather than structure on
paths.)
From the homotopy-theoretic point of view, concatenation and inversion are just the ``first level'' of higher groupoid structure --- we also need coherence laws on these operations, and analogous operations at higher dimensions.
For instance, we need to know that concatenation is \emph{associative}, and that inversion provides \emph{inverses} with respect to concatenation.

\begin{lem}\label{thm:omg}%[The $\omega$-groupoid structure of types]
  Suppose $A:\type$, that $x,y,z,w:A$ and that $p:x= y$ and $q:y = z$ and $r:z=w$.
  We have the following:
  \begin{enumerate}
  \item $p= p\ct \refl{y}$ and $p = \refl{x} \ct p$.\label{item:omg1}
  \item $\opp{p}\ct p=  \refl{y}$ and $p\ct \opp{p}= \refl{x}$.
  \item $\opp{(\opp{p})}= p$.
  \item $p\ct (q\ct r)=  (p\ct q)\ct r$.\label{item:omg4}
  \end{enumerate}
\end{lem}

Note, in particular, that~\ref{item:omg1}--\ref{item:omg4} are themselves propositional equalities, living in the identity types \emph{of} identity types, such as $p=_{x=y}q$ for $p,q:x=y$.
Topologically, they are \emph{paths of paths}, i.e.\ homotopies.
It is a familiar fact in topology that when we concatenate a path $p$ with the reversed path $\opp p$, we don't literally obtain a constant path (which corresponds to the equality $\refl{}$ in type theory) --- instead we have a homotopy, or higher path, from $p\ct\opp p$ to the constant path.

\begin{proof}[Proof of~\autoref{thm:omg}]
  All the proofs use the induction principle for equalities.
  \begin{enumerate}
  \item \emph{(First proof)} Let $D:\prd{x,y:A}{p:x=y} \type$ be the type family given by 
    \begin{equation*}
      D(x,y,p)\defeq (p= p\ct \refl{y}).
    \end{equation*}
    Then $D(x,x,\refl{x})$ is $\refl{x}=\refl{x}\ct\refl{x}$.
    Since $\refl{x}\ct\refl{x}\jdeq\refl{x}$, it follows that $D(x,x,\refl{x})\jdeq (\refl{x}=\refl{x})$.
    Thus, there is a function
    \begin{equation*}
      d\defeq\lam{x} \refl{\refl{x}}:\prd{x:A} D(x,x,\refl{x}).
    \end{equation*}
    Now the induction principle for identity types gives an element $J(D,d,p):(p= p\ct\refl{y})$ for each $p:x= y$.
    The other equality is proven similarly.

    \noindent
    \emph{(Second proof)} By induction on $p$, it suffices to assume that $y$ is $x$ and that $p$ is $\refl x$.
    But in this case, we have $\refl{x}\ct\refl{x}\jdeq\refl{x}$.
  \item \emph{(First proof)} Let $D:\prd{x,y:A}{p:x=y} \type$ be the type family given by 
    \begin{equation*}
      D(x,y,p)\defeq (\opp{p}\ct p=  \refl{y}).
    \end{equation*}
    Then $D(x,x,\refl{x})$ is $\opp{\refl{x}}\ct\refl{x}=\refl{x}$.
    Since $\opp{\refl{x}}\jdeq\refl{x}$ and $\refl{x}\ct\refl{x}\jdeq\refl{x}$, we get that $D(x,x,\refl{x})\jdeq (\refl{x}=\refl{x})$.
    Hence we find the function
    \begin{equation*}
      d\defeq\lam{x} \refl{\refl{x}}:\prd{x:A} D(x,x,\refl{x}).
    \end{equation*}
    Now path induction gives an element $J(D,d,p):\opp{p}\ct p=\refl{y}$ for each $p:x= y$ in $A$.
    The other equality is similar.

    \noindent \emph{(Second proof)} By induction, it suffices to assume $p$ is $\refl x$.
    But in this case, we have $\opp{p} \ct p \jdeq \opp{\refl x} \ct \refl x \jdeq \refl x$.
  \item \emph{(First proof)} Let $D:\prd{x,y:A}{p:x=y} \type$ be the type family given by
    \begin{equation*}
      D(x,y,p)\defeq (\opp{\opp{p}}= p).
    \end{equation*}
    Then $D(x,x,\refl{x})$ is the type $(\opp{\opp{\refl x}}=\refl{x})$.
    But since $\opp{\refl{x}}\jdeq \refl{x}$ for each $x:A$, we have $\opp{\opp{\refl{x}}}\jdeq \opp{\refl{x}} \jdeq\refl{x}$, and thus $D(x,x,\refl{x})\jdeq(\refl{x}=\refl{x})$.
    Hence we find the function
    \begin{equation*}
      d\defeq\lam{x} \refl{\refl{x}}:\prd{x:A} D(x,x,\refl{x}).
    \end{equation*}
    Now path induction gives an element $J(D,d,p):\opp{\opp{p}}= p$ for each $p:x= y$.

    \noindent \emph{(Second proof)} By induction, it suffices to assume $p$ is $\refl x$.
    But in this case, we have $\opp{\opp{p}}\jdeq \opp{\opp{\refl x}} \jdeq \refl x$.
  \item \emph{(First proof)} Let $D_1:\prd{x,y:A}{p:x=y} \type$ be the type family given by
    \begin{equation*}
      D_1(x,y,p)\defeq\prd{z,w:A}{q:y= z}{r:z= w} \big(p\ct (q\ct r)=  (p\ct q)\ct r\big).
    \end{equation*}
    Then $D_1(x,x,\refl{x})$ is
    \begin{equation*}
      \prd{z,w:A}{q:x= z}{r:z= w} \big(\refl{x}\ct(q\ct r)= (\refl{x}\ct q)\ct r\big).
    \end{equation*}
    To construct an element of this type, let $D_2:\prd{x,z:A}{q:x=z} \type$ be the type family
    \begin{equation*}
      D_2 (x,z,q) \defeq \prd{w:A}{r:z=w} \big(\refl{x}\ct(q\ct r)= (\refl{x}\ct q)\ct r\big).
    \end{equation*}
    Then $D_2(x,x,\refl{x})$ is
    \begin{equation*}
      \prd{w:A}{r:x=w} \big(\refl{x}\ct(\refl{x}\ct r)= (\refl{x}\ct \refl{x})\ct r\big).
    \end{equation*}
    To construct an element of \emph{this} type, let $D_3:\prd{x,w:A}{r:x=w} \type$ be the type family
    \begin{equation*}
      D_3(x,w,r) \defeq \big(\refl{x}\ct(\refl{x}\ct r)= (\refl{x}\ct \refl{x})\ct r\big).
    \end{equation*}
    Then $D_3(x,x,\refl{x})$ is
    \begin{equation*}
      \big(\refl{x}\ct(\refl{x}\ct \refl{x})= (\refl{x}\ct \refl{x})\ct \refl{x}\big)
    \end{equation*}
    which is definitionally equal to the type $(\refl{x} = \refl{x})$, and is therefore inhabited by $\refl{\refl{x}}$.
    Applying the identity elimination rule three times, therefore, we obtain an element of the overall desired type.

    \noindent \emph{(Second proof)} By induction, it suffices to assume $p$, $q$, and $r$ are all $\refl x$.
    But in this case, we have
    \begin{align*}
      p\ct (q\ct r)
      &\jdeq \refl{x}\ct(\refl{x}\ct \refl{x})\\
      &\jdeq \refl{x}\\
      &\jdeq (\refl{x}\ct \refl x)\ct \refl x\\
      &\jdeq (p\ct q)\ct r.
    \end{align*}
    Thus, we have $\refl{\refl{x}}$ inhabiting this type.\qedhere
  \end{enumerate}
\end{proof}

\begin{rmk}
  There are other ways to define all of these higher paths.
  For instance, in \autoref{thm:omg}\ref{item:omg4} we might do induction only over one or two paths rather than all three.
  Each possibility will produce a \emph{definitionally} different proof, but they will all be equal to each other.
  Such an equality between any two particular proofs can, again, be proven by induction, reducing all the paths in question to reflexivities and then observing that both proofs reduce themselves to reflexivities.
\end{rmk}

We are still not really done with the higher groupoid structure: the paths~\ref{item:omg1}--\ref{item:omg4} must also satisfy their own higher coherence laws, which are themselves higher paths, and so on ``all the way up to infinity" (this can be made precise using e.g.\ the notion of a globular operad).
However, for most purposes it is unnecessary to make the whole infinite-dimensional structure explicit.
One of the nice things about homotopy type theory is that all of this structure can be \emph{proven} starting from only the inductive property of identity types, so we can make explicit as much or as little of it as we need.
In particular, in this book we will not need any of the complicated combinatorics involved in making precise notions such as ``coherent structure at all higher levels''.

One particularly important case of iterated identity types (path types) is when the start and end points are the same.
In set theory, the propositon $a=a$ is entirely uninteresting, but in homotopy theory paths from a point to itself are called \emph{loops} and carry lots of interesting higher structure.
Thus, given a type $A$ with a point $a:A$, we define its \emph{loop space} $\Omega(A,a)$ to be the type $\id[A]{a}{a}$.
We may sometimes write simply $\Omega A$ if the point $a$ is understood from context.

Since any two elements of $\Omega A$ are paths with the same start and end points, they can be concatenated;
thus we have an operation $\Omega A\times \Omega A\to \Omega A$.
More generally, the higher groupoid structure of $A$ gives $\Omega A$ the analogous structure of a ``higher group''.

It can also be useful to consider the loop space \emph{of} the loop space of $A$, which is the space of 2-dimensional loops on the identity loop in $a$.
This is written $\Omega^2(A,a)$ and represented in type theory by the type $\id[({\id[A]{a}{a}})]{\refl{a}}{\refl{a}}$.
While $\Omega^2(A,a)$, as a loop space, is again a ``higher group", it now also has some additional structure resulting from the fact that its elements are 2-dimensional loops between 1-dimensional loops.  

\begin{thm}[Eckmann-Hilton]\label{thm:EckmannHilton}
The composition operation on the second loop space $$\Omega^2(A)\times \Omega^2(A)\to \Omega^2(A)$$  is commutative: $\alpha\ct\beta = \beta\ct\alpha$, for any $\alpha, \beta:\Omega^2(A)$.
\end{thm}

\begin{proof}
First, observe that the composition of $1$-loops $\Omega A\times \Omega A\to \Omega A$ induces an operation 
$$
\star : \Omega^2(A)\times \Omega^2(A)\to \Omega^2(A)
$$
 as follows: consider elements $a, b, c : A$ and 1- and 2-paths,
%
$$
\begin{array}{ccc}
p: a = b & , & r : b = c \\
q : a = b & , & s : b = c \\
\alpha : p = q & , & \beta : r = s
\end{array}
$$
%
as depicted in the following diagram.
$$
\ \xy
(0,0)*+{a}="a";
(45,0)*+{b}="b";
(90,0)*+{c}="c";
{\ar@/^3pc/^{p} "a";"b"};
{\ar@/_3pc/_{q} "a";"b"};
{\ar@/^3pc/^{r} "b";"c"};
{\ar@/_3pc/_{s} "b";"c"};
{\ar@{=>} (22,10)*{};(22,-10)*{}};
(28,0)*{\alpha};
{\ar@{=>} (67,10)*{};(67,-10)*{}};
(72,0)*{\beta};
\endxy \ 
$$

Composing the upper and lower 1-paths, respectively, we get two paths $p\ct r,\ q\ct s : a = c$, and there is then a ``horizontal composition" $$\alpha\star\beta : p\ct r = q\ct s$$ between them,
defined as follows: first let $\alpha \rightwhisker r : p\ct r = q\ct r$ be determined by path induction on $r$, then let $q\leftwhisker \beta : q\ct r = q\ct s$ be given by induction on $q$.  Since these paths are composable in the type of 2-paths, we can define the \emph{horizontal composition}  by:
\[
\alpha\star\beta\ \defeq\ (\alpha\rightwhisker r) \ct (q\leftwhisker \beta)\, .
\]
Now suppose that $a \jdeq  b \jdeq  c$, so that all the above 1-paths are in $\Omega(A,a)$, and assume moreover that $q \jdeq  \refl{a}\jdeq  r$, so that $\alpha$ and $\beta$ become composable.  In that case, we therefore have
\[
\alpha\star\beta\ =\ (\alpha\rightwhisker\refl{a}) \ct (\refl{a}\leftwhisker \beta) = \alpha \ct \beta\, .
\]
On the other hand, we can define another horizontal composition analogously by
\[
\alpha\star'\beta\ \defeq\ (p\leftwhisker \beta)\ct (\alpha\rightwhisker s)\, .
\]
and setting $p \jdeq  \refl{a}\jdeq  s$ we learn that in that case 
\[
\alpha\star'\beta\ =\ (\refl{a}\leftwhisker \beta)\ct (\alpha\rightwhisker \refl{a}) = \beta\ct\alpha\, .
\]
But, in general, the two ways of defining horizontal composition agree, $\alpha\star\beta = \alpha\star'\beta$, as we can see by induction on $\alpha$ and $\beta$.  Thus when $p \jdeq  q \jdeq  \refl{a} \jdeq  r\jdeq  s$ we have
\[\alpha \ct \beta = \alpha\star\beta = \alpha\star'\beta = \beta\ct\alpha\,.
\qedhere
\]
\end{proof}

The foregoing fact, which is known as the \emph{Eckmann-Hilton argument}, comes from classical homotopy theory,  and indeed it is used in \autoref{cha:homotopy} below to show that the higher homotopy groups of a type are always abelian groups. 

As this example suggests, the algebra of higher path types is much more intricate than just the groupoid-like structure at each level; the levels interact to give many further operations and laws, as in the study of iterated loop spaces in homotopy theory.
Indeed, as in classical homotopy theory, we can make the following general definitions:

\begin{defn} \label{def:pointedtype}
  A \textbf{pointed type} $(A,a)$ is a type $A:\type$ together with a point $a:A$.
  We write $\pointed{\type} \defeq \sm{A:\type} A$ for the type of pointed types in the universe $\type$.
\end{defn}

\begin{defn} \label{def:loopspace}
  Given a pointed type $(A,a)$, we define the \textbf{loop space} of $(A,a)$ to be the following pointed type:
  \[\Omega(A,a)=((\id[A]aa),\refl{A}(a)).\]
  For $n:\N$, the \textbf{$n$-fold iterated loop space} of a pointed type $(A,a)$ is defined recursively by:
  \begin{align*}
    \Omega^0(A,a)&=(A,a)\\
    \Omega^{n+1}(A,a)&=\Omega^n(\Omega(A,a)).
  \end{align*}
\end{defn}

We will return to iterated loop spaces in \autoref{cha:hlevels,cha:hits,cha:homotopy}.


\section{Functions are functors}
\label{sec:functors}

Now we wish to establish that functions $f:A\to B$ behave functorially on paths.
In traditional type theory, this is equivalently the statement that functions respect equality.
Topologically, this corresponds to saying that every function is ``continuous'', i.e.\ preserves paths.

\begin{lem}\label{lem:map}
  Suppose that $f:A\to B$ is a function.
  Then for any $x,y:A$ there is an operation
  \begin{equation*}
    \apfunc f : (\id[A] x y) \to (\id[B] {f(x)} {f(y)}).
  \end{equation*}
  Moreover, for each $x:A$ we have $\apfunc{f}(\refl{x})\jdeq \refl{f(x)}$.
\end{lem}

The notation $\apfunc f$ can be read either as the \underline{ap}plication of $f$ to a path, or as the \underline{a}ction on \underline{p}aths of $f$.

\begin{proof}[First proof]
  Let $D:\prd{x,y:A}{p:x=y}\type$ be the type family defined by
  \[D(x,y,p)\defeq (f(x)= f(y)).\]
  Then we have
  \begin{equation*}
    d\defeq\lam{x} \refl{f(x)}:\prd{x:A} D(x,x,\refl{x}).
  \end{equation*}
  Applying $J$, we obtain $\apfunc f : \prd{x,y:A}{p:x=y}(f(x)=g(x))$.
  The conversion rule implies $\apfunc f({\refl{x}})\jdeq\refl{f(x)}$ for each $x:A$.
\end{proof}

\begin{proof}[Second proof]
  By induction, it suffices to assume $p$ is $\refl{x}$.
  In this case, we may define $\apfunc f(p) \defeq \refl{f(x)}:f(x)\jdeq f(x)$.
\end{proof}

We will often write $\apfunc f (p)$ as simply $\ap f p$.
This is strictly speaking ambiguous, but generally no confusion arises.
It matches the common convention in category theory of using the same symbol for the application of a functor to objects and to morphisms.

We note that $\apfunc{}$ behaves functorially, in all the ways that one might expect.

\begin{lem}\label{lem:ap-functor}
  For functions $f:A\to B$ and $g:B\to C$ and paths $p:\id[A]xy$ and $q:\id[b]yz$, we have:
  \begin{enumerate}
  \item $\apfunc f(p\ct q) = \apfunc f(p) \ct \apfunc f(q)$.\label{item:apfunctor-ct}
  \item $\apfunc f(\opp p) = \opp{\apfunc f (p)}$.\label{item:apfunctor-opp}
  \item $\apfunc g (\apfunc f(p)) = \apfunc{g\circ f} (p)$.\label{item:apfunctor-compose}
  \end{enumerate}
\end{lem}
\begin{proof}
  Left to the reader.
\end{proof}

Now, since \emph{dependently typed} functions are essential in type theory, we will also need a version of \autoref{lem:map} for these.
However, this is not quite so simple to state, because if $f:\prd{x:A} B(x)$ and $p:x=y$, then $f(x):B(x)$ and $f(y):B(y)$ are elements of distinct types, so that \emph{a priori} we cannot even ask whether they are equal.
The missing ingredient is that $p$ itself gives us a way to relate the types $B(x)$ and $B(y)$.

\begin{lem}[Transport]\label{lem:transport}
  Suppose that $P$ is a type family over $A$ and that $p:\id[A]xy$.
  Then there is a function $\transf{p}:P(x)\to P(y)$.
\end{lem}

\begin{proof}[First proof]
  Let $D:\prd{x,y:A}{p:\id{x}{y}} \type$ be the type family defined by
  \[D(x,y,p)\defeq P(x)\to P(y).\]
  Then we have the function
  \begin{equation*}
    d\defeq\lam{x} \idfunc[P(x)]:\prd{x:A} D(x,x,\refl{x}),
  \end{equation*}
  so that the induction principle gives us $J_{D,d}(x,y,p):P(x)\to P(y)$ for $p:x= y$, which we define to be $\transf p$.
\end{proof}

\begin{proof}[Second proof]
  By induction, it suffices to assume $p$ is $\refl x$.
  But in this case, we can take $\transf{(\refl x)}:P(x)\to P(x)$ to be the identity function.
\end{proof}

Sometimes, it is necessary to notate the type family $P$ in which the transport operation happens.
In this case, we may write
\[\transfib P p - : P(x) \to P(y).\]

Recall that a type family $P$ over a type $A$ can be seen as a property of elements of $A$, which holds at $x$ in $A$ if $P(x)$ is inhabited.
Then the transportation lemma says that $P$ respects equality, in the sense that if $x$ is equal to $y$, then $P(x)$ holds if and only if $P(y)$ holds.
In fact, we will see later on that if $x=y$ then actually $P(x)$ and $P(y)$ are \emph{equivalent}.

Topologically, the transportation lemma can be viewed as a ``path lifting'' operation in a fibration.
We think of a type family $P:A\to \type$ as a fibration with base $A$ and total space $\sm{x:A}P(x)$, with $P(x)$ being the fiber over $x$.
The defining property of a fibration is that given a path $p:x=y$ in the base space $A$ and a point $u:P(x)$ in the fiber over $x$, we may lift the path $p$ to a path in the total space starting at $u$.
The point $\trans p u$ can be thought of as the other endpoint of this lifted path.
We can also define the path itself in type theory:

\begin{lem}[Path lifting property]\label{thm:path-lifting}
  Let $P:A\to\type$ be a type family over $A$ and assume we have $u:P(x)$ for some $x:A$.
  Then for any $p:x=y$, we have
  \begin{equation*}
    \mathsf{lift}(u,p):(x,u)=(y,\trans{p}{u})
  \end{equation*}
  in $\sm{x:A}P(x)$.
\end{lem}
\begin{proof}
  Left to the reader.
  We will prove a more general theorem in \autoref{sec:compute-sigma}.
\end{proof}

% \begin{proof}[First proof]
% Let $D:\prd{x,y:A}{p:x=y}\type$ be defined by
% \begin{equation*}
% D(x,y,p)\defeq (x,u)=(y,\trans{p}{u}).
% \end{equation*}
% Then $D(x,x,\refl{x})\defeq (x,u)=(x,\trans{\refl{x}}{u})$. By the conversion rule we have $\trans{\refl{x}}{u}\defeq u$, so we see that $D(x,x,\refl{x})\defeq (x,u)=(x,u)$. Therefore we find $d(x)\defeq\refl{(x,u)}:D(x,x,\refl{x})$. Now path induction gives a function of type $\prd{x,y:A}{p:x=y}(x,u)=(y,\trans{p}{u})$.
% \end{proof}
% \begin{proof}[Second proof] 
%   By induction, it suffices to find an element of $(x,u)=(x,\trans{\refl{x}}{u})$.
%   Note that $\trans{\refl{x}}{u}\jdeq u$, so we really need to find an element of $(x,u)=(x,u)$.
%   But here we can use reflexivity.
% \end{proof}

Now we can prove the dependent version of \autoref{lem:map}.
The topological intuition is that given $f:\prd{x:A} P(x)$ and a path $p:\id[A]xy$, we ought to be able to apply $f$ to $p$ and obtain a path in the total space of $P$ which ``lies over'' $p$, as shown below.

\begin{center}
  \begin{tikzpicture}[yscale=.5,xscale=2]
    \draw (0,0) arc (-90:170:1cm) node[anchor=south east] {$A$} arc (170:270:1cm);
    \draw (0,4) arc (-90:170:1cm) node[anchor=south east] {$\sm{x:A} P(x)$} arc (170:270:1cm);
    \draw[->] (0,3.8) -- node[auto] {$\proj1$} (0,2.2);
    \node[circle,fill,inner sep=1pt,label=left:{$x$}] (b1) at (-.5,1) {};
    \node[circle,fill,inner sep=1pt,label=right:{$y$}] (b2) at (.5,1) {};
    \draw[decorate,decoration={snake,amplitude=1}] (b1) -- node[auto,swap] {$p$} (b2);
    \node[circle,fill,inner sep=1pt,label=left:{$f(x)$}] (b1) at (-.5,5) {};
    \node[circle,fill,inner sep=1pt,label=right:{$f(y)$}] (b2) at (.5,5) {};
    \draw[decorate,decoration={snake,amplitude=1}] (b1) -- node[auto] {$f(p)$} (b2);
  \end{tikzpicture}
\end{center}

We \emph{can} obtain such a thing from \autoref{lem:map}.
Given $f:\prd{x:A} P(x)$, we can define a non-dependent function $f':A\to \sm{x:A} P(x)$ by setting $f'(x)\defeq (x,f(x))$, and then consider $\ap{f'}{p} : f'(x) = f'(y)$.
However, it is not obvious from the type of such a path that it lies over a specific path in $A$ (in this case, $p$), which is sometimes important.

The solution is to use the transport lemma.
Since there is a canonical path from $u:P(x)$ to $\trans p u :P(y)$ which (at least intuitively) lies over $p$, any path from $u$ to $v:P(y)$ lying over $p$ should factor through this path, essentially uniquely, by a path from $\trans p u$ to $v$ lying entirely in the fiber $P(y)$.
Thus, up to equivalence, it makes sense to define ``a path from $u$ to $v$ lying over $p:x=y$'' to mean a path $\trans p u = v$ in $P(y)$.
And, indeed, we can show that dependent functions produce such paths.

\begin{lem}[Dependent map]\label{lem:mapdep}
  Suppose $f:\prd{x: A} P(x)$; then we have a map
  \[\apdfunc f : \prd{p:x=y}\big(\id[P(y)]{\trans p{f(x)}}{f(y)}\big).\]
\end{lem}

\begin{proof}[First proof]
  Let $D:\prd{x,y:A}{p:\id{x}{y}} \type$ be the type family defined by
  \begin{equation*}
    D(x,y,p)\defeq \trans p {f(x)}= f(y).
  \end{equation*}
  Then $D(x,x,\refl{x})$ is $\trans{(\refl{x})}{f(x)}= f(x)$.
  But since $\trans{(\refl{x})}{f(x)}\jdeq f(x)$, we get that $D(x,x,\refl{x})\jdeq (f(x)= f(x))$.
  Thus, we find the function
  \begin{equation*}
    d\defeq\lam{x} \refl{f(x)}:\prd{x:A} D(x,x,\refl{x})
  \end{equation*}
  and now $J$ gives us $\apdfunc f(p):\trans p{f(x)}= f(y)$ for each $p:x= y$.
\end{proof}

\begin{proof}[Second proof]
  By induction, it suffices to assume $p$ is $\refl x$.
  But in this case, the desired equation is $\trans{(\refl{x})}{f(x)}\jdeq f(x)$, which holds judgmentally.
\end{proof}

We will refer generally to paths which ``lie over other paths'' in this sense as \emph{dependent paths}.
They will play an increasingly important role starting in \autoref{cha:hits}.
In \autoref{sec:computational} we will see that for a few particular kinds of type families, there are equivalent ways to represent the notion of dependent paths that are sometimes more convenient.

Now recall from section \ref{sec:pi-types} that a non-dependently typed function $f:A\to B$ is just the special case of a dependently typed function $f:\prd{x:A} P(x)$ when $P$ is a constant type family, $P(x) \defeq B$.
In this case, $\apdfunc{f}$ and $\apfunc{f}$ are closely related, because of the following lemma:

\begin{lem}\label{thm:trans-trivial}
  If $P:A\to\type$ is defined by $P(x) \defeq B$ for a fixed $B:\type$, then for any $x,y:A$ and $p:x=y$ and $b:B$ we have a path
  \[ \transconst Bpb : \transfib P p b = b. \]
\end{lem}
\begin{proof}[First proof]
  Fix a $b:B$, and let $D:\prd{x,y:A}{p:\id{x}{y}} \type$ be the type family defined by
  \[ D(x,y,p) \defeq (\transfib P p b = b). \]
  Then $D(x,x,\refl x)$ is $(\transfib P{\refl{x}}{b} = b)$, which is judgmentally equal to $(b=b)$ by the computation rule for transporting.
  Thus, we have the function
  \[ d \defeq \lam{x} \refl{b} : \prd{x:A} D(x,x,\refl x). \]
  Now path induction gives us an element of $\prd{x,y:A}{p:x=y}(\transfib P p b = b)$, as desired.
\end{proof}
\begin{proof}[Second proof]
  By induction, it suffices to assume $y$ is $x$ and $p$ is $\refl x$.
  But $\transfib P {\refl x} b \jdeq b$, so in this case what we have to prove is $b=b$, and we have $\refl{b}$ for this.
\end{proof}

Thus, by concatenating with $\transconst B p b$, for any $x,y:A$ and $p:x=y$ and $f:A\to B$ we obtain functions
\begin{align}
  \big(f(x) = f(y)\big) &\to \big(\trans{p}{f(x)} = f(y)\big)\label{eq:ap-to-apd}
  \qquad\text{and} \\
  \big(\trans{p}{f(x)} = f(y)\big) &\to \big(f(x) = f(y)\big).\label{eq:apd-to-ap}
\end{align}
In fact, these functions are inverse equivalences (in the sense to be introduced in \autoref{sec:basics-equivalences}), and they relate $\apfunc f (p)$  to $\apdfunc f (p)$.

\begin{lem}\label{thm:apd-const}
  For $f:A\to B$ and $p:\id[A]xy$, we have
  \[ \apdfunc f(p) = \transconst B p{f(x)} \ct \apfunc f (p) \]
\end{lem}
\begin{proof}[First proof]
  Let $D:\prd{x,y:A}{p:\id xy} \type$ be the type family defined by
  \[ D(x,y,p) \defeq \big(\apdfunc f (p) = \transconst Bp{f(x)} \ct \apfunc f (p)\big). \]
  Thus, we have
  \[D(x,x,\refl x) \jdeq \big(\apdfunc f (\refl x) = \transconst B{\refl x}{f(x)} \ct \apfunc f ({\refl x})\big).\]
  But by definition, all three paths appearing in this type are $\refl{f(x)}$, so we have
  \[ \refl{\refl{f(x)}} : D(x,x,\refl x). \]
  Thus, path induction gives us an element of $\prd{x,y:A}{p:x=y} D(x,y,p)$, which is what we wanted.
\end{proof}
\begin{proof}[Second proof]
  By induction, it suffices to assume $y$ is $x$ and $p$ is $\refl x$.
  In this case, what we have to prove is $\refl{f(x)} = \refl{f(x)} \ct \refl{f(x)}$, which is true judgmentally.
\end{proof}

Because the types of $\apdfunc{f}$ and $\apfunc{f}$ are different, it is often clearer to use different notations for them.
% We may sometimes use a notation $\apd f p$ for $\apdfunc{f}(p)$, which is similar to the notation $\ap f p$ for $\apfunc{f}(p)$.

At this point, we hope the reader is starting to get a feel for proofs by induction on identity types.
From now on we stop giving both styles of proofs, allowing ourselves to use whatever is most clear and convenient (and often the second, more concise one).
Here are a few other useful lemmas about transport; we leave it to the reader to give the proofs (in either style).

\begin{lem}\label{thm:transport-concat}
  Given $P:A\to\type$ with $p:\id[A]xy$ and $q:\id[A]yz$ while $u:P(x)$, we have
  \[ \trans{q}{\trans{p}{u}} = \trans{(p\ct q)}{u}. \]
\end{lem}

\begin{lem}\label{thm:transport-compose}
  For a function $f:A\to B$ and a type family $P:B\to\type$, and any $p:\id[A]xy$ and $u:P(f(x))$, we have
  \[ \transfib{P\circ f}{p}{u} = \transfib{P}{\apfunc f(p)}{u}. \]
\end{lem}

\begin{lem}\label{thm:ap-transport}
  For $P,Q:A\to \type$ and a family of functions $f:\prd{x:A} P(x)\to Q(x)$, and any $p:\id[A]xy$ and $u:P(x)$, we have
  \[ \transfib{Q}{p}{f_x(u)} = f_y(\transfib{P}{p}{u}). \]
\end{lem}


\section{Summary of the basic higher structure}
\label{sec:basics-summary}

Here we summarize the basic definitions made in the previous two sections.

\begin{itemize}
\item $\opp{p} : y=x$, for $p:x=y$, defined by
  \[\opp{\;\refl{x}}\jdeq \refl{x}.\]
\item $p\ct q :y=z$, for $p:x=y$ and $q:y=z$, defined by
  \[ \refl{x}\ct\refl{x}\jdeq\refl{x}.\]
\item If $P$ is a type family over $A$ then $\transf{p}:P(x)\to P(y)$, for $p:x=y$, defined by
  \[\transf{(\refl{x})}\jdeq \idfunc[P(x)].\]
\item If $f:A\to B$ then $\map{f}{p}:f(x)=f(y)$, for $p:x=y$, defined by
  \[\map{f}{\refl{x}}\jdeq \refl{f(x)}.\]
\item If $f:\prod_{x:A}P(x)$ then $\mapdep{f}{p}:\trans{p}{f(x)}=f(y)$, for $p:x=y$, defined by
  \[\mapdep{f}{\refl{x}}\jdeq \refl{f(x)}.\]
\end{itemize}


\input{basics-equivalences}

\input{computational}


\section{Universal properties}
\label{sec:universal-properties}

By combining the path computation rules described in \autoref{sec:computational}, we can show that various type forming operations satisfy the expected universal properties.
For instance, given types $X,A,B$, we have a function
\begin{equation}
  (X\to A\times B) \;\to \; (X\to A)\times (X\to B)\label{eq:prod-ump-map}
\end{equation}
defined by $f \mapsto (\proj1 \circ f, \proj2\circ f)$.

\begin{thm}\label{thm:prod-ump}
  \eqref{eq:prod-ump-map} is an equivalence.
\end{thm}
\begin{proof}
  We define a quasi-inverse to send $(g,h)$ to the function $x\mapsto (g(x),h(x))$.
  (Technically, we have used the induction principle for the cartesian product $(X\to A)\times (X\to B)$, to reduce to the case of a pair.)

  Now given $f:X\to A\times B$, the round-trip composite yields the function
  \begin{equation}
    x\mapsto (\proj1(f(x)),\proj2(f(x))).\label{eq:prod-ump-rt1}
  \end{equation}
  By \autoref{thm:path-prod}, for any $x:X$ we have $(\proj1(f(x)),\proj2(f(x))) = f(x)$.
  Thus, by function extensionality, the function~\eqref{eq:prod-ump-rt1} is equal to $f$.

  On the other hand, given $(g,h)$, the round-trip composite yields the pair $(x\mapsto g(x),x\mapsto h(x))$.
  By function extensionaility, the two components of this are equal to $g$ and $h$ respectively, so by \autoref{thm:path-prod}, the pair is equal to $(g,h)$.
\end{proof}

In fact, we also have a dependently typed version of this universal property.
Suppose given a type $X$ and type families $A,B:X\to \type$.
Then we have a function
\begin{equation}\label{eq:prod-umpd-map}
  \Big(\prd{x:X} (A(x)\times B(x))\Big) \;\to\; \Big(\prd{x:X} A(x)\Big) \times \Big(\prd{x:X} B(x)\Big)
\end{equation}
defined as before by $f \mapsto (\proj1 \circ f, \proj2\circ f)$.

\begin{thm}\label{thm:prod-umpd}
  \eqref{eq:prod-umpd-map} is an equivalence.
\end{thm}
\begin{proof}
  Left to the reader.
\end{proof}

Just as $\Sigma$-types are a generalization of cartesian products, they satisfy a generalized version of this universal property.
Jumping right to the dependently typed version, suppose we have a type $X$ and type families $A:X\to \type$ and $P:\prd{x:X} A(x)\to\type$.
Then we have a function
\begin{equation}
  \label{eq:sigma-ump-map}
  \Big(\dprd{x:X}\dsm{a:A(x)} P(x,a)\Big)  \;\to\;
  \Big(\dsm{g:\prd{x:X} A(x)} \dprd{x:X} P(x,g(x))\Big)
\end{equation}
Note that if we have $P(x,a) \defeq B(x)$ for some $B:X\to\type$, then~\eqref{eq:sigma-ump-map} reduces to~\eqref{eq:prod-umpd-map}.

\begin{thm}\label{thm:ttac}
  \eqref{eq:sigma-ump-map} is an equivalence.
\end{thm}
\begin{proof}
  As before, we define a quasi-inverse to send $(g,h)$ to the function $x\mapsto (g(x),h(x))$.
  Now given $f:\prd{x:X} \sm{a:A(x)} P(x,a)$, the round-trip composite yields the function
  \begin{equation}
    x\mapsto (\proj1(f(x)),\proj2(f(x))).\label{eq:prod-ump-rt2}
  \end{equation}
  Now for any $x:X$, by \autoref{thm:eta-sigma} ($\eta$-equivalence for $\Sigma$-types) we have $$(\proj1(f(x)),\proj2(f(x))) = f(x).$$
  Thus, by function extensionality,~\eqref{eq:prod-ump-rt2} is equal to $f$.

  On the other hand, given $(g,h)$, the round-trip composite yields the pair $(x\mapsto g(x),x\mapsto h(x))$.
  But $x\mapsto g(x)$ and $x\mapsto h(x)$ are judgmentally equal to $g$ and $h$, respectively, and hence this pair of functions is also equal to $(g,h)$.
\end{proof}

This is noteworthy because the propositions-as-types interpretation of~\eqref{eq:sigma-ump-map} is ``the axiom of choice''.
If we read $\Sigma$ as ``there exists'' and $\Pi$ (sometimes) as ``for all'', we can pronounce:
\begin{itemize}
\item $\prd{x:X} \sm{a:A(x)} P(x,a)$ as ``for all $x:X$ there exists an $a:A(x)$ such that $P(x,a)$'', and
\item $\sm{g:\prd{x:X} A(x)} \prd{x:X} P(x,g(x))$ as ``there exists a choice function $g:\prd{x:X} A(x)$ such that for all $x:X$ we have $P(x,g(x))$''.
\end{itemize}
Thus, \autoref{thm:ttac} says that not only is the axiom of choice ``true'', it hypotheses are equivalent to its conclusion.
(On the other hand, the classical mathematician may find that~\eqref{eq:sigma-ump-map} does not carry the usual meaning of the axiom of choice, since we have already specified the values of $g$, and there are no choices left to be made.
We will return to this point in \autoref{sec:logic}.)

The above universal property for pair types is for ``mapping in'', which is familiar from the category-theoretic notion of products.
However, pair types also have a universal property for ``mapping out'', which may look less familiar.
In the case of cartesian products, the non-dependent version simply expresses the cartesian closedness adjunction:
\[ \eqvspaced{\big((A\times B) \to C\big)}{\big(A\to (B\to C)\big)}.\]
The dependent version of this is formulated for a type family $C:A\times B\to \type$:
\[ \eqvspaced{\Big(\prd{w:A\times B} C(w)\Big)}{\Big(\prd{x:A}{y:B} C(x,y)\Big)}. \]
Here the left-to-right function is simply the induction principle for $A\times B$, while the right-to-left is evaluation at a pair.
We leave it to the reader to prove that these are quasi-inverses.
There is also a version for $\Sigma$-types:
\begin{equation}
  \eqvspaced{\Big(\prd{w:\sm{x:A} B(x)} C(w)\Big)}{\Big(\prd{x:A}{y:B(x)} C(x,y)\Big)}\label{eq:sigma-lump}
\end{equation}
Again, the left-to-right function is the induction principle.

Some other induction principles are also part of universal properties of this sort.
For instance, path induction is the right-to-left direction of an equivalence as follows:
\begin{equation}
  \label{eq:path-lump}
  \eqvspaced{\Big(\prd{x:A}{p:a=x} B(x,p)\Big)}{B(a,\refl a)}
\end{equation}
for any $a:A$ and type family $B:\prd{x:A} (a=x) \to\type$.
However, inductive types with recursion, such as the natural numbers, have more complicated universal properties; see \autoref{cha:induction}.


\section{Sets and \texorpdfstring{$n$}{n}-types}
\label{sec:basics-sets}

While types in general behave like spaces or higher groupoids, there is a subclass of them that behave more like the sets in a traditional set-theoretic system.
Categorically, we may consider \emph{discrete} groupoids, which are determined by a set of objects and only identity morphisms and higher morphisms; while topologically, we may consider spaces having the discrete topology.
More generally, we may consider groupoids or spaces that are \emph{equivalent} to ones of this sort; since everything we do in type theory is up to homotopy, we can't expect to tell the difference.

Intuitively, we would expect a type to ``be a set'' in this sense if it has no higher homotopical information: any two parallel paths are equal (up to homotopy), and similarly for parallel higher paths at all dimensions.
Fortunately, because everything in homotopy type theory is automatically functorial/continuous, it turns out to be sufficient to ask this at the bottom level.

\begin{defn}\label{defn:set}
  A type $A$ is a \textbf{set} if for all $x,y:A$ and all $p,q:x=y$, we have $p=q$.
\end{defn}

More precisely, the proposition $\isset(A)$ is defined to be the type
\[ \isset(A) \defeq \prd{x,y:A}{p,q:x=y} (p=q). \]

As mentioned in \autoref{sec:types-vs-sets},
the sets in homotopy type theory are not like the sets in ZF set theory, in that there is no global ``membership predicate'' $\in$.
They are more like the sets used in structural mathematics and in category theory, whose elements are ``abstract points'' to which we give structure with functions and relations.
This is all we need in order to use them as a foundational system for most set-based mathematics; we will see some examples in \autoref{cha:set-math}.

Which types are sets?
In \autoref{cha:hlevels} we will study a more general form of this question in depth, but for now we can observe some easy examples.

\begin{eg}
  The type \unit is a set.
  For by \autoref{thm:path-unit}, for any $x,y:\unit$ the type $(x=y)$ is equivalent to \unit.
  Since any two elements of \unit are equal, this implies that any two elements of $x=y$ are equal.
\end{eg}

\begin{eg}
  The type $\emptyt$ is a set, for given any $x,y:\emptyt$ we may deduce anything we like by contradiction.
\end{eg}

\begin{eg}\label{thm:nat-set}
  The type \nat of natural numbers is also a set.
  This follows from \autoref{thm:path-nat}, since all equality types $\id[\nat]xy$ are equivalent to either \unit or \emptyt, and any two inhabitants of \unit or \emptyt are equal.
  We will see another proof of this fact in \autoref{cha:hlevels}.
\end{eg}

Most of the type forming operations we have considered so far also preserve sets.

\begin{eg}\label{thm:isset-prod}
  If $A$ and $B$ are sets, then so is $A\times B$.
  For given $x,y:A\times B$ and $p,q:x=y$, by \autoref{thm:path-prod} we have $p= \pairpath(\projpath1(p),\projpath2(p))$ and $q= \pairpath(\projpath1(q),\projpath2(q))$.
  But $\projpath1(p)=\projpath1(q)$ since $A$ is a set, and $\projpath2(p)=\projpath2(q)$ since $B$ is a set; hence $p=q$.

  Similarly, if $A$ is a set and $B:A\to\type$ is such that each $B(x)$ is a set, then $\sm{x:A} B(x)$ is a set.
\end{eg}

\begin{eg}\label{thm:isset-forall}
  If $A$ is \emph{any} type and $B:A\to \type$ is such that each $B(x)$ is a set, then the type $\prd{x:A} B(x)$ is a set.
  For suppose $f,g:\prd{x:A} B(x)$ and $p,q:f=g$.
  By function extensionality, we have $p = {\funext (x \mapsto \happly(p,x))}$ and likewise $q = {\funext (x \mapsto \happly(q,x))}$.
  But for any $x:A$, we have $\happly(p,x):f(x)=g(x)$ and also $\happly(q,x):f(x)=g(x)$, so since $B(x)$ is a set we have $\happly(p,x) = \happly(q,x)$.
  Now using function extensionality again, the dependent functions $(x \mapsto \happly(p,x))$ and $(x \mapsto \happly(q,x))$ are equal, and hence (applying $\apfunc{\funext}$) so are $p$ and $q$.
\end{eg}

For more examples, see \autoref{ex:isset-coprod,ex:isset-sigma}.  For a more systematic investigation of the subsystem (category) of all sets in homotopy type theory, see~\autoref{cha:set-math}.

Sets are just the first rung on a ladder of what are called \emph{homotopy $n$-types}.
The next rung consists of \emph{$1$-types}, which are analogous to $1$-groupoids in category theory.
The defining property of a set (which we may also call a \emph{$0$-type}) is that it has no non-trivial paths.
Similarly, the defining property of a $1$-type is that it has no non-trivial paths between paths:

\begin{defn}\label{defn:1type}
  A type $A$ is a \textbf{1-type} if for all $x,y:A$ and $p,q:x=y$ and $r,s:p=q$, we have $r=s$.
\end{defn}

Similary, we can define $2$-types, $3$-types, and so on.
We will define the general notion of $n$-type inductively in \autoref{cha:hlevels}, and study the relationships between them.

However, for now it is useful to have two facts in mind.
First, the levels are upward-closed: if $A$ is an $n$-type then $A$ is an $(n+1)$-type.
For example:
%%  will make precise the sense in which this
%% ``suffices for all higher levels'', but as an example, we observe that
%% it suffices for the next level up.

\begin{lem}\label{thm:isset-is1type}
  If $A$ is a set (that is, $\isset(A)$ is inhabited), then $A$ is a 1-type.
\end{lem}
\begin{proof}
  Suppose $f:\isset(A)$; then for any $x,y:A$ and $p,q:x=y$ we have $f(x,y,p,q):p=q$.
  Fix $x$, $y$, and $p$, and define $g: \prd{q:x=y} (p=q)$ by $g(q) \defeq f (x,y,p,q)$.
  Then for any $r:q=q'$, we have $\apdfunc{g}(r) : \trans{r}{g(q)} = g(q')$.
  By \autoref{cor:transport-path-prepost}, therefore, we have $g(q) \ct r = g(q')$.

  In particular, suppose given $x,y,p,q$ and $r,s:p=q$, as in \autoref{defn:1type}, and define $g$ as above.
  Then $g(p) \ct r = g(q)$ and also $g(p) \ct s = g(q)$, hence by cancellation $r=s$.
\end{proof}

Second, this stratification of types by level is not degenerate, in the
sense that not all types are sets:  

\begin{eg}\label{thm:type-is-not-a-set}
  The universe \type is not a set.
  To prove this, it suffices to exhibit a type $A$ and a path $p:A=A$ which is not equal to $\refl A$.
  Take $A=\bool$, and let $f:A\to A$ be defined by $f(\btrue)\defeq \bfalse$ and $f(\bfalse)\defeq \btrue$.
  Then $f(f(x))=x$ for all $x$ (by an easy case analysis), so $f$ is an equivalence.
  Hence, by univalence, $f$ gives rise to a path $p:A=A$.

  If $p$ were equal to $\refl A$, then (again by univalence) $f$ would equal the identity function of $A$.
  But this would imply that $\btrue=\bfalse$, contradicting \autoref{rmk:true-neq-false}.
\end{eg}

In \autoref{cha:hits,cha:homotopy} we will show that for any $n$, there are types which are not $n$-types.

Note that $A$ is a 1-type exactly when for any $x,y:A$, the identity type $\id[A]xy$ is a set.
(Thus, \autoref{thm:isset-is1type} could equivalently be read as saying that the identity types of a set are also sets.)
This will be the basis of the inductive definition of $n$-types we will give in \autoref{cha:hlevels}.

We can also extend this characterization ``downwards'' from sets.
That is, a type $A$ is a set just when for any $x,y:A$, any two elements of $\id[A]xy$ are equal.
Since sets are equivalently 0-types, it is natural to call a type a \emph{$(-1)$-type} if it has this latter property (any two elements of it are equal).
Such types may be regarded as \emph{propositions in a narrow sense}, and their study is just what is usually called ``logic''.


%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Logic}
\label{sec:logic}

Type theory, formal or informal, is a collection of rules for manipulating types and their elements.
But when writing mathematics informally in natural language, we generally use familiar words, particularly logical connectives such as ``and'' and ``or'', and logical quantifiers such as ``for all'' and ``there exists''.
In contrast to set theory, type theory offers us more than one way to regard these English phrases as operations on types.
This potential ambiguity needs to be resolved, by setting out local or global conventions, by introducing new annotations to informal mathematics, or both.
This requires some getting used to, but is offset by the fact that because type theory permits this finer analysis of logic, we can represent mathematics more faithfully, with fewer ``abuses of language'' than in set-theoretic foundations.
In this section we will explain the issues involved, and justify the choices we have made.


\subsection{Propositions as types?}
\label{subsec:pat?}

Until now, we have been following the straightforward ``propositions as types'' philosophy described in \autoref{sec:pat}, according to which English phrases such as ``there exists an $x:A$ such that $P(x)$'' are interpreted by corresponding types such as $\sm{x:A} P(x)$, with the proof of a statement being regarded as judging some specific element to inhabit that type.
However, we have also seen some ways in which the ``logic'' resulting from this reading seems unfamiliar to a classical mathematician.
For instance, in \autoref{thm:ttac} we saw that the statement
\begin{equation}\label{eq:english-ac}
  \parbox{\textwidth-2cm}{``If for all $x:X$ there exists an $a:A(x)$ such that $P(x,a)$, then there exists a function $g:\prd{x:A} A(x)$ such that for all $x:X$ we have $P(x,g(x))$,''}
\end{equation}
which looks like the classical \emph{axiom of choice}, is always true under this reading. This is a noteworthy, and often useful, feature of the propositions-as-types logic, but it also illustrates how significantly it differs from the classical interpretation of logic, under which the axiom of choice is not a logical truth, but an additional ``axiom".

On the other hand, we can now also show that corresponding statements looking like the classical \emph{law of double negation} and \emph{law of excluded middle} are incompatible with the univalence axiom.

\begin{thm}\label{thm:not-dneg}
  It is not the case that for all $A:\UU$ we have $\neg(\neg A) \to A$.
\end{thm}
\begin{proof}
  Recall that $\neg A \jdeq (A\to\emptyt)$.
  We also read ``it is not the case that \dots'' as the operator $\neg$.
  Thus, in order to prove this statement, it suffices to assume given some $f:\prd{A:\UU} (\neg\neg A \to A)$ and construct an element of \emptyt.

  The idea of the following proof is to observe that $f$, like any function in type theory, is ``continuous''.
  By univalence, this implies that $f$ is \emph{natural} with respect to equivalences of types.
  From this, and a fixed-point-free autoequivalence, we will be able to extract a contradiction.

  Let $e:\eqv\bool\bool$ be the equivalence defined by $e(\bfalse)\defeq\btrue$ and $e(\btrue)\defeq\bfalse$, as in \autoref{thm:type-is-not-a-set}.
  Let $p:\bool=\bool$ be the path corresponding to $e$ by univalence, i.e.\ $p\defeq \ua(e)$.
  Then we have $f(\bool) : \neg\neg\bool \to\bool$ and
  \[\apd f p : \transfib{A\mapsto (\neg\neg A \to A)}{p}{f(\bool)} = f(\bool).\]
  Hence, for any $u:\neg\neg\bool$, we have
  \[\happly(\apd f p,u) : \transfib{A\mapsto (\neg\neg A \to A)}{p}{f(\bool)}(u) = f(\bool)(u).\]

  Now by~\eqref{eq:transport-arrow}, transporting $f(\bool):\neg\neg\bool\to\bool$ along $p$ in the type family ${A\mapsto (\neg\neg A \to A)}$ is equal to the function which transports its argument along $\opp p$ in the type family $A\mapsto \neg\neg A$, applies $f(\bool)$, then transports the result along $p$ in the type family $A\mapsto A$:
  \[ \transfib{A\mapsto (\neg\neg A \to A)}{p}{f(\bool)}(u) =
  \transfib{A\mapsto A}{p}{f(\bool) (\transfib{A\mapsto \neg\neg A}{\opp{p}}{u})}
  \]
  However, any two points $u,v:\neg\neg\bool$ are equal by function extensionality, since for any $x:\neg\bool$ we have $u(x):\emptyt$ and thus we can derive any conclusion, in particular $u(x)=v(x)$.
  Thus, we have $\transfib{A\mapsto \neg\neg A}{\opp{p}}{u} = u$, and so from $\happly(\apd f p,u)$ we obtain an equality
  \[ \transfib{A\mapsto A}{p}{f(\bool)(u)} = f(\bool)(u).\]
  Finally, as discussed in \autoref{sec:compute-universe}, transporting in the type family $A\mapsto A$ along the path $p\jdeq \ua(e)$ is equivalent to applying the equivalence $e$; thus we have
  \begin{equation}
    e(f(\bool)(u)) = f(\bool)(u).\label{eq:fpaut}
  \end{equation}

  However, we can also prove that
  \begin{equation}
    \prd{x:\bool} \neg(e(x)=x).\label{eq:fpfaut}
  \end{equation}
  This follows from a case analysis on $x$: both cases are immediate from the definition of $e$ and the injectivity of $\inl$ and $\inr$ which we proved in \autoref{sec:compute-coprod}.
  Thus, applying~\eqref{eq:fpfaut} to $f(\bool)(u)$ and~\eqref{eq:fpaut}, we obtain an element of $\emptyt$.
\end{proof}

\begin{rmk}
  In particular, this implies that there can be no Hilbert-style ``choice operator'' which selects an element of every nonempty type.
  The point is that no such operator can be \emph{natural}, and under the univalence axiom, all functions acting on types must be natural with respect to equivalences.
\end{rmk}

\begin{rmk}
  It is, however, still the case that $\neg\neg\neg A \to \neg A$ for any $A$; see \autoref{ex:neg-ldn}.
\end{rmk}

\begin{cor}\label{thm:not-lem}
  It is not the case that for all $A:\UU$ we have $A+(\neg A)$.
\end{cor}
\begin{proof}
  Suppose we had $g:\prd{A:\UU} (A+(\neg A))$.
  We will show that then $\prd{A:\UU} (\neg\neg A \to A)$, so that we can apply \autoref{thm:not-dneg}.
  Thus, suppose $A:\UU$ and $u:\neg\neg A$; we want to construct an element of $A$.

  Now $g(A):A+(\neg A)$, so by case analysis, we may assume either $g(A)\jdeq \inl(a)$ for some $a:A$, or $g(A)\jdeq \inr(w)$ for some $w:\neg A$.
  In the first case, we have $a:A$, while in the second case we have $u(w):\emptyt$ and so we can obtain anything we wish (such as $A$).
  Thus, in both cases we have an element of $A$, as desired.
\end{proof}

Thus, if we want to assume the univalence axiom (which, of course, we do) and still leave ourselves the option of classical reasoning (which is also desirable), we cannot use the unmodified propositions-as-types principle to interpret \emph{all} informal mathematical statements into type theory, since then the law of excluded middle would be false.
However, neither do we want to discard propositions-as-types entirely, because of its many good properties (such as simplicity, constructivity, and computability).
We now discuss a modification of propositions-as-types which resolves these problems; in \autoref{subsec:when-trunc} we will return to the question of which logic to use when.


\subsection{Mere propositions}
\label{subsec:hprops}

We have seen that the propositions-as-types logic has both good and bad properties.
Both have a common cause: when types are viewed as propositions, they can contain more information than mere truth or falsity, and all ``logical'' constructions on them must respect this additional information.
This suggests that we could obtain a more conventional logic by restricting attention to types that do \emph{not} contain any more information than a truth value, and only regarding these as logical propositions.

Such a type $A$ will be ``true'' if it is inhabited, and ``false'' if its inhabitation yields a contradiction (i.e.\ if $\neg A \jdeq (A\to\emptyt)$ is inhabited).
What we want to avoid, in order to obtain a more traditional sort of logic, is treating as logical propositions those types for which giving an element of them gives more information than simply knowing that the type is inhabited.
For instance, if we are given an element of \bool, then we receive more information than the mere fact that \bool contains some element.
Indeed, we receive exactly \emph{one bit} more information: we know \emph{which} element of \bool we were given.
By contrast, if we are given an element of \unit, then we receive no more information than the mere fact that \unit contains an element, since any two elements of \unit are equal to each other.
This suggests the following definition.

\begin{defn}
  A type $P$ is a \textbf{mere proposition} if for all $x,y:P$ we have $x=y$.
\end{defn}

Note that since we are still doing mathematics \emph{in} type theory, this is a definition \emph{in} type theory, which means it is a type --- or, rather, a type family.
Specifically, for any $P:\type$, the type $\isprop(P)$ is defined to be
\[ \isprop(P) \defeq \prd{x,y:P} (x=y). \]
Thus, to assert that ``$P$ is a mere proposition'' means to exhibit an inhabitant of $\isprop(P)$, which is a dependent function connecting any two elements of $P$ by a path.
The continuity/naturality of this function implies that not only are any two elements of $P$ equal, but $P$ contains no higher homotopy either.

\begin{lem}\label{thm:inhabprop-eqvunit}
  If $P$ is a mere proposition and $x_0:P$, then $\eqv P \unit$.
\end{lem}
\begin{proof}
  Define $f:P\to\unit$ by $f(x)\defeq \ttt$, and $g:\unit\to P$ by $g(u)\defeq x_0$.
  The claim follows from the next lemma, and the observation that \unit is a mere proposition by \autoref{thm:path-unit}.
\end{proof}

\begin{lem}\label{lem:equiv-iff-hprop}
  If $P$ and $Q$ are mere propositions such that $P\to Q$ and $Q\to P$, then $\eqv P Q$.
\end{lem}
\begin{proof}
  Suppose given $f:P\to Q$ and $g:Q\to P$.
  Then for any $x:P$, we have $g(f(x))=x$ since $P$ is a mere proposition.
  Similarly, for any $y:Q$ we have $f(g(y))=y$ since $Q$ is a mere proposition; thus $f$ and $g$ are quasi-inverses.
\end{proof}

In homotopy theory, a space that is homotopy equivalent to \unit is said to be \emph{contractible}.
Thus, any mere proposition which is inhabited is contractible (see also \autoref{sec:contractibility}).
On the other hand, the uninhabited type \emptyt is also (vacuously) a mere proposition.
In classical mathematics, at least, these are the only two possibilities.

Mere propositions are also called \emph{subterminal objects} (if thinking categorically), \emph{subsingletons} (if thinking set-theoretically), or \emph{h-propositions}.
The discussion in \autoref{sec:basics-sets} suggests we should also call them \emph{$(-1)$-types}; we will return to this in \autoref{cha:hlevels}.
The adjective ``mere'' emphasizes that although any type may be regarded as a proposition (which we prove by giving an inhabitant of it), a type that is a mere proposition cannot usefully be regarded as any \emph{more} than a proposition: there is no additional information contained in a witness of its truth.

Note that a type $A$ is a set if and only if for all $x,y:A$, the identity type $\id[A]xy$ is a mere proposition.
On the other hand, by copying and simplifying the proof of \autoref{thm:isset-is1type}, we have:

\begin{lem}\label{thm:prop-set}
  Every mere proposition is a set.
\end{lem}
\begin{proof}
  Suppose $f:\isprop(A)$; thus for all $x,y:A$ we have $f(x,y):x=y$.  Fix $x:A$
  and define $g(y)\defeq f(x,y)$.   Then for any $y,z:A$ and $p:y=z$ we have $\apd
  g p : \trans{p}{g(y)}={g(z)}$.  Hence by \autoref{cor:transport-path-prepost}, we have
  $g(y)\ct p = g(z)$, which is to say that $p=\opp{g(y)}\ct g(z)$.  Thus, for
  any $p,q:x=y$, we have $p = \opp{g(x)}\ct g(y) = q$.
\end{proof}

In particular, this implies:

\begin{lem}\label{thm:isprop-isprop}\label{thm:isprop-isset}
  For any type $A$, the types $\isprop(A)$ and $\isset(A)$ are mere propositions.
\end{lem}
\begin{proof}
  Suppose $f,g:\isprop(A)$.  By function extensionality, to show $f=g$ it
  suffices to show $f(x,y)=g(x,y)$ for any $x,y:A$.  But $f(x,y)$ and $g(x,y)$
  are both paths in $A$, and hence are equal because, by either $f$ or $g$, we
  have that $A$ is a proposition, and hence by Lemma~\ref{thm:prop-set} is a
  set.  Similarly, suppose $f,g:\isset(A)$, which is to say that for all
  $a,b:A$, $f(a,b):a=b$ and $g(a,b):a=b$.  But by then since $A$ is a set (by
  either $f$ or $g$), it follows that $f(a,b)=g(a,b)$, and hence $f=g$ by
  function extensionality.
\end{proof}

We have seen one other example so far: condition~\ref{item:be3} in \autoref{sec:basics-equivalences} asserts that for any function $f$, the type $\isequiv (f)$ should be a mere proposition.


\subsection{Classical vs.\ intuitionistic logic}
\label{sec:intuitionism}

With the notion of mere proposition in hand, we can now give the proper formulation of the \emph{law of excluded middle} in homotopy type theory:
\begin{equation}
  \label{eq:lem}
  \mathsf{LEM}\;\defeq\;
  \prd{A:\UU} \Big(\isprop(A) \to (A + \neg A)\Big).
\end{equation}
Similarly, the \emph{law of double negation} is
\begin{equation}
  \label{eq:ldn}
  \mathsf{DN}\;\defeq\;
  \prd{A:\UU} \Big(\isprop(A) \to (\neg\neg A \to A)\Big).
\end{equation}
These formulations avoid the paradoxes of \autoref{thm:not-dneg,thm:not-lem}, since \bool is not a mere proposition.
In order to distinguish these from the more general Propositions-as-Types formulations, we rename the latter:
\begin{align*}
  \mathsf{LEM}_\infty\;\defeq\;&
  \prd{A:\UU} (A + \neg A)\\
  \mathsf{DN}_\infty\;\defeq\;&
  \prd{A:\UU}(\neg\neg A \to A).
\end{align*}


Although $\mathsf{LEM}$ and $\mathsf{DN}$  are not consequences of the basic type theory described in \autoref{cha:typetheory}, they may be consistently assumed as axioms (unlike their $\infty$ counterparts).
For instance, we will assume them in \autoref{sec:wellorderings}.
(The two are also easily seen to be equivalent to each other; see \autoref{ex:lem-ldn}.)

However, it can be surprising how far we can get without using such axioms.
Quite often, a simple reformulation of a definition or theorem enables us to avoid invoking excluded middle or double negation.
While this takes a little getting used to sometimes, it is often worth the hassle, resulting in more elegant and more general proofs.
We discussed some of the benefits of this in the introduction.

For instance, in classical mathematics, double negations are frequently used unnecessarily.
A very simple example is the common assumption that a set $A$ is ``nonempty'', which literally means it is \emph{not} the case that $A$ contains \emph{no} elements.
Almost always what is really meant is the positive assertion that $A$ \emph{does} contain at least one element, and by removing the double negation we make the statement less dependent on LEM.
Thus we say that a type $A$ is \textbf{inhabited} to mean that we assert $A$ itself as a proposition (i.e.\ we construct an element of $A$, usually unnamed).

Similarly, it is not uncommon in classical mathematics to find unnecessary proofs by contradiction.
Of course, proof by contradiction proceeds by way of the law of double negation: we assume $\neg A$ and derive a contradiction, thereby deducing $\neg \neg A$, and thus by DN we obtain $A$.
However, often the derivation of a contradiction from $\neg A$ can be rephrased slightly so as to yield a direct proof of $A$, avoiding the need for DN.

It is also important to note that if the goal is to prove a \emph{negation}, then ``proof by contradiction'' does not involve DN.
In fact, since $\neg A$ is by definition the type $A\to\emptyt$, by definition to prove $\neg A$ is to prove a contradiction (\emptyt) under the assumption of $A$.
Similarly, the law of double negation does hold for negated propositions: $\neg\neg\neg A \to \neg A$.
With practice, one learns to distinguish more carefully between negated and un-negated propositions and to notice when LEM and DN are being used and when they are not.

Thus, contrary to how it may appear on the surface, doing mathematics ``constructively'' does not usually involve giving up important theorems, but rather finding the best way to state the definitions so as to make the important theorems constructively provable.
That is, we may freely use the LEM when first investigating a subject, but once that subject is better understood, we can hope to refine its definitions and proofs so as to avoid that axiom.
% For instance, the theory of ordinal numbers, which classically makes heavy use of LEM, works quite well constructively once we choose the correct definition of ``ordinal''; see \autoref{sec:ordinals}.
This sort of observation is even more pronounced in \emph{homotopy} type theory, where the powerful tools of univalence and higher inductive types allow us to constructively attack many problems that traditionally would require classical reasoning.
We will see several examples of this in \autoref{part:mathematics}.
% For instance, none of the ``synthetic'' homotopy theory we will develop in \autoref{cha:homotopy} requires LEM or DN --- despite the fact that classical homotopy theory (formulated using topological spaces or simplicial sets) makes heavy use of them (as well as the axiom of choice).



\subsection{Subsets}
\label{subsec:prop-subsets}

As another example of the usefulness of mere propositions, we discuss subsets (and more generally subtypes).
Suppose $P:A\to\type$ is a type family, with each type $P(x)$ regarded as a proposition.
Then $P$ itself is a \emph{predicate} on $A$, or a \emph{property} of elements of $A$.

In set theory, whenever we have a predicate on $P$ on a set $A$, we may form the subset $\setof{x\in A | P(x)}$.
In type theory, the obvious analogue is the $\Sigma$-type $\sm{x:A} P(x)$.
An inhabitant of $\sm{x:A} P(x)$ is, of course, a pair $(x,p)$ where $x:A$ and $p$ is a proof of $P(x)$.
However, for general $P$, an element $a:A$ might give rise to more than one distinct element of $\sm{x:A} P(x)$, if the proposition $P(a)$ has more than one distinct proof.
This is counter to the usual intuition of a \emph{subset}.
But if $P$ is a \emph{mere} proposition, then this cannot happen.

\begin{lem}\label{thm:path-subset}
  Suppose $P:A\to\type$ is a type family such that $P(x)$ is a mere proposition for all $x:A$.
  If $u,v:\sm{x:A} P(x)$ are such that $\proj1(u) = \proj1(v)$, then $u=v$.
\end{lem}
\begin{proof}
  Suppose $p:\proj1(u) = \proj1(v)$.
  By \autoref{thm:path-sigma}, to show $u=v$ it suffices to show $\trans{p}{\proj2(u)} = \proj2(v)$.
  But $\trans{p}{\proj2(u)}$ and $\proj2(v)$ are both elements of $P(\proj1(v))$, which is a mere proposition; hence they are equal.
\end{proof}

For instance, recall that in \autoref{sec:basics-equivalences} we defined
\[(\eqv A B) \;\defeq\; \sm{f:A\to B} \isequiv (f),\]
where each type $\isequiv (f)$ was supposed to be a mere proposition.
It follows that if two equivalences have equal underlying functions, then they are equal as equivalences.

Henceforth, if $P:A\to \type$ is a family of mere propositions, we may write
\[\setof{x:A | P(x)}\]
as an alternative notation for $\sm{x:A} P(x)$.
If $A$ is a set, we call $\setof{x:A | P(x)}$ a \textbf{subset} of $A$; for general $A$ we might call it a \textbf{subtype}.

As another example, we may define the ``subuniverses'' of sets and of mere propositions in a universe \UU:
\begin{align*}
  \set_\UU &\defeq \setof{A:\UU | \isset(A) }\\
  \prop_\UU &\defeq \setof{A:\UU | \isprop(A) }.
\end{align*}
An element of $\set_\UU$ is a type $A:\UU$ together with evidence $s:\isset(A)$, and similarly for $\prop_\UU$.
\autoref{thm:path-subset} implies that $\id[\set_\UU]{(A,s)}{(B,t)}$ is equivalent to $\id[\UU]AB$ (and hence to $\eqv AB$).
Thus, we will frequently abuse notation and write simply $A:\set_\UU$ instead of $(A,s):\set_\UU$.
We may also drop the subscript \UU if there is no need to specify the universe in question.

Recall that for any two universes $\UU_i$ and $\UU_{i+1}$, if $A:\UU_i$ then also $A:\UU_{i+1}$.
Thus, for any $(A,s):\set_{\UU_i}$ we also have $(A,s):\set_{\UU_{i+1}}$, and similarly for $\prop_{\UU_i}$, giving natural maps
\begin{align}
  \set_{\UU_i} &\to \set_{\UU_{i+1}}\label{eq:set-up}\\
  \prop_{\UU_i} &\to \prop_{\UU_{i+1}}.\label{eq:prop-up}
\end{align}
The map~\eqref{eq:set-up} cannot be an equivalence, since then we could reproduce the paradoxes of Cantorian set theory.
However, although~\eqref{eq:prop-up} is not automatically an equivalence in the type theory we have presented so far, it is consistent to suppose that it is.
This axiom is called \textbf{impredicativity for mere propositions}.
It follows automatically if $\UU_{i+1}$ satisfies LEM (see \autoref{ex:lem-impred}).

One use for impredicativity is to define powersets.
It is natural to define the powerset of a set $A$ to be $A\to\prop_\UU$; but in the absence of impredicativity, this definition depends (even up to equivalence) on the choice of the universe \UU.
Impredicativity means that we may as well take it to be the smallest universe $\UU_0$, since any other universe would give an equivalent result.
See also \autoref{subsec:piw}.


\subsection{The logic of mere propositions}
\label{subsec:logic-hprop}

We mentioned in \autoref{sec:types-vs-sets} that in contrast to type theory, which has only one basic notion (types), set-theoretic foundations have two basic notions: sets and propositions.
Thus, a classical mathematician is accustomed to manipulating these two kinds of objects separately.

It is possible to recover a similar dichotomy in type theory, with the role of the set-theoretic propositions being played by the types (and type families) that are \emph{mere} propositions.
In many cases, the logical connectives and quantifiers can be represented in this logic by simply restricting the corresponding type-former to the mere propositions.
Of course, this requires knowing that the type-former in question preserves mere propositions.

\begin{eg}
  If $A$ and $B$ are mere propositions, so is $A\times B$.
  This is easy to show using the characterization of paths in products, just like \autoref{thm:isset-prod} but simpler.
  Thus, the connective ``and'' preserves mere propositions.
\end{eg}

\begin{eg}\label{thm:isprop-forall}
  If $A$ is any type and $B:A\to \type$ is such that for all $x:A$, the type $B(x)$ is a mere proposition, then $\prd{x:A} B(x)$ is a mere proposition.
  The proof is just like \autoref{thm:isset-forall} but simpler: given $f,g:\prd{x:A} B(x)$, for any $x:A$ we have $f(x)=g(x)$ since $B(x)$ is a mere proposition.
  But then by function extensionality, we have $f=g$.

  In particular, if $B$ is a mere proposition, then so is $A\to B$ regardless of what $A$ is.
  In even more particular, since \emptyt is a mere proposition, so is $\neg A \jdeq (A\to\emptyt)$.
  Thus, the connectives ``implies'' and ``not'' preserve mere propositions, as does the quantifier ``for all''.
\end{eg}

On the other hand, some type formers do not preserve mere propositions.
Even if $A$ and $B$ are mere propositions, $A+B$ will not in general be.
For instance, \unit is a mere proposition, but $\bool=\unit+\unit$ is not.
Logically speaking, $A+B$ is a ``purely constructive'' sort of ``or'': a witness of it contains the additional information of \emph{which} disjunct is true.
Sometimes this is very useful, but if we want a more classical sort of ``or'' that preserves mere propositions, we need a way to ``truncate'' this type into a mere proposition by forgetting this additional information.

The same issue arises with the $\Sigma$-type $\sm{x:A} P(x)$.
This is a purely constructive interpretation of ``there exists an $x:A$ such that $P(x)$'' which remembers the witness $x$, and hence is not generally a mere proposition even if each type $P(x)$ is.
(Recall that we observed in \autoref{subsec:prop-subsets} that $\sm{x:A} P(x)$ can also be regarded as ``the subset of those $x:A$ such that $P(x)$''.)


\subsection{Propositional truncation}
\label{subsec:prop-trunc}

The \emph{propositional truncation}, also called the \emph{$(-1)$-truncation}, \emph{bracket type}, or \emph{squash type}, is an additional type former which ``truncates'' or ``squashes'' a type down to a mere proposition, forgetting all information contained in inhabitants of that type other than their existence.

More precisely, for any type $A$, there is a type $\brck{A}$.
It has two constructors:
\begin{itemize}
\item For any $a:A$ we have $\bproj a : \brck A$.
\item For any $x,y:\brck A$, we have $x=y$.
\end{itemize}
The first constructor means that if $A$ is inhabited, so is $\brck A$.
The second ensures that $\brck A$ is a mere proposition; usually we leave the witness of this fact nameless.

The induction principle of $\brck A$ says that:
\begin{itemize}
\item If $B$ is a mere proposition and we have $f:A\to B$, then there is an induced $g:\brck A \to B$ such that $g(\bproj a) \jdeq f(a)$ for all $a:A$.
\end{itemize}
In other words, any mere proposition which follows from (the inhabitedness of) $A$ already follows from $\brck A$.
Thus, $\brck A$, as a mere proposition, contains no more information than the inhabitedness of $A$.

In \autoref{ex:lem-brck,ex:impred-brck,sec:hittruncations} we will describe some ways to construct $\brck{A}$ in terms of more general things.
For now, we simply assume it as an additional rule alongside those of \autoref{cha:typetheory}.

With the propositional truncation, we can extend the ``logic of mere propositions'' to cover disjunction and the existential quantifier.
Specifically, $\brck{A+B}$ is a mere propositional version of ``$A$ or $B$'', which does not ``remember'' the information of which disjunct is true.

The induction principle of truncation implies that we can still do a case analysis on $\brck{A+B}$ \emph{when attempting to prove a mere proposition}.
That is, suppose we have an assumption $u:\brck{A+B}$ and we are trying to prove a mere proposition $Q$.
In other words, we are trying to define an element of $\brck{A+B} \to Q$.
Since $Q$ is a mere proposition, by the induction principle for propositional truncation, it suffices to construct a function $A+B\to Q$.
But now we can use case analysis on $A+B$.

Similarly, for a type family $P:A\to\type$, we can consider $\brck{\sm{x:A} P(x)}$, which is a mere propositional version of ``there exists an $x:A$ such that $P(x)$''.
As for disjunction, by combining the induction principles of truncation and $\Sigma$-types, if we have an assumption of type $\brck{\sm{x:A} P(x)}$, we may introduce new assumptions $x:A$ and $y:P(x)$ \emph{when attempting to prove a mere proposition}.
In other words, if we know that there exists some $x:A$ such that $P(x)$, but we don't have a particular such $x$ in hand, then we are free to make use of such an $x$ as long as we aren't trying to construct anything which might depend on the particular value of $x$.
Requiring the codomain to be a mere proposition expresses this independence of the result on the witness, since all possible inhabitants of such a type must be equal.

For the purposes of set-level mathematics in \autoref{cha:real-numbers,cha:set-math},
where we deal mostly with sets and mere propositions, it is convenient to use the
traditional logical notations to refer only to ``propositionally truncated logic''.

\begin{defn} \label{defn:logical-notation}
  We define \emph{traditional logical notation} using truncation as follows, where $P$ and $Q$ denote mere propositions (or families thereof):
  \begin{align*}
    \top            &\ \defeq \ \unit \\
    \bot            &\ \defeq \ \emptyt \\
    P \land Q       &\ \defeq \ P \times Q \\
    P \Rightarrow Q &\ \defeq \ P \to Q \\
    P \Leftrightarrow Q &\ \defeq \ P = Q \\
    \neg P          &\ \defeq \ P \to \emptyt \\
    P \lor Q        &\ \defeq \ \brck{P + Q} \\
    \fall{x : A} P(x) &\ \defeq \ \prd{x : A} P(x) \\
    \exis{x : A} P(x) &\ \defeq \ \brck{\sm{x : A} P(x)}
  \end{align*}
\end{defn}

The notations $\land$ and $\lor$ are also used in homotopy theory for the smash product and the wedge of pointed spaces, which we will introduce in \autoref{cha:hits}.
This technically creates a potential for conflict, but no confusion will generally arise.


\subsection{The Axiom of Choice}
\label{sec:axiom-choice}

We can now properly formulate the \emph{axiom of choice} in homotopy type theory.
Assume a type $X$ and type families $A:X\to\type$ and $P:\prd{x:X} A(x)\to\type$, and moreover that
\begin{itemize}
\item $X$ is a set,
\item $A(x)$ is a set for all $x:X$, and
\item $P(x,a)$ is a mere proposition for all $x:X$ and $a:A(x)$.
\end{itemize}
The \textbf{axiom of choice} asserts that under these assumptions,
\begin{equation}\label{eq:ac}
  \left(\prd{x:X} \brck{\sm{a:A(x)} P(x,a)}\right)
  \to
  \brck{\sm{g:\prd{x:X} A(x)} \prd{x:X} P(x,g(x))}
\end{equation}
Of course, this is a direct translation of~\eqref{eq:english-ac} where we read ``there exists $x:A$ such that $B(x)$'' as $\brck{\sm{x:A}B(x)}$, so we could have written the statement in the familiar logical notation as
\begin{equation*}
  \textstyle
  \Big(\fall{x:X}\exis{a:A(x)} P(x,a)\Big)
  \Rightarrow
  \Big(\exis{g : (\prd{x:X} A(x))} \fall{x : X} P(x,g(x))\Big).
\end{equation*}
%
In particular, note that the propositional truncation appears twice.
The truncation in the domain means we assume that for every $x$ there exists some $a:A(x)$ such that $P(x,a)$, but that these values are not chosen or specified in any known way.
The truncation in the codomain means we conclude that there exists some function $g$, but this function is not determined or specified in any known way.

In fact, because of \autoref{thm:ttac}, this axiom can also be expressed in a simpler form.

\begin{lem}\label{thm:ac-epis-split}
  The axiom of choice~\eqref{eq:ac} is equivalent to the statement that for any set $X$ and any $Y:X\to\type$ such that each $Y(x)$ is a set, we have
  \begin{equation}
    \left(\prd{x:X} \brck{Y(x)}\right)
    \to
    \brck{\prd{x:X} Y(x)}.\label{eq:epis-split}
  \end{equation}
\end{lem}

This corresponds to a well-known equivalent form of the classical axiom of choice, namely ``the cartesian product of a family of nonempty sets is nonempty.''

\begin{proof}
  By \autoref{thm:ttac}, the codomain of~\eqref{eq:ac} is equivalent to
  \[\brck{\prd{x:X} \sm{a:A(x)} P(x,a)}.\]
  Thus,~\eqref{eq:ac} is equivalent to the instance of~\eqref{eq:epis-split} where $Y(x) \defeq \sm{a:A(x)} P(x,a)$.
  Conversely,~\eqref{eq:epis-split} is equivalent to the instance of~\eqref{eq:ac} where $A(x)\defeq Y(x)$ and $P(x,a)\defeq\unit$.
  Thus, the two are logically equivalent.
  Since both are mere propositions, by \autoref{lem:equiv-iff-hprop} they are equivalent types.
\end{proof}

As with LEM and DN, the equivalent forms~\eqref{eq:ac} and~\eqref{eq:epis-split} are not a consequence of our basic type theory, but they may consistently be assumed as axioms.

\begin{rmk}
  It is easy to show that the right side of~\eqref{eq:epis-split} always implies the left.
  Since both are mere propositions, by \autoref{lem:equiv-iff-hprop} the axiom of choice is also equivalent to asking for an equivalence
  \[ \eqv{\left(\prd{x:X} \brck{Y(x)}\right)}{\brck{\prd{x:X} Y(x)}} \]
  This illustrates a common pitfall: although dependent function types preserve mere propositions (\autoref{thm:isprop-forall}), they do not commute with truncation: $\brck{\prd{x:A} P(x)}$ is not generally equivalent to $\prd{x:A} \brck{P(x)}$.
  The axiom of choice, if we assume it, says that this is true \emph{for sets}; as we will see below, it fails in general.
\end{rmk}

The restriction in the axiom of choice to types that are sets can be relaxed to a certain extent.
For instance, we may allow $A$ and $P$ in~\eqref{eq:ac}, or $Y$ in~\eqref{eq:epis-split}, to be arbitrary type families; this results in a seemingly stronger statement that is equally consistent.
We may also replace the propositional truncation by the more general $n$-truncations to be considered in \autoref{cha:hlevels}, obtaining a spectrum of axioms AC$_n$ interpolating between~\eqref{eq:ac}, which we call simply AC, and \autoref{thm:ttac}, which we shall call AC$_\infty$.  However, observe that we cannot relax the requirement that $X$ be a set.  

\begin{lem}\label{thm:no-higher-ac}
  There exists a type $X$ and a family $Y:X\to \type$ such that each $Y(x)$ is a set, but such that~\eqref{eq:epis-split} is false.
\end{lem}
\begin{proof}
  Define $X\defeq \sm{A:\type} \brck{\bool = A}$, and let $x_0 \defeq (\bool, \bproj{\refl{\bool}}) : X$.
  Then by the identification of paths in $\Sigma$-types, the fact that $\brck{A=\bool}$ is a mere proposition, and univalence, we have $\eqv{(\id[X]{(A,p)}{(B,p)})}{(\eqv AB)}$.
  In particular, $\eqv{(\id[X]{x_0}{x_0})}{(\eqv \bool\bool)}$, so as in \autoref{thm:type-is-not-a-set}, $X$ is not a set.
  But if we define $Y(x) \defeq (x_0=x)$, then each $Y(x)$ is a set.

  Now by definition, for any $(A,p):X$ we have $\brck{\bool=A}$, and hence $\brck{x_0 = (A,p)}$.
  Thus, we have $\prd{x:X} \brck{Y(x)}$.
  If~\eqref{eq:epis-split} held for this $X$ and $Y$, then we would also have $\brck{\prd{x:X} Y(x)}$.
  Since we are trying to derive a contradiction ($\emptyt$), which is a mere proposition, we may assume $\prd{x:X} Y(x)$, i.e.\ that $\prd{x:X} (x_0=x)$.
  But this implies $X$ is a mere proposition, and hence a set, which is a contradiction.
\end{proof}

\subsection{The principle of unique choice}
\label{sec:unique-choice}

The following observation is trivial, but very useful.

\begin{lem}
  If $P$ is a mere proposition, then $\eqv P {\brck P}$.
\end{lem}
\begin{proof}
  Of course, we have $P\to \brck{P}$ by definition.
  And since $P$ is a mere proposition, the universal property of $\brck P$ applied to $\idfunc[P] :P\to P$ yields $\brck P \to P$.
  These functions are quasi-inverses by \autoref{lem:equiv-iff-hprop}.
\end{proof}

Among its important consequences is the following.

\begin{cor}[The principle of unique choice]\label{cor:UC}
  Suppose a type family $P:A\to \type$ such that
  \begin{enumerate}
  \item For each $x$, the type $P(x)$ is a mere proposition, and
  \item For each $x$ we have $\brck {P(x)}$.
  \end{enumerate}
  Then we have $f:\prd{x:A} P(x)$.
\end{cor}
\begin{proof}
  Immediate from the two assumptions and the previous lemma.
\end{proof}

The corollary also encapsulates a very useful technique of reasoning.
Namely, suppose we know that $\brck A$, and we want to use this to construct an element of some other type $B$.
We would like to use an element of $A$ in our construction of an element of $B$, but this is allowed only if $B$ is a mere proposition, so that we can apply the induction principle for the propositional truncation $\brck A$; the most we could hope to do in general is to show $\brck B$.
%
Instead, we can extend $B$ with additional data which characterizes \emph{uniquely} the object we wish to construct.
Specifically, we define a predicate $Q:B\to\type$ such that $\sm{x:B} Q(x)$ is a mere proposition.
Then from an element of $A$ we construct an element $b:B$ such that $Q(b)$, hence from $\brck A$ we can construct $\brck{\sm{x:B} Q(x)}$, and because $\brck{\sm{x:B} Q(x)}$ is equivalent to $\sm{x:B} Q(x)$ an element of $B$ may be projected from it.
We provide an example below.

A similar issue arises in set-theoretic mathematics, although it manifests slightly
differently. If we are trying to define a function $f: A \to B$, and depending on an
element $a : A$ we are able to prove mere existence of some $b : B$, we are not done yet
because we need to actually pinpoint an element of~$B$, not just prove its existence.
One option is of course to refine the argument to unique existence of $b : B$, like we did in type theory. But in set theory the problem can often be avoided more simply by an application of the axiom of choice, which picks the required elements for us.
In homotopy type theory, however, quite apart from any desire to avoid choice, the available forms of choice are simply less applicable, since they require that the domain of choice be a \emph{set}.
Thus, if $A$ is not a set (such as perhaps a universe $\UU$), there is no consistent form of choice that will allow us to simply pick an element of $B$ for each $a : A$ to use in defining $f(a)$.


\begin{thm}
  Suppose $P:\nat\to\type$ is such that each $P(n)$ is a mere proposition, and that $\prd{n:\nat} (P(n) + \neg P(n))$ (such a predicate is called \emph{decidable}).
  Then
  \[ \brck{\sm{n:\nat} P(n)} \;\to\; \sm{n:\nat}P(n).\]
\end{thm}
\begin{proof}[Sketch of proof]
  The hypotheses imply that
  \[ \Big(\sm{n:\nat}P(n)\Big) \;\to\; \sm{n:\nat}\Big(P(n) \times \prd{m:\nat} \big((m<n) \to \neg P(m)\big)\Big). \]
  In words, given $n$ such that $P(n)$, we can find the least such $n$: we test every $m<n$ in turn, using decidability to do a case analysis, until we find the first one that satisfies $P(m)$.
  However, the right-hand side of the above implication is a mere proposition: if both $n$ and $n'$ are least numbers satisfying~$P$ then they must be equal.
  Therefore, we also have
  \[ \Brck{\sm{n:\nat}P(n)} \;\to\; \sm{n:\nat}\Big(P(n) \times \prd{m:\nat} \big((m<n) \to \neg P(m)\big)\Big) \]
  from which the claim follows.
\end{proof}

\subsection{When are propositions truncated?}
\label{subsec:when-trunc}

At first glance, it may seem that the truncated versions of $+$ and $\Sigma$ are actually closer to the informal mathematical meaning of ``or'' and ``there exists'' than the untruncated ones.
Certainly, they are closer to the \emph{precise} meaning of ``or'' and ``there exists'' in the first-order logic which underlies formal set theory, since the latter makes no attempt to remember any witnesses to the truth of propositions.
However, it may come as a surprise to realize that the practice of \emph{informal} mathematics is often more accurately described by the untruncated forms.

For example, consider a statement like ``every prime number is either $2$ or odd.''
The working mathematician feels no compunction about using this fact not only to prove \emph{theorems} about prime numbers, but also to perform \emph{constructions} on prime numbers, perhaps doing one thing in the case of $2$ and another in the case of an odd prime.
The end result of the construction is not merely the truth of some statement, but a piece of data which may depend on the parity of the prime number.
Thus, from a type-theoretic perspective, such a construction is naturally phrased using the induction principle for the coproduct type ``$(p=2)+(p\text{ is odd})$'', not its propositional truncation.

Admittedly, this is not an ideal example, since ``$p=2$'' and ``$p$ is odd'' are mutually exclusive, so that $(p=2)+(p\text{ is odd})$ is in fact already a mere proposition and hence equivalent to its truncation (see \autoref{ex:disjoint-or}). % and~\ref{ex:prop-eqvtrunc}).
More compelling examples come from the existential quantifier.
It is not uncommon to prove a theorem of the form ``there exists an $x$ such that \dots'' and then refer later on to ``the $x$ constructed in Theorem Y'' (note the definite article).
Moreover, when deriving further properties of this $x$, one may use phrases such as ``by the construction of $x$ in the proof of Theorem Y''.

A very common example is ``$A$ is isomorphic to $B$'', which strictly speaking means only that there exists \emph{some} isomorphism between $A$ and $B$.
But almost invariably, when proving such a statement, one exhibits a specific isomorphism or proves that some previously known map is an isomorphism, and it often matters later on what particular isomorphism was given.

Set-theoretically trained mathematicians often feel a twinge of guilt at such ``abuses of language''.
We may attempt to apologize for them, expunge them from final drafts, or weasel out of them with vague words like ``canonical''.
The problem is exacerbated by the fact that in formalized set theory, there is technically no way to ``construct'' objects at all --- we can only prove that an object with certain properties exists.
Untruncated logic in type theory thus captures some common practices of informal mathematics that the set theoretic reconstruction obscures.
(This is similar to how the univalence axiom validates the common, but formally unjustified, practice of identifying isomorphic objects.)

On the other hand, sometimes truncated logic is essential.
We have seen this in the statements of LEM and AC; some other examples will appear later on in the book.
Thus, we are faced with the problem: when writing informal type theory, what should we mean by the words ``or'' and ``there exists'' (along with common synonyms such as ``there is'' and ``we have'')?

A universal consensus may not be possible.
Perhaps depending on the sort of mathematics being done, one convention or the other may be more useful --- or, perhaps, the choice of convention may be irrelevant.
In this case, a remark at the beginning of a mathematical paper may suffice to inform the reader of the linguistic conventions in use therein.
However, even after one overall convention is chosen, the other sort of logic will usually arise at least occasionally, so we need a way to refer to it.
More generally, one may consider replacing the propositional truncation with another operation on types that behaves similarly, such as the double negation operation $A\mapsto \neg\neg A$, or the $n$-truncations to be considered in \autoref{cha:hlevels}.
As an experiment in exposition,  in what follows we will occasionally use \emph{adverbs} to denote the application of such ``modalities'' as propositional truncation.

For instance, if untruncated logic is the default convention, we may use the adverb \textbf{merely} to denote propositional truncation.
Thus the phrase
\begin{center}
  ``there merely exists an $x:A$ such that $P(x)$''
\end{center}
indicates the type $\brck{\sm{x:A} P(x)}$.
Similarly, we will say that a type $A$ is \textbf{merely inhabited} to mean that its propositional truncation $\brck A$ is inhabited (i.e.\ that we have an unnamed element of it).
Note that this is a \emph{definition} of the adverb ``merely'' as it is to be used in our informal mathematical English, in the same way that we define nouns like ``group'' and ``ring'', and adjectives like ``regular'' and ``normal'', to have precise mathematical meanings.
We are not claiming that the dictionary definition of ``merely'' refers to propositional truncation; the choice of word is meant only to remind the mathematician reader that a mere proposition contains ``merely'' the information of a truth value and nothing more.

On the other hand, if truncated logic is the current default convention, we may use an adverb such as \textbf{purely} or \textbf{constructively} to indicate its absence, so that
\begin{center}
``there purely exists an $x:A$ such that $P(x)$''
\end{center}
would denote the type $\sm{x:A} P(x)$.
We may also use ``purely'' or ``actually'' just to emphasize the absence of truncation, even when that is the default convention.

In this book we will continue using untruncated logic as the default convention, for a number of reasons.
\begin{enumerate}[label=(\arabic*)]
\item We want to encourage the newcomer to experiment with it, rather than sticking to truncated logic simply because it is more familiar.
\item Using truncated logic as the default in type theory suffers from the same sort of ``abuse of language'' problems as set-theoretic foundations, which untruncated logic avoids.
  For instance, our definition of ``$\eqv A B$'' as the type of equivalences between $A$ and $B$, rather than its propositional truncation, means that to prove a theorem of the form ``$\eqv A B$'' is literally to construct a particular such equivalence.
  This specific equivalence can then be referred to later on.
\item We want to emphasize that the notion of ``mere proposition'' is not a fundamental part of type theory.
  As we will see in \autoref{cha:hlevels}, mere propositions are just the second rung on an infinite ladder, and there are also many other modalities not lying on this ladder at all.
\item Many statements that classically are mere propositions are no longer so in homotopy type theory.
  Of course, foremost among these is equality.
\item On the other hand, one of the most interesting observations of homotopy type theory is that a surprising number of types are \emph{automatically} mere propositions, or can be slightly modified to become so, without the need for any truncation.
  (See \autoref{thm:isprop-isprop} and Chapters~\ref{cha:equivalences}, \ref{cha:hlevels}, \ref{cha:category-theory}, and~\ref{cha:set-math}.)
  Thus, although these types contain no data beyond a truth value, we can nevertheless use them to construct untruncated objects, since there is no need to use the induction principle of propositional truncation.
  This useful fact is more clumsy to express if propositional truncation is applied to all statements by default.
\item Finally, truncations are not very useful for most of the mathematics we will be doing in this book, so it is simpler to notate them explicitly when they occur.
\end{enumerate}

\section{Contractibility}
\label{sec:contractibility}

In \autoref{thm:inhabprop-eqvunit} we observed that a mere proposition which is inhabited must be equivalent to unit, and it is not hard to see that the converse also holds.
A type with this property is called \emph{contractible}.
Another equivalent definition of contractibility, which is also sometimes convenient, is the following.

\begin{defn}\label{defn:contractible}
  A type $A$ is \textbf{contractible}, or a \textbf{singleton}, if there is $a:A$, called the \textbf{center of contraction}, such that $a=x$ for all $x:A$.
  We denote the specified path $a=x$ by $\contr_x$.
\end{defn}

In other words, the type $\iscontr(A)$ is defined to be
\[ \iscontr(A) \defeq \sm{a:A} \prd{x:A}(a=x). \]
Note that under the usual propositions-as-types reading, we can pronounce $\iscontr(A)$ as ``$A$ contains exactly one element'', or more precisely ``$A$ contains an element, and every element of $A$ is equal to that element''.

\begin{rmk}
  We can also pronounce $\iscontr(A)$ more topologically as ``there is a point $a:A$ such that for all $x:A$ there exists a path from $a$ to $x$''.
  Note that to a classical ear, this sounds like a definition of \emph{connectedness} rather than contractibility.
  The point is that the meaning of ``there exists'' in this sentence is a continuous/natural one.
  A more correct way to express connectedness would be $\sm{a:A}\prd{x:A} \brck{a=x}$; we will come back to this later.
\end{rmk}

\begin{lem}\label{thm:contr-unit}
  For a type $A$, the following are logically equivalent.
  \begin{enumerate}
  \item $A$ is contractible in the sense of \autoref{defn:contractible}.\label{item:contr}
  \item $A$ is a mere proposition, and there is a point $a:A$.\label{item:contr-inhabited-prop}
  \item $A$ is equivalent to \unit.\label{item:contr-eqv-unit}
  \end{enumerate}
\end{lem}
\begin{proof}
  If $A$ is contractible, then it certainly has a point $a:A$ (the center of contraction), while for any $x,y:A$ we have $x=a=y$; thus $A$ is a mere proposition.
  Conversely, if we have $a:A$ and $A$ is a mere proposition, then for any $x:A$ we have $x=a$; thus $A$ is contractible.
  And we showed~\ref{item:contr-inhabited-prop}$\Rightarrow$\ref{item:contr-eqv-unit} in \autoref{thm:inhabprop-eqvunit}, while the converse follows since \unit easily has property~\ref{item:contr-inhabited-prop}.
\end{proof}

\begin{lem}\label{thm:isprop-iscontr}
  For any type $A$, the type $\iscontr(A)$ is a mere proposition.
\end{lem}
\begin{proof}
  Suppose given $c,c':\iscontr(A)$.
  We may assume $c\jdeq(a,p)$ and $c'\jdeq(a',p')$ for $a,a':A$ and $p:\prd{x:A} (a=x)$ and $p':\prd{x:A} (a'=x)$.
  By the characterization of paths in $\Sigma$-types, to show $c=c'$ it suffices to exhibit $q:a=a'$ such that $\trans{q}{p}=p'$.

  We choose $q\defeq p(a')$.
  For the other equality, by function extensionality we must show that $(\trans q p)(x)=p'(x)$ for any $x:A$.
  For this, it will suffice to show that for any $x,y:A$ and $u:x=y$ we have $u= \opp{p(x)} \ct p(y)$, since then we would have $(\trans q p)(x) = \opp{p(x)} \ct p(y) = p'(x)$.
  But now we can invoke path induction to assume that $x\jdeq y$ and $u\jdeq \refl{x}$.
  In this case our goal is to show that $\refl x = \opp{p(x)} \ct p(x)$, which is just the inversion law for paths.
\end{proof}

\begin{cor}\label{thm:contr-contr}
  If $A$ is contractible, then so is $\iscontr(A)$.
\end{cor}
\begin{proof}
  By \autoref{thm:isprop-iscontr} and \autoref{thm:contr-unit}\ref{item:contr-inhabited-prop}.
\end{proof}

Like mere propositions, contractible types are preserved by many type constructors.
For instance, we have:

\begin{lem}\label{thm:contr-forall}
  If $P:A\to\type$ is a type family such that each $P(a)$ is contractible, then $\prd{x:A} P(x)$ is contractible.
\end{lem}
\begin{proof}
  By \autoref{thm:isprop-forall}, $\prd{x:A} P(x)$ is a mere proposition since each $P(x)$ is.
  But it also has an element, namely the function sending each $x:A$ to the center of contraction of $P(x)$.
  Thus by \autoref{thm:contr-unit}\ref{item:contr-inhabited-prop}, $\prd{x:A} P(x)$ is contractible.
\end{proof}

(In fact, the statement of \autoref{thm:contr-forall} is equivalent to the function extensionality axiom.
See Appendix~[?].)

Of course, if $A$ is equivalent to $B$ and $A$ is contractible, then so is $B$.
More generally, it suffices for $B$ to be a \emph{retract} of $A$.
By definition, a \define{retraction} is a function $r : A \to B$ such that there exists a function $s : B \to A$, called its \define{section}, and a homotopy $\epsilon:\prd{y:Y} (r(s(y))=y)$; then we say that $B$ is a \define{retract} of $A$.

\begin{lem}\label{thm:retract-contr}
  If $B$ is a retract of $A$, and $A$ is contractible, then so is $B$.
\end{lem}
\begin{proof}
  Let $a_0 : A$ be the center of contraction.
  We claim that $b_0 \defeq p(a_0) : B$ is a center of contraction for $B$.
  Let $b : B$; we need a path $b = b_0$.
  But we have $\epsilon_b : p(s(b)) = b$ and $\contr_{s(b)} : s(b) = a_0$, so by composition
  \[ \opp{\epsilon_b} \ct \ap{p}{\contr_{s(b)}} : b = p(a_0) \jdeq b_0 . \qedhere\]
\end{proof}

Contractible types may not seem very interesting, since they are all equivalent to \unit.
One reason the notion is useful is that sometimes a collection of individually nontrivial data will collectively form a contractible type.
An important example is the space of paths with one free endpoint.
As we will see in \autoref{sec:identity-systems}, this fact essentially encapsulates the Paulin-Mohring induction principle for paths.

\begin{lem}\label{thm:contr-paths}
  For any $A$ and any $a:A$, the type $\sm{x:A} (a=x)$ is contractible.
\end{lem}
\begin{proof}
  We choose as center the point $(a,\refl a)$.
  Now suppose $(x,p):\sm{x:A}(a=x)$; we must show $(a,\refl a) = (x,p)$.
  By the characterization of paths in $\Sigma$-types, it suffices to exhibit $q:a=x$ such that $\trans{q}{\refl a} = p$.
  But we can take $q\defeq p$, in which case $\trans{q}{\refl a} = p$ follows from the characterization of transport in path types.
\end{proof}

When this happens, it can allows us to simplify a complicated construction up to equivalence, using the informal principle that contractible data can be freely ignored.
This principle consists of many lemmas, most of which we leave to the reader; the following is an example.

\begin{lem}\label{thm:omit-contr}
  Let $P:A\to\type$ be a type family.
  \begin{enumerate}
  \item If each $P(x)$ is contractible, then $\sm{x:A} P(x)$ is equivalent to $A$.\label{item:omitcontr1}
  \item If $A$ is contractible with center $a$, then $\sm{x:A} P(x)$ is equivalent to $P(a)$.\label{item:omitcontr2}
  \end{enumerate}
\end{lem}
\begin{proof}
  In the situation of~\ref{item:omitcontr1}, we show that $\proj1:\sm{x:A} P(x) \to A$ is an equivalence.
  For quasi-inverse we define $g(x)\defeq (x,c_x)$ where $c_x$ is the center of $P(x)$.
  The composite $\proj1 \circ g$ is obviously $\idfunc[A]$, whereas the opposite composite is homotopic to the identity by using the contractions of each $P(x)$.

  We leave the proof of~\ref{item:omitcontr2} to the reader (see \autoref{ex:omit-contr2}).
\end{proof}

Another reason contractible types are interesting is that they extend the ladder of $n$-types mentioned in \autoref{sec:basics-sets} downwards one more step.

\begin{lem}\label{thm:prop-minusonetype}
  A type $A$ is a mere proposition if and only if for all $x,y:A$, the type $\id[A]xy$ is contractible.
\end{lem}
\begin{proof}
  For ``if'', we simply observe that any contractible type is inhabited.
  For ``only if'', we observed in \autoref{subsec:hprops} that every mere proposition is a set, so that each type $\id[A]xy$ is a mere proposition.
  But it is also inhabited (since $A$ is a mere proposition), and hence by \autoref{thm:contr-unit}\ref{item:contr-inhabited-prop} it is contractible.
\end{proof}

Thus, contractible types may also be called \emph{$(-2)$-types}.
They are the bottom rung of the ladder of $n$-types, and will be the base case of the inductive definition of $n$-types in \autoref{cha:hlevels}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%
\sectionNotes

The definition of identity types and the elimination rule $J$ are due to Martin-L\"of \cite{ml:itt}.
Our identity types are generally called \emph{intensional}, by contrast with the \emph{extensional} case which would have an additional ``reflection rule'' saying that if $p:x=y$, then in fact $x\jdeq y$.
This reflection rule implies that all the higher groupoid structure collapses, so for nontrivial homotopy we must use the intensional version. 
One may argue, however, that homotopy type theory is more ``extensional'' than traditional extensional type theory, because of the function extensionality and univalence rules.  

The proofs of symmetry (inversion) and transitivity (concatenation) for equalities are well-known in type theory.
The fact that these make each type into a 1-groupoid (up to homotopy) is also folklore, and was exploited in~\cite{hs:gpd-typethy} to give the first ``homotopy" style semantics for type theory.  

The actual homotopical interpretation, with identity types as path spaces, and dependent types as fibrations, is due to \cite{aw:hiit}, who used the formalism of Quillen model categories.  An interpretation in (strict) $\infty$-groupoids was also given in the thesis \cite{mw:thesis}.
For a construction of \emph{all} the higher operations and coherences of an $\infty$-groupoid in type theory, see~\cite{pll:wkom-type} and~\cite{bg:type-wkom}.

Operations such as $\transfib{P}{p}{-}$ and $\apfunc{f}$, and one good notion of equivalence, were first studied extensively in type theory by Voevodsky, using the proof assistant Coq.
Subsequent researchers have found many other equivalent definitions of equivalence, which we will compare in \autoref{cha:equivalences}.

The ``computational'' interpretation of identity types, transport, and so on described in \autoref{sec:computational} has been emphasized by~\cite{lh:canonicity}.
They also described a ``1-truncated'' type theory (see \autoref{cha:hlevels}) in which these rules really are computation steps (that is, definitional equalities which a computer can ``evaluate'').
The possibility of extending this to the full untruncated theory is a subject of current research.

The naive form of function extensionality which says that ``if two functions are pointwise equal, then they are equal'' is a common axiom in type theory.
Some stronger forms of function extensionality were considered in~\cite{garner:depprod}.
The version we have used, which identifies the identity types of function types up to equivalence, was first studied by Voevodsky, who also proved that it is implied by the naive version.

The univalence axiom is also due to Voevodsky.
It was originally motivated by semantic considerations; see~\cite{klv:ssetmodel}.

The simple conclusions in \crefrange{sec:compute-coprod}{sec:compute-nat} such as ``coproduct injections are injective and disjoint'' are well-known in type theory, and the construction of the function \encode is the usual way to prove them.
The more refined approach we have described, which characterizing the entire identity type of a positive type (up to equivalence), is a more recent development; see e.g.~\cite{ls:pi1s1}.

The type-theoretic axiom of choice~\eqref{eq:sigma-ump-map} was noticed in William Howard's original paper~\cite{howard:pat} on the propositions-as-types correspondence, and was studied further by Martin-L\"of with the introduction of his dependent type theory.  It is mentioned as a ``distributivity law" in Bourbaki's set theory \cite{Bourbaki}.

The fact that it is possible to define sets, mere propositions, and contractible types in type theory, with all higher homotopies automatically taken care of as in \autoref{sec:basics-sets,subsec:hprops,sec:contractibility}, was first observed by Voevodsky.
In fact, he defined the entire hierarchy of $n$-types by induction, as we will do in \autoref{cha:hlevels}.

\autoref{thm:not-dneg,thm:not-lem} rely in essense on a classical theorem of Hedberg, which we will prove in \autoref{cha:hlevels}.
The implication that the propositions-as-types form of LEM contradicts univalence was observed by Martin Escardo on the Agda mailing list.
The proof we have given of \autoref{thm:not-dneg} is due to Thierry Coquand.

The propositional truncation was introduced in the extensional type theory of
NuPRL in 1983 by Constable~\cite{Con85} as an
application of ``subset'' and ``quotient'' types.  What is here called the
``propositional truncation'' was called ``squashing'' in the NuPRL type theory~\cite{constable+86nuprl-book}.
Rules characterizing the propositional truncation directly, still in extensional type theory, were given in~\cite{ab:bracket-types}.
The intensional version in homotopy type theory was constructed by Voevodsky using an impredicative quantification, and later by Lumsdaine using higher inductive types (see \autoref{sec:hittruncations}).

The adverb ``purely'' as used to refer to untruncated logic is a reference to the use of monadic modalities to model effects in programming languages.
A computation is said to be \emph{pure} if its execution results in no side effects (such as printing a message to the screen, playing music, or sending data over the Internet).
There exist ``purely functional'' programming languages, such as Haskell, in which it is technically only possible to write pure functions: side effects are represented by applying ``monads'' to output types.
For instance, a function of type $\mathsf{Int}\to\mathsf{Int}$ is pure, while a function of type $\mathsf{Int}\to \mathsf{IO}(\mathsf{Int})$ may perform input and output along the way to computing its result.
Inside of type theory, the propositional truncation $\brck-$ is also a monad, as are the more general modalities one might consider replacing it with; thus it makes sense to call a type \emph{pure} when no such modality is present.


\sectionExercises

\begin{ex}\label{ex:basics:concat}
  Show that the three obvious proofs of \autoref{lem:concat} are pairwise equal.
\end{ex}

\begin{ex}
  Show that the three equalities of proofs constructed in the previous exercise form a commutative triangle.
  In other words, if the three definitions of concatenation are denoted by $(p \ct_1 q)$, $(p\ct_2 q)$, and $(p\ct_3 q)$, then the concatenated equality
  \[(p\ct_1 q) = (p\ct_2 q) = (p\ct_3 q)\]
  is equal to the equality $(p\ct_1 q) = (p\ct_3 q)$.
\end{ex}

\begin{ex}
  Give a fourth, different, proof of \autoref{lem:concat}, and prove that it is equal to the others.
\end{ex}

\begin{ex}
  Prove that the functions~\eqref{eq:ap-to-apd} and~\eqref{eq:apd-to-ap} are inverse equivalences.
  % and that they take $\apfunc f(p)$ to $\apdfunc f (p)$ and vice versa. (that was \autoref{thm:apd-const})
\end{ex}

\begin{ex}\label{ex:equiv-concat}
  Prove that if $p:x=y$, then the function $(p\ct -):(y=z) \to (x=z)$ is an equivalence.
\end{ex}

\begin{ex}\label{ex:ap-sigma}
  State and prove a generalization of \autoref{thm:ap-prod} from cartesian products to $\Sigma$-types.
\end{ex}

\begin{ex}
  State and prove an analogue of \autoref{thm:ap-prod} for coproducts.
\end{ex}

\begin{ex}\label{ex:coprod-ump}
  Prove that coproducts have the expected universal property:
  \[ \eqv{(A+B \to X)}{(A\to X)\times (B\to X)} \]
  Can you generalize this to an equivalence involving dependent functions?
\end{ex}

\begin{ex}
  Prove that if $\eqv A B$ and $A$ is a set, then so is $B$.
\end{ex}

\begin{ex}\label{ex:isset-coprod}
  Prove that if $A$ and $B$ are sets, then so is $A+B$.
\end{ex}

\begin{ex}\label{ex:isset-sigma}
  Prove that if $A$ is a set and $B:A\to \type$ is a type family such that $B(x)$ is a set for all $x:A$, then $\sm{x:A} B(x)$ is a set.
\end{ex}

\begin{ex}\label{ex:neg-ldn}
  Show that for any type $A$, we have $\neg\neg\neg A \to \neg A$.
\end{ex}

\begin{ex}\label{ex:eqvboolbool}
  Show that $\eqv{(\eqv\bool\bool)}{\bool}$.
\end{ex}

\begin{ex}\label{ex:prop-endocontr}
  Show that $A$ is a mere proposition if and only if $A\to A$ is contractible.
\end{ex}

\begin{ex}
  Show that if $A$ is a mere proposition, then so is $A+(\neg A)$.
  Thus, there is no need to insert a propositional truncation in~\eqref{eq:lem}.
\end{ex}

\begin{ex}\label{ex:disjoint-or}
  More generally, show that if $A$ and $B$ are mere propositions and $\neg(A\times B)$, then $A+B$ is also a mere proposition.
\end{ex}

% \begin{ex}\label{ex:hprop-iff-equiv}
%   Show that if $A$ and $B$ are mere propositions such that $A\to B$ and $B\to A$, then $\eqv A B$.
% \end{ex}

% \begin{ex}\label{ex:isprop-isprop}
%   Show that for any type $A$, the types $\isprop(A)$ and $\isset(A)$ are mere propositions.
% \end{ex}

% \begin{ex}\label{ex:prop-eqvtrunc}
%   Show that if $A$ is already a mere proposition, then $\eqv A{\brck{A}}$.
% \end{ex}

\begin{ex}\label{ex:brck-qinv}
  Assuming that some type $\isequiv(f)$ satisfies conditions~\ref{item:be1}--\ref{item:be3} of \autoref{sec:basics-equivalences}, show that the type $\brck{\qinv(f)}$ satisfies the same conditions and is equivalent to $\isequiv(f)$.
\end{ex}

\begin{ex}
  Show that if LEM holds, then the type $\prop \defeq \sm{A:\type} \isprop(A)$ is equivalent to \bool.
\end{ex}

\begin{ex}\label{ex:lem-impred}
  Show that if $\UU_{i+1}$ satisfies LEM, then the canonical inclusion $\prop_{\UU_i} \to \prop_{\UU_{i+1}}$ is an equivalence.
\end{ex}

\begin{ex}
  Show that it is not the case that for all $A:\type$ we have $\brck{A} \to A$.
  (However, there can be particular types for which $\brck{A}\to A$.
  \autoref{ex:brck-qinv} implies that $\qinv(f)$ is such.)
\end{ex}

\begin{ex}
  Show that if LEM holds, then for all $A:\type$ we have $\bbrck{(\brck A \to A)}$.
  (This property is a very simple form of the axiom of choice, which can fail in the absence of LEM; see~\cite{krausgeneralizations}.)
\end{ex}

\begin{ex}
  We showed in \autoref{thm:not-lem} that the following naive form of LEM is inconsistent with univalence:
  \[ \prd{A:\type} (A+(\neg A)) \]
  In the absence of univalence, this axiom is consistent.
  However, show that it implies the axiom of choice~\eqref{eq:ac}.
\end{ex}

\begin{ex}\label{ex:lem-brck}
  Show that assuming LEM, the double negation $\neg \neg A$ has the same universal property as the propositional truncation $\brck A$, and is therefore equivalent to it.
  Thus, under LEM, the propositional truncation can be defined rather than taken as a separate type former.
\end{ex}

\begin{ex}\label{ex:impred-brck}
  Show that if we assume impredicativity of mere propositions as in \autoref{subsec:prop-subsets}, then the type
  \[\prd{P:\prop} \big((A\to P)\to P\big)\]
  has the same universal property as $\brck A$.
  Thus, we can also define the propositional truncation in this case.
\end{ex}

\begin{ex}
  Assuming LEM, show that double negation commutes with universal quantification of mere propositions over sets.
  That is, show that if $X$ is a set and each $Y(x)$ is a mere proposition, then LEM implies
  \begin{equation}
    \eqv{\big(\prd{x:X} \neg\neg Y(x)\big)}{\big(\neg\neg \prd{x:X} Y(x)\big)}.\label{eq:dnshift}
  \end{equation}
  Observe that if we assume instead that each $Y(x)$ is a set, then~\eqref{eq:dnshift} becomes equivalent to the axiom of choice~\eqref{eq:epis-split}.
\end{ex}

\begin{ex}\label{ex:prop-trunc-ind}
  Show that the rules for the propositional truncation given in \autoref{subsec:prop-trunc} are sufficient to imply a dependent version of the induction principle: for any type family $B:\brck A \to \type$ such that each $B(x)$ is a mere proposition, if for every $a:A$ we have $B(\bproj a)$, then for every $x:\brck A$ we have $B(x)$.
\end{ex}

\begin{ex}\label{ex:lem-ldn}
  Show that the law of excluded middle~\eqref{eq:lem} and the law of double negation~\eqref{eq:ldn} are logically equivalent.
\end{ex}

\begin{ex}\label{ex:omit-contr2}
  Prove \autoref{thm:omit-contr}\ref{item:omitcontr2}: if $A$ is contractible with center $a$, then $\sm{x:A} P(x)$ is equivalent to $P(a)$.
\end{ex}

% Local Variables:
% TeX-master: "main"
% End:
