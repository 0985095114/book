\let\zero\emptyt
\let\one\unit
\let\two\bool
\newcommand{\twoh}{\ensuremath{\mathbf{2^h}}\xspace}
\newcommand{\three}{\ensuremath{\mathbf{3}}\xspace}
\newcommand{\natw}{\ensuremath{\mathbf{N^w}}\xspace}
\newcommand{\nath}{\ensuremath{\mathbf{N^h}}\xspace}
\let\true\btrue
\let\false\bfalse
\newcommand{\z}{\ensuremath{0}\xspace}
\newcommand{\zw}{\ensuremath{0^\mathbf{w}}\xspace}
\newcommand{\myInd}{\ensuremath{\mathbf{Ind \;}}\xspace}
\let\s\suc
\newcommand{\sw}{\ensuremath{\mathbf{s^w}}\xspace}
\newcommand{\alt}{\;|\;\;}
\newcommand{\disj}[2]{#1 + #2}
\newcommand{\dbl}{\ensuremath{\mathbf{double}}}
\newcommand{\nalg}{\mathsf{\nat Alg}}
\newcommand{\nhom}{\mathsf{\nat Hom}}
\newcommand{\ishinitw}{\ensuremath{\mathsf{isHinit_W}}}
\newcommand{\ishinitn}{\ensuremath{\mathsf{isHinit_\nat}}}
\newcommand{\w}{\mathsf{W}}
\newcommand{\walg}{\mathsf{\w Alg}}
\newcommand{\whom}{\mathsf{\w Hom}}

\chapter{Induction}
\label{cha:induction}

In \autoref{cha:typetheory}, we introduced many ways to form new types from old ones.
Except for (dependent) function types and universes, all these rules are special cases of the general notion of \emph{inductive definition}.
In this chapter we study inductive definitions more generally.


\section{Finite types and natural numbers}
\label{sec:bool-nat}

An \emph{inductive type} $X$ can be intuitively understood as a type ``freely generated'' by a certain finite collection of \emph{constructors}, each of which is a function (of some number of arguments) with codomain $X$.
This includes functions of zero arguments, which are simply elements of $X$.

When describing a particular inductive type, we list the constructors with bullets.
For instance, the type \bool from \autoref{sec:type-booleans} is inductively generated by the following constructors:
\begin{itemize}
\item $\btrue:\bool$
\item $\bfalse:\bool$
\end{itemize}
Similarly, $\unit$ is inductively generated by the constructor:
\begin{itemize}
\item $\ttt:\unit$
\end{itemize}
while $\emptyt$ is inductively generated by no constructors at all.
An example where the constructor functions take arguments is the coproduct $A+B$, which is generated by the two constructors
\begin{itemize}
\item $\inl:A\to A+B$
\item $\inr:B\to A+B$.
\end{itemize}
And an example with a constructor taking multiple arguments is the cartesian product $A\times B$, which is generated by one constructor
\begin{itemize}
\item $(-,-) : A\to B \to A\times B$.
\end{itemize}
Crucially, we also allow constructors of inductive types that take arguments from the inductive type being defined.
For instance, the type $\nat$ of natural numbers has constructors
\begin{itemize}
\item $0:\nat$
\item $\suc:\nat\to\nat$.
\end{itemize}

How should we intuitively understand the notion of an inductive type? Taking the type $\two$ as the simplest example, we would expect it to behave \emph{as if} its only inhabitants were $\true$ and $\false$. We note that the \emph{as if} clause is crucial: in a nonempty context, it is generally not the case that any $b : \two$ is either $\true$ or $\false$; for instance $b$ may be a variable declared in the context. Even when considering only empty contexts, the conclusion that any $b : \two$ in empty context is either $\true$ or $\false$ is far from trivial - this statement is known as \emph{canonicity for $\two$}. While several proofs of this result exist for Martin-L{\"o}f type theory, whether canonicity holds in the presence of the Univalence Axiom is currently an open problem.

The intuition that the inductive type $\two$ behaves as if its only inhabitants were $\true$ and $\false$ is made precise in the following principle of \emph{(dependent) elimination}:

\begin{itemize}
\item When proving a statement $E : \two \to \type$ about \emph{all} Booleans, it suffices to prove it for $\true$ and $\false$, i.e., give proofs
$ e_t : E(\true)$ and $e_f : E(\false)$.
\end{itemize}

Furthermore, the resulting proof $\rec\two(E;e_t;e_f): \prd{b : \two}E(b)$ behaves as expected when applied to the constructors $\true$ and $\false$; this principle is expressed by the \emph{computation rules}:
\begin{itemize}
\item We have $\rec\two(E;e_t;e_f;\true) \equiv e_t$.
\item We have $\rec\two(E;e_t;e_f;\false) \equiv e_f$.
\end{itemize}

The rules for the type $\two$ of Booleans allow us to reason by \emph{case analysis}. Since neither of the two constructors takes any arguments, this is all we need for Booleans. For natural numbers, however, case analysis is generally not sufficient: in the case corresponding to the inductive step $s(n)$, we also want to presume that the statement being proven has already been shown for $n$. This gives us the following principle of \emph{induction}: 
\begin{itemize}
\item When proving a statement $E : \nat \to \type$ about \emph{all} natural numbers, it suffices to prove it for $\z$ and for $\s(n)$, assuming it holds
for $n$. This entails giving the proofs $e_z : E(\z)$ and $e_s : \prd{n : \nat}{y : E(n)} E(\s(n))$.
\end{itemize}
The variable $y$ represents our inductive hypothesis. Like for Booleans, we also have the associated computation rules for the function $\rec\nat(E;e_z;e_s) : \prd{x:\nat} E(x)$:
\begin{itemize}
\item We have $\rec\nat(E;e_z;e_s;\z) \equiv e_z$.
\item For any $n : \nat$, we have $\rec\nat(E;e_z;e_s;\s(n)) \equiv e_s(n;\rec\nat(E;e_z;e_s;n))$.
\end{itemize}
The dependent function $\rec\nat(E;e_z;e_s)$ can thus be understood as being defined recursively on the argument $x : \nat$, via the recurrences $e_z$ and $e_s$: When $x$ is zero, the function simply returns $e_z$. When $x$ is the successor of another natural number $n$, the result is obtained by taking the recurrence $e_s$ and plugging in the specific predecessor $n$ and the recursive call value $\rec\nat(E;e_z;e_s;n)$.

As an example we look at how to define a function on natural numbers which doubles its argument. We wish to apply dependent elimination with the constant type family $E \defeq \lam{x:\nat} \nat$ since the intended type of $\dbl$ is $\nat \to \nat$. We first need to supply the value of $\dbl(\z)$, which is easy: we put $e_z \defeq \z$. Next, to compute the value of $\dbl(\s(n))$ for a natural number $n$, we first compute the value of $\dbl(n)$ and then perform the successor operation twice. This is captured by the recurrence $e_s(n,y) \defeq \s(\s(y))$; the variable $y$ stands for the result of the recursive call $\dbl(n)$. Thus, we define
\[ \dbl \defeq \rec\nat(\big(\lam{x:\nat} \nat\big); \; \z; \;  \big(\lam{n:\nat}{y:\nat} \s(\s(y))\big)) \]
This indeed has the correct computational behavior: for example, we have 
\begin{align*}
\dbl(\s(\s(\z))) & \equiv e_s(\s(\z), \dbl(\s(\z))) \\
                 & \equiv \s(\s(\dbl(\s(\z)))) \\
                 & \equiv \s(\s(e_s(\z,\dbl(\z)))) \\
                 & \equiv \s(\s(\s(\s(\dbl(\z))))) \\
                 & \equiv \s(\s(\s(\s(e_z)))) \\
                 & \equiv \s(\s(\s(\s(\z))))
\end{align*}
as desired, where an arbitrary evaluation strategy was chosen.

There are many similar cases when one wants to use dependent elimination into a constant family.
This is referred to as \emph{simple elimination}, which corresponds to recursion---instead of proving a separate statement $P(n)$ for each natural number $n$ as done in the dependent case, we are recursively constructing a function into a simple type $C$. In this case, we omit the $\lambda$-binder for simplicity and write $\rec\nat(C; \ldots)$ instead of $\rec\nat(\lam{n:\nat} C; \ldots)$

The dependent elimination principle is quite strong and allows us to prove a variety of interesting theorems. For example, by specifying the terms $e_z$ and $e_s$, we  uniquely determine how the recursor behaves on canonical terms, and thus on all natural numbers. If we now have another function which obeys the same recurrence, then our intuition suggests these two functions should be equal. It turns out this is indeed the case and we have the following \emph{uniqueness principle}:

\begin{thm}\label{thm:nat-uniq}
Let $f,g : \prd{x:\nat} E(x)$ be two functions which satisfy the recurrences $e_z : E(\z)$ and $e_s : \prd{n : \nat}{y : E(n)} E(\s(n))$ up to propositional equality, i.e., such that
\begin{align*}
\id{f(\z)}{e_z} \\ 
\id{g(\z)}{e_z}
\end{align*}
and 
\begin{align*}
\prd{n : \nat} \id{f(\s(n))}{e_s(n, f(n))} \\
\prd{n : \nat} \id{g(\s(n))}{e_s(n, g(n))}
\end{align*}
Then $f$ and $g$ are equal. 
\end{thm}

\begin{proof}
We use dependent elimination with the type $E(x) \defeq \id{f(x)}{g(x)}$. For the base case, we have \[f(\z) = e_z = g(\z)\]
For the inductive case, assume $n : \nat$ such that $f(n) = g(n)$. Then
\[ f(\s(n)) = e_s(n, f(n)) = e_s(n, g(n)) = g(\s(n)) \]
The first and last equality follow from the assumptions on $f$ and $g$. The middle equality follows from the inductive hypothesis and the fact that application preserves equality. This gives us pointwise equality between $f$ and $g$; invoking function extensionality finishes the proof.
\end{proof}
We note that the function $f$ is only required to satisfy the recurrences \emph{up to propositional equality}. The theorem itself only asserts propositional equality between functions---indeed, it is possible to construct functions which satisfy the same recurrence but are not definitionally equal (exercise). It is also possible to have a function which satisfies more than one set of recurrences (exercise).

Similar uniqueness theorems can generally be formulated and shown for other inductive types as well. In the next section, we show how to use the uniqueness property together with univalence to prove that natural numbers are completely characterized by their introduction, elimination, and computation rules.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Uniqueness of inductive types}
\label{sec:appetizer-univalence}

\newcommand{\natp}{\ensuremath{\nat'}\xspace}
\newcommand{\zp}{\ensuremath{0'}\xspace}
\newcommand{\Sp}{\ensuremath{\suc'}\xspace}
\newcommand{\dblp}{\ensuremath{\mathbf{double'}}}

We have defined ``the'' natural numbers to be a particular type \nat with particular inductive generators $\z$ and $\suc$.
However, by the general principle of inductive definitions in type theory described in the previous section, there is nothing preventing us from defining \emph{another} type in an identical way.
That is, suppose we let $\natp$ be the inductive type generated by the constructors
\begin{itemize}
\item $\zp:\natp$
\item $\Sp:\natp\to\natp$.
\end{itemize}
Then $\natp$ will have identical-looking induction and recursion principles to $\nat$.
When proving a statement $E : \natp \to \type$ for all of these ``new'' natural numbers, it suffices to give the proofs $e_z : E(\zp)$ and $e_s : \prd{n : \natp}{x : E(n)} E(\Sp(n))$.
And the function $\rec\natp(E;e_z;e_s) : \prd{n:\natp} E(n)$ has the following computation rules:
\begin{itemize}
\item We have $\rec\natp(E;e_z;e_s;\zp) \jdeq e_z$.
\item For any $n : \natp$, we have $\rec\natp(E;e_z;e_s;\Sp(n)) \jdeq e_s(n;\rec\natp(E;e_z;e_s;n))$.
\end{itemize}
But what is the relation between $\nat$ and $\natp$?

This is not just an academic question, since structures that ``look like'' the natural numbers can be found in many other places.
For instance, lists over type with one element (this is arguably the oldest appearance, found on walls of caves), in the non-negative integers, as a substructure of the rationals and the reals, and so on.
And from a programming point of view, the ``unary'' representation of our natural numbers is very inefficient, so we might prefer sometimes to use a binary one instead.
We would like to be able to identify all of these versions of ``the natural numbers'' with each other, in order to transfer constructions and results from one to another.

Of course, if two versions of the natural numbers satisfy identical induction principles, then they have identical induced structure.
For instance, recall the example of the function $\dbl$ defined above. A similar function
for our new natural numbers is readily defined by duplication and adding primes:
\[ \dblp \defeq \rec\natp(\natp; \; \zp; \;  \lam{n:\natp}{m:\natp} \Sp(\Sp(m))) \]
Simple as this may seem, it has the obvious drawback of leading to a
proliferation of duplicates. Not only functions have to be
duplicated, but also all lemmas and their proofs. For example,
an easy result such as  $\prd{n : \nat} \dbl(\s(n))=\s(\s(\dbl(n)))$, as well
as its proof by induction, also has to be ``primed''.

In traditional mathematics, one just proclaims that $\nat$ and $\natp$ are obviously ``the same'', and can be substituted for each other whenever the need arises.
This is usually unproblematic, but it sweeps a fair amount under the rug, widening the gap between informal mathematics and its precise description.
In homotopy type theory, we can do better.

First observe that we have the following definable maps:
\begin{itemize}
\item $f \defeq \rec\nat(\nat; \; \zp; \;  \lam{n:\nat} \Sp)
       : \nat \to\natp$,
\item $g \defeq \rec\natp(\natp; \; \z; \;  \lam{n:\natp}, \s)
       : \natp \to\nat$.
\end{itemize}
Since the composition of $g$ and $f$ satisfies the same recurrences as the identity function on $\nat$, \autoref{thm:nat-uniq} gives that $\prd{n : \nat} \id{g(f(n))}{n}$, and the ``primed'' version of the same theorem gives $\prd{n : \natp} \id{f(g(n))}{n}$.
Thus, $f$ and $g$ are quasi-inverses, so that $\eqv{\nat}{\natp}$.
We can now transfer functions on $\nat$ directly to functions on $\natp$ (and vice versa) along this equivalence, e.g.
\[ \dblp \defeq \lam{n:\natp} f(\dbl(g(n))) \]
It is an easy exercise to show that this version of $\dblp$ is equal to the earlier one.

Of course, there is nothing surprising about this; such an isomorphism is exactly how a mathematician will envision ``identifying'' $\nat$ with $\natp$.
However, the mechanism of ``transfer'' across an isomorphism depends on the thing being transferred; it is not always as simple as pre- and post-composing a single function with $f$ and $g$.
Consider, for instance, a simple lemma such as
\[\prd{n : \natp} \dblp(\Sp(n))=\Sp(\Sp(\dblp(n))).\]
Inserting the correct $f$s and $g$s is only a little easier than re-proving it by induction on $n:\natp$ directly.

Here is where the Univalence Axiom steps in: since $\eqv{\nat}{\natp}$, we also have $\id[\type]{\nat}{\natp}$, i.e.\ $\nat$ and $\natp$ are
\emph{equal} as types.
Now the induction principle for identity guarantees that any construction or proof relating to $\nat$ can automatically be transferred to $\natp$ in the same way.
We simply consider the type of the function or theorem as a type-indexed family of types $P:\type\to\type$, with the given object being an elementn of $P(\nat)$, and transport along the path $\id \nat\natp$.
This involves considerably less overhead.

These conclusions are not confined to the natural numbers: they apply to any inductive type.
If we have an inductively defined type $W$, say, and some other type $W'$ which satisfies the same induction principle as $W$, then it follows that $\eqv{W}{W'}$, and hence $W=W'$.
We use the derived recursion principles for $W$ and $W'$ to construct maps $W\to W'$ and $W'\to W$, respectively, and then the induction principles for each to prove that both composites are equal to identities.
For instance, in \autoref{cha:typetheory} we saw that the coproduct $A+B$ could also have been defined as $\sm{x:\bool} \rec{\bool}(\UU,A,B,x)$.
The latter type satisfies the same induction principle as the former; hence they are canonically equivalent.

This is, of course, very similar to the familiar fact in category theory that if two objects have the same \emph{universal property}, then they are equivalent.
In \autoref{sec:initial-alg} we will see that inductive types actually do have a universal property, so that this is a manifestation of that general principle.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Coproducts and dependent sums}
\label{sec:coprod-depsum}

A \emph{coproduct} of two types $A$ and $B$ behaves much like a disjunction: we can choose to supply either an element of $A$ or an element of $B$.
Schematically, we declare this as follows:
\[ \disj{A}{B} : \type \defeq \myInd \left\{
\begin{array}{ll}
\inl : & A \to (\disj{A}{B}) \\
\inr : & B \to (\disj{A}{B})
\end{array}
\right. \]

In this definition, $A$ and $B$ act as \emph{parameters}, i.e., they denote arbitrary types. In other words, the type constructor $+$ is really a function taking the types $A$ and $B$ as an argument and returning a new inductive type $\disj{A}{B}$. Although omitted from the above schema for clarity, the constructors $\inl$ and $\inr$ likewise take the types $A$ and $B$ as the first 2 arguments, followed by an element of $A$ (for \inl) or of $B$ (for \inr).

Elimination for a coproduct amounts to case analysis:
\begin{itemize}
\item When proving a statement $E : (\disj{A}{B}) \to \type$ about \emph{all} terms of the coproduct $\disj{A}{B}$, it suffices to prove it for $\inl(a)$ and $\inr(b)$, i.e., give proofs $e_l : \prd{a:A} E(\inl(a))$ and $e_r : \prd{b:B} E(\inr(b))$.
\end{itemize}
The associated computation rules for the function $\rec{\disj{A}{B}}(E;e_l;e_r) : \prd{x:\disj{A}{B}} E(x)$ are as expected:
\begin{itemize}
\item For each $a : A$, we have $\rec{\disj{A}{B}}(E;e_l;e_r;\inl(a)) \equiv e_l(a)$.
\item For each $b : B$, we have $\rec{\disj{A}{B}}(E;e_l;e_r;\inr(b)) \equiv e_r(b)$.
\end{itemize}

A \emph{product} of two types $A$ and $B$ (denoted by $A \times B$) is the type of pairs $(a,b)$, where $a : A$ and $b : B$. A \emph{dependent sum} is a generalization of this concept, where we allow the type $B$ to depend on $A$. A simple example is the type of pairs $(n,(c_1,\ldots,c_n))$, where the first component is a natural number $n : \nat$ and the second component is a vector $(c_1,\ldots,c_n) : \mathbf{Vec}_C(n)$ of length $n$ over another type $C : \type$.
Given $A : \type$ and $B : A \to \type$, the dependent sum of $A$ and $B$ is given schematically as
\[ \sm{a:A} B(a) : \type \defeq \myInd \left\{
\begin{array}{ll}
\mathbf{pair} : & \prd{a:A}{b:B(a)} \\
& \sm{a:A} B(a)
\end{array}
\right. \]
Thus, the parameterized inductive type $\sm{a:A} B(a)$ has a single constructor $\mathbf{pair}$. Its first two arguments (not shown) are the parameters $A$ and $B$; the remaining two arguments are the respective components $a : A$ and $b : B(a)$. We will often denote $\mathbf{pair}(a,b)$ simply by $(a,b)$.

The elimination and computation rules are very simple:

\begin{itemize}
\item When proving a statement $E : \big(\sm{a:A} B(a)\big) \to \type$ about \emph{all} terms of the dependent sum $\sm{a:A} B(a)$, it suffices to prove it for a pair $(a,b)$, i.e., give a proof $e : \prd{a:A}{b:B(a)} E((a,b))$.
\end{itemize}

\begin{itemize}
\item For any $a : A$ and $b : B(a)$, we have $\rec{\sm{a:A} B(a)}(E;e;(a,b)) \equiv e(a,b)$.
\end{itemize}
Using the elimination operator, it is very easy to construct the well-known projection functions $\pi_1$ and $\pi_2$, extracting the first resp. the second component of a pair.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{W-types}
\label{sec:w-types}

Inductive types are very general, which is excellent for their usefulness and applicability, but makes them difficult to study as a whole.
Fortunately, they can all be formally reduced to a few special cases.
It is beyond the scope of this book to discuss this reduction --- which is anyway irrelevant to the mathematician using type theory in practice --- but we will take a little time to discuss the one of the basic special cases that we have not yet met.
These are Martin-L{\"o}f's \emph{W-types}, also known as the types of \emph{well-founded trees}.
W-types are a generalization of such types as natural numbers, lists, and binary trees, which are sufficiently general to encapsulate the ``recursion'' aspect of \emph{any} inductive type.

A particular W-type is specified by giving two parameters $A : \type$ and $B : A \to \type$, in which case the resulting $W$-type is written $\wtype{a:A} B(a)$.
The type $A$ represents the type of \emph{labels} for $\wtype{a :A} B(a)$, which function as constructors (however, we reserve that word for the actual functions which arise in inductive definitions). For instance, when defining natural numbers as a W-type, the type $A$ would be the type $\two$ inhabited by the two terms $\true$ and $\false$, since there are precisely two ways how to obtain a natural number---either it will be zero or a successor of another natural number. 

The type family $B : A \to \type$ is used to record the arity of labels: a label $a : A$ will take a family of inductive arguments, indexed over $B(a)$. We can therefore think of the ``$B(a)$-many'' arguments of $a$. These arguments are represented by a function $f : B(a) \to \wtype{a :A} B(a)$, with the understanding that for any $b : B(a)$, $f(b)$ is the ``$b$-th'' argument to the label $a$. The W-type $\wtype{a :A} B(a)$ can thus be thought of as the type of well-founded trees, where nodes are labeled by terms of $A$ and each node labeled by $a : A$ has $B(a)$-many branches.

In the case of natural numbers, the label $\true $ has arity 0, since it constructs the constant zero; the label $\false$ has arity 1, since it constructs the successor of its argument. We can capture this by using simple elimination on $\two$ to define a function $\rec\two(\bbU;\zero;\one)$ into a universe of types; this function returns the empty type $\zero$ for $\true$ and the unit type $\one$ for $\false$. We can thus define
\[ \natw \defeq \wtype{b:\two} \rec\two(\bbU;\zero;\one) \]
where the superscript \textbf{w} serves to distinguish this version of natural numbers from the previously used one.
Similarly, we can define the type of lists over $A$ as a W-type with $\disj{\one}{A}$ many labels: one nullary label for the empty list, plus one unary label for each $a : A$, corresponding to appending $a$ to the head of a list:
\[ \lst A \defeq \wtype{x:\disj{\one}{A}} \rec{\disj{\one}{A}}(\bbU; \; \zero; \; \lam{a:A} \one) \]

In general, the W-type $\wtype{x:A} B(x)$ specified by  $A : \type$ and $B : A \to \type$ is the inductive type generated by the following constructor:
\begin{itemize}
\item $\supp : \prd{a:A} \Big(B(a) \to \wtype{x:A} B(x)\Big) \to \wtype{x:A} B(x)$
\end{itemize}

The constructor $\supp$ (short for supremum) takes a label $a : A$ and a function $f : B(a) \to \wtype{a:A} B(a)$ representing the arguments to $a$, and constructs a new element of $\wtype{x:A} B(x)$. Using our previous encoding of natural numbers as W-types, we can for instance define
\begin{align*}
\zw \defeq \supp(\true, \; \lam{x:\zero} \rec\zero(\natw;x))
\end{align*}
Put differently, we use the label $\true$ to construct $\zw$. Then, $\rec\two(\bbU;\zero;\one; \true)$ evaluates to $\zero$, as it should since $\true$ is a nullary label. Thus, we need to construct a function $f : \zero \to \natw$, which represents the (zero) arguments supplied to $\true$. This is of course trivial, using simple elimination on $\zero$ as shown. Similarly, we can define
\begin{align*}
1^\mathbf{w} \defeq \supp(\false, \; \lam{x:\one} 0^\mathbf{w}) \\
2^\mathbf{w} \defeq \supp(\false, \; \lam{x:\one} 1^\mathbf{w})
\end{align*}
and so on.

We have the following elimination rule for W types:
\begin{itemize}
\item When proving a statement $E : \big(\wtype{x:A} B(x)\big) \to \type$ about \emph{all} terms of the $W$-type $\wtype{x:A} B(x)$, it suffices to prove it for $\supp(a,f)$, assuming it holds for all $f(b)$ with $b : B(a)$. 
In other words, it suffices to give a proof 
\begin{align*}
e : & \; \prd{a:A}{f : B(a) \to \wtype{x:A} B(x)}{g : \prd{b : B(a)} E(f(b))} E(\supp(a,f))
\end{align*}
\end{itemize}

The variable $g$ represents our inductive hypothesis, namely that all arguments of $a$ satisfy $E$. To state this, we quantify over all terms of type $B(a)$, since each $b : B(a)$ corresponds to one argument $f(b)$ of $a$.

How would we define the function $\dbl$ on natural numbers encoded as a W-type? We would like to use the (simple) elimination on $\natw$ into the type $\natw$ itself. We thus need to construct a suitable function
\[e : \prd{a : \two}{f : B(a) \to \natw}{g : B(a) \to \natw} \nat\]
which will represent the recurrence for the $\dbl$ function; for simplicity we denote the type $\rec\two(\bbU;\zero;\one)$ by $B$.

Clearly, $e$ will be a function taking $a : \two$ as its first argument. The next step is to perform case analysis on $a$ and proceed based on whether it is $\true$ or $\false$. This suggests the following form
\[ e \defeq \lam{a:\two} \rec\two(C,e_t,e_f,a) \]
where \[C \defeq \prd{f : B(a) \to \natw}{g : B(a) \to \natw} \natw\]
If $a$ is $\true$, the type $B(a)$ becomes $\zero$. Thus, given $f : \zero \to \natw$ and $g : \zero \to \natw$, we want to construct an element of $\natw$. Since the label $\true$ represents $\zero$, it needs zero inductive arguments and the variables $f$ and $g$ are irrelevant. We return $\zw$ as a result:
\[ e_t \defeq \lam{f:\zero \to \natw}{g:\zero \to \natw} \zw \]
Analogously, if $a$ is $\false$, the type $B(a)$ becomes $\one$.
Since the label $\false$ represents the successor operator, it needs one inductive argument---the predecessor---which is represented by the variable $f : \one \to \natw$.
The value of the recursive call on the predecessor is represented by the variable $g : \one \to \natw$.
Thus, taking this value (namely $g(\ttt)$) and applying the successor operator twice thus yields the desired result:
\begin{align*}
e_f \defeq & \; \lam{f:\one \to \natw}{g:\one \to \natw} \\
  & \; \supp(\false, (\lam{x:\one} \supp(\false, (\lam{y : \one} g(\ttt)))))
\end{align*}
Putting this together, we thus have
\[ \dbl \defeq \rec\natw(\natw; e) \]
with $e$ as defined above.

The associated computation rule for the function $\rec{\wtype{x:A} B(x)}(E;e) : \prd{w : \wtype{x:A} B(x)} E(w)$ is as follows.
\begin{itemize}
\item For any $a : A$ and $f : B(a) \to \wtype{x:A} B(x)$ we have 
\begin{align*}
& \rec{\wtype{x:A} B(x)}(E;e;\supp(a,f)) \equiv \\ & e(a,f,\big(\lam{b:B(a)} \rec{\wtype{x:A} B(x)}(E;f(b))\big))
\end{align*}
\end{itemize}
In other words, the function $\rec{\wtype{x:A} B(x)}(E;e)$ satisfies the recurrence $e$.

By the above computation rule, the function $\dbl$ behaves as expected:
\begin{align*}
\dbl(\zw) & \equiv \rec\natw(\natw; e; \supp(\true, \; \lam{x:\zero} \rec\zero(\natw;x))) \\
& \equiv e(\true, \big(\lam{x:\zero} \rec\zero(\natw;x)\big), \\
 & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \big(\lam{x:\zero} \dbl(\rec\zero(\natw;x))\big)) \\
 & \equiv e_t(\big(\lam{x:\zero} \rec\zero(\natw;x)\big), \big(\lam{x:\zero} \dbl(\rec\zero(\natw;x))\big)) \\
 & \equiv \zw \\
 \\
\dbl(1^\mathbf{w}) & \equiv \rec\natw(\natw; e; \supp(\false, \; \lam{x:\one} \zw)) \\
& \equiv e(\false, \big(\lam{x:\one} \zw\big), \big(\lam{x:\one} \dbl(\zw)\big)) \\
 & \equiv e_f(\big(\lam{x:\one} \zw\big), \big(\lam{x:\one} \dbl(\zw)\big)) \\
 & \equiv \supp(\false, (\lam{x:\one} \supp(\false,(\lam{y:\one} \dbl(\zw))))) \\
 & \equiv \supp(\false, (\lam{x:\one} \supp(\false,(\lam{y:\one} \zw)))) \\
 & \equiv 2^\mathbf{w}
\end{align*}
and so on.

Just like for natural numbers, we can prove a uniqueness theorem for 
W-types:
\begin{thm}\label{thm:w-uniq}
Let $g,h : \prd{w:\wtype{x:A}B(x)} E(w)$ be two functions which satisfy the recurrence $e : \prd{a,f} (\prd{b : B(a)} E(f(b))) \to E(\supp(a,f))$ up to propositional equality, i.e., such that
\begin{align*}
 \prd{a,f} \id{g(\supp(a,f))}{e(a,f,\lam{b:B(a)} g(f(b)))} \\
 \prd{a,f} \id{h(\supp(a,f))}{e(a,f,\lam{b:B(a)} h(f(b)))}
\end{align*}
Then $g$ and $h$ are equal. 
\end{thm}



\section{Inductive types are initial algebras}
\label{sec:initial-alg}

We mentioned previously that inductive types also have a category-theoretic universal property.
Specifically, they are \emph{homotopy-initial algebras}: initial objects (up to coherent homotopy) in a category of ``algebras'' determined by the specified constructors.

For example, consider the natural numbers.
The appropriate sort of ``algebra'' is a type equipped with the same structure that the constructors of $\nat$ give to it.

\begin{defn}
A \textbf{$\nat$-algebra} is a type $C$ with two terms $c_0 : C$, $c_s : C \to C$. Thus,
\begin{align*}
\nalg \defeq \sm {C : \type} C \times (C \to C)
\end{align*}
\end{defn}

\begin{defn}
Fix any $\nat$-algebras $(C,c_0,c_s)$ and $(D,d_0,d_s)$. A \textbf{$\nat$-homomorphism} between them is a function $h : C \to D$ such that $h(c_0) = d_0$ and $h(c_s(c)) = d_s(h(c))$ for any $c : C$. Thus,
\begin{align*}
\nhom((C,c_0,c_s),(D,d_0,d_s)) \defeq \sm {h : C \to D} (\id{h(c_0)}{d_0}) \times \prd{c:C} \id{h(c_s(c))}{d_s(h(c))}
\end{align*}
\end{defn}

We thus have a category of $\nat$-algebras and $\nat$-homomorphisms, and the claim is that $\nat$ is the initial object of this category.
A category theorist will immediately recognize this as the definition of a \emph{natural numbers object} in a category.

Of course, since our types behave like $\infty$-groupoids, we actually have an $(\infty,1)$-category of $\nat$-algebras, and we should ask $\nat$ to be initial in the appropriate $(\infty,1)$-categorical sense.
Fortunately, we can formulate this without needing to define $(\infty,1)$-categories.

\begin{defn}
A $\nat$-algebra $I$ is called \textbf{homotopy-initial}, or \textbf{h-initial} for short, if for any other $\nat$-algebra $C$, the type of $\nat$-homomorphisms from $I$ to $C$ is contractible. Thus,
\begin{align*}
\ishinitn (I) \defeq \prd{C : \nalg} \iscontr(\nhom(I,C)).
\end{align*}
\end{defn}

When they exist, h-initial algebras are unique --- not just up to isomorphism, as usual in category theory, but up to equality, by the univalence axiom.

\begin{thm}
  Any two h-initial $\nat$-algebras are equal.
  Thus, the type of h-initial $\nat$-algebras is a mere proposition.
\end{thm}
\begin{proof}
  Suppose $I$ and $J$ are h-initial $\nat$-algebras.
  Then $\nhom(I,J)$ is contractible, hence inhabited by some $\nat$-homomorphism $f:I\to J$, and likewise we have an $\nat$-homomorphism $g:J\to I$.
  Now the composite $g\circ f$ is a $\nat$-homomorphism from $I$ to $I$, as is $\idfunc[I]$; but $\nhom(I,I)$ is contractible, so $g\circ f = \idfunc[I]$.
  Similarly, $f\circ g = \idfunc[J]$.
  Hence $\eqv IJ$, and so $I=J$.
\end{proof}

We now have the following theorem.

\begin{thm}\label{thm:nat-hinitial}
The $\nat$-algebra $(\nat, \zero, \s)$ is homotopy initial.
\end{thm}
\begin{proof}[Sketch of proof]
  Fix an arbitrary $\nat$-algebra $(C,c_0,c_s)$.
  The recursion principle of $\nat$ yields a function $f:\nat\to C$ defined by
  \begin{align*}
    f(0) &\defeq c_0\\
    f(\suc(n)) &\defeq c_s(f(n)).
  \end{align*}
  These two equalities make $f$ a $\nat$-homomorphism, which we can take as the center of contraction for $\nhom(\nat,C)$.
  The uniqueness theorem (\autoref{thm:nat-uniq}) then implies that any other $\nat$-homomorphism is equal to $f$.
\end{proof}

Analogous theorems are true for other sorts of inductive types.
For instance, here are the notions of $\w$-algebras, $\w$-homomorphisms, and h-initiality for W-types.

\begin{defn}
A \emph{$\w$-algebra} for a type $A : \type$ and type family $B : A \to \type$ is a type $C$ with a function $c : \prd{a:A} (B(a) \to C) \to C$. Thus,
\begin{align*}
\walg(A,B) \defeq \sm {C : \type} \prd{a:A} (B(a) \to C) \to C
\end{align*}
\end{defn}

\begin{defn}
  Fix $A : \type, B : A \to \type$ and any $\w$-algebras $(C,c),(D,d) : \walg \; A \; B$. A \emph{$\w$-homomorphism} between them is a function $h : C \to D$ such that $h(c(a,f)) = d(a,\lam{b:B(a)} h(f(b)))$ for any $a : A$ and $f : B(a) \to C$. Thus,
\begin{align*}
  \whom_{A,B}((C, c),(D,d)) \defeq \sm {h : C \to D} \prd{a:A}{f:B(a)\to C} \id{h(c(a,f))}{\lam{b:B(a)} h(f(b))}
\end{align*}
\end{defn}

\begin{defn}
A $\w$-algebra $I$ is called \emph{h-initial} if for any other $\w$-algebra $C$, the type of $\w$-homomorphisms from $I$ to $C$ is contractible. Thus,
\begin{align*}
\ishinitw_{A, B}(I) \defeq \prd{C : \walg(A,B)} \iscontr(\whom_{A,B}(I,C))
\end{align*}
\end{defn}

\begin{thm}\label{thm:w-hinit}
For any type $A : \type$ and type family $B : A \to \type$, the $\w$-algebra $(\wtype{x:A}{B(x)}, \supp)$ is h-initial.
\end{thm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Homotopy-inductive types}
\label{sec:htpy-inductive}

In \autoref{sec:w-types} we showed how to encode natural numbers as W-types, with 
\begin{align*}
\natw & \defeq \wtype{b:\two} \rec\two(\bbU;\zero;\one) \\
\zw & \defeq \supp(\true, (\lam{x:\zero} \rec\zero(\natw;x))) \\
\sw & \defeq \lam{n:\natw} \supp(\false, (\lam{x:\one} n))
\end{align*}
We also showed how one can define a $\dbl$ function on $\natw$ using the recursion principle.
When it comes to the induction principle, however, this encoding is no longer satisfactory: given $E : \natw \to \type$ and recurrences $e_z : E(\zw)$ and $e_s : \prd{n : \natw}{y : E(n)} E(\sw(n))$, we can only construct a dependent function $r(E,e_z,e_s) : \prd{n : \natw} E(n)$ satisfying the given recurrences \emph{propositionally}, i.e.\ up to a path.
This means that the computation rules for natural numbers, which give judgmental equalities, cannot be derived from the rules for W-types in any obvious way.

This problem goes away if instead of the conventional inductive types we consider \emph{homotopy-inductive types}, where all computation rules are stated up to a path, i.e., the symbol $\jdeq$ is replaced by $=$. For instance, the computation rule for the homotopy version of W-types $\mathsf{W^h}$ becomes:
\begin{itemize}
\item For any $a : A$ and $f : B(a) \to \wtypeh{x:A} B(x)$ we have 
\begin{equation*}
  \rec{\wtypeh{x:A} B(x)}(E;\supp(a,f)) = e\Big(a,f,\big(\lam{b:B(a)} \rec{\wtypeh{x:A} B(x)}(E;f(b))\big)\Big)
\end{equation*}
\end{itemize}

Homotopy-inductive types have an obvious disadvantage when it comes to computational properties---the behavior of any function constructed using the elimination principle can now only be characterized propositionally. However, there are numerous advantages to this approach as well---for instance, the notion of a homotopy-inductive type is now internal to the type theory. This means we can, e.g., form a type of all natural numbers objects and make assertions about it. In the case of W-types, we can characterize $\wtype{x:A} B(x)$ as any type endowed with a supremum function and a dependent eliminator satisfying the appropriate computation rule:
\begin{align*}
\w_d \; A \; B \defeq & \; \sm{W : \type} \\
                      & \; \sm{\supp : \prd {a} (B(a) \to W) \to W} \\
                      & \; \prd{E : W \to \type} \\
                      & \; \prd{e : \prd{a,f} (\prd{b : B(a)} E(f(b))) \to E(\supp(a,f))} \\
                      & \; \sm{\rec : \prd{w : W} E(w)} \\
                      & \; \prd{a,f} \rec{}(\supp(a,f)) = e(a,\lam{b:B(a)} \rec{}(f(b)))
\end{align*}

\begin{thm}
For any $A : \type$ and $B : A \to \type$, the type $\w_d \; A \; B$ is a mere proposition.
\end{thm}

It turns out that there is an equivalent characterization of W-types using simple elimination, plus certain \emph{uniqueness} and \emph{coherence} laws. First we give the rule for simple elimination:

\begin{itemize}
\item When constructing a function from the the $W$-type $\wtypeh{x:A} B(x)$ into the type $C$, it suffices to give its value for $\supp(a,f)$, assuming we are given the values of all $f(b)$ with $b : B(a)$.
In other words, it suffices to construct a function
\begin{align*}
c : & \; \prd{a:A} (B(a) \to C) \to C
\end{align*}
\end{itemize}
The associated computation rule for $\rec{\wtypeh{x:A} B(x)}(C,c) : (\wtype{x:A} B(x)) \to C$ is as follows:
\begin{itemize}
\item For any $a : A$ and $f : B(a) \to \wtypeh{x:A} B(x)$ we have 
\begin{align*}
  & \rec{\wtypeh{x:A} B(x)}(C;c;\supp(a,f)) = \\ & c(a,\lam{b:B(a)} \rec{\wtypeh{x:A} B(x)}(C;c;f(b)))
\end{align*}
i.e., there is a witness $\beta(C;c;a;f)$ for the above equality.
\end{itemize}

Furthermore, we have the following uniqueness rule, saying that any two functions defined by the same recurrence are equal:
\begin{itemize}
\item Let $C : \type$ and $c : \prd{a:A} (B(a) \to C) \to C$ be given. Let $g,h : (\wtypeh{x:A} B(x)) \to C$ be two functions which satisfy the recurrence $c$ up to propositional equality, i.e., such that we have terms
\begin{align*}
  \beta_g : \prd{a,f} \id{g(\supp(a,f))}{c(a,\lam{b: B(a)} g(f(b)))} \\
  \beta_h : \prd{a,f} \id{h(\supp(a,f))}{c(a,\lam{b: B(a)} h(f(b)))}
\end{align*}
Then $g$ and $h$ are equal, i.e.\ there is an equality $\alpha(C;c;f;g,\beta_g;\beta_h) : g = h$.
\end{itemize}

We note that in the dependent case the uniqueness rule is derivable from dependent elimination (\autoref{thm:w-uniq}). When the eliminator is simple rather than dependent, this rule is no longer derivable---and in fact, the statement is not even true (exercise). We hence postulate it as an axiom, together with the following coherence law, which tells us how the proof of uniqueness behaves on canonical terms:
\begin{itemize}
\item
For any $a : A$ and $f : B(a) \to C$, the following diagram commutes propositionally:
\[\xymatrix{
  \;\;\;\;\; g(\supp(x,f)) \ar_{\alpha(\supp(x,f))}[d] \ar^{\beta_g \;\;\;\;\;\;\;\;\;\;}[r] & c(a,\lam{b:B(a)} g(f(b)))
  \ar^{c(a,-)(\funext \; \lam{b} \alpha(f(b)))}[d] \\
  \;\;\;\;\; h(\supp(x,f)) \ar_{\beta_h \;\;\;\;\;\;\;\;\;\;}[r] & c(a,\lam{b: B(a)} h(f(b))) \\
}\]
where $\alpha$ abbreviates the path $\alpha(C;c;f;g,\beta_g;\beta_h) : g = h$.
\end{itemize}

Putting all of this data together yields another characterization of $\wtype{x:A} B(x)$, as a type with a supremum function, satisfying simple elimination, computation, uniqueness, and coherence rules:
\begin{align*}
\w_s \; A \; B \defeq & \; \sm{W : \type} \\
                      & \; \sm{\supp : \prd {a} (B(a) \to W) \to W} \\
                      & \; \prd{C : \type} \\
                      & \; \prd{c : \prd{a} (B(a) \to C) \to C} \\
                      & \; \sm{\rec : W \to C} \\
                      & \; \sm{\beta : \prd{a,f} \rec{}(\supp(a,f)) = c(a,\lam{b: B(a)} \rec{}(f(b)))} \\
                      & \; \prd{g : W \to C} \\
                      & \; \prd{h : W \to C} \\
                      & \; \prd{\beta_g : \prd{a,f} g(\supp(a,f)) = c(a,\lam{b: B(a)} g(f(b)))} \\
                      & \; \prd{\beta_h : \prd{a,f} h(\supp(a,f)) = c(a,\lam{b: B(a)} h(f(b)))} \\
                      & \; \sm{\alpha : \prd {w : W} g(w) = h(w)} \\
                      & \; \alpha(\supp(x,f)) \ct \beta_h = \beta_g \ct c(a,-)(\funext \; \lam{b} \alpha(f(b)))
\end{align*}

\begin{thm}
For any $A : \type$ and $B : A \to \type$, the type $\w_s \; A \; B$ is a mere proposition.
\end{thm}

Finally, we have a third, very concise characterization of $\wtype{x:A} B(x)$ as an h-initial $\w$-algebra:
\begin{align*}
\w_h \; A \; B \defeq \sm{\chi : \walg \; A \; B} \ishinitw(\chi)
\end{align*}

\begin{thm}
For any $A : \type$ and $B : A \to \type$, the type $\w_h \; A \; B$ is a mere proposition.
\end{thm}

It turns out all three characterizations of W-types are in fact equivalent:
\begin{thm}
For any $A : \type$ and $B : A \to \type$, we have
\[ \w_d \; A \; B \simeq \w_s \; A \; B \simeq \w_h \; A \; B \]
\end{thm}

Thus, we have the following corrollary, which is an improvement over \autoref{thm:w-hinit}:

\begin{cor}
The types satisfying the formation, introduction, elimination, and propositional computation rules for W-types are precisely the homotopy-initial $\w$-algebras.
\end{cor}

Finally, as desired, it can be shown that homotopy-natural numbers can be encoded as homotopy-W-types:

\begin{thm}
The rules for natural numbers with propositional computation rules can be derived from the rules for W-types with propositional computation rules.
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The general syntax of inductive definitions}
\label{sec:strictly-positive}

So far, we have been discussing only particular inductive types: $\emptyt$, $\unit$, $\bool$, $\nat$, coproducts, products, $\Sigma$-types, $W$-types, etc.
However, an important aspect of type theory is the ability to define \emph{new} inductive types, rather than being restricted only to some particular fixed list of them.
In order to be able to do this, however, we need to know what sorts of ``inductive definitions'' are valid or reasonable.

To see that not everything which ``looks like an inductive definition'' makes sense, consider the following ``constructor'' of a type $C$:
\begin{itemize}
\item $g:(C\to \nat) \to C$.
\end{itemize}
The recursion principle for such a type $C$ ought to say that given a type $P$, in order to construct a function $f:C\to P$, it suffices to consider the case when the input $c:C$ is of the form $g(\alpha)$ for some $\alpha:C\to\nat$.
Moreover, we would expect to be able to use the ``recursive data'' of $f$ applied to $\alpha$ in some way.
However, it is not at all clear how to ``apply $f$ to $\alpha$'', since both are functions with domain $C$.

We could write down a ``recursion principle'' for $C$ by just supposing (unjustifiably) that there is some way to apply $f$ to $\alpha$ and obtain a function $P\to\nat$.
Then the input to the recursion rule would ask for a type $P$ together with a function
\begin{equation}
  h:(C\to\nat) \to (P\to\nat) \to P\label{eq:fake-recursor}
\end{equation}
where the two arguments of $h$ are $\alpha$ and ``the result of applying $f$ to $\alpha$''.
However, what would the computation rule for the resulting function $f:C\to P$ be?
Looking at other computation rules, we would expect something like ``$f(g(\alpha)) \jdeq h(\alpha,f(\alpha))$'' for $\alpha:C\to\nat$, but as we have seen, ``$f(\alpha)$'' does not make sense.
The induction principle of $C$ is even more problematic; it's not even clear how to write down the hypotheses.
(See also \autoref{ex:loop,ex:loop2}.)

This example suggests one restriction on inductive definitions: the domains of all the constructors must be \emph{covariant functors} of the type being defined, so that we can ``apply $f$ to them'' to get the result of the ``recursive call''.
In other words, if we replace all occurrences of the type being defined with a variable $X:\type$, then each domain of a constructor must be an expression that can be made into a covariant functor of $X$.
This is the case for all the examples we have considered so far.
For instance, with the constructor $\inl:A\to A+B$, the relevant functor is constant at $A$ (i.e.\ $X\mapsto A$), while for the constructor $\suc:\nat\to\nat$, the functor is the identity functor ($X\mapsto X$).

However, this necessary condition is also not sufficient.
Covariance prevents the inductive type from occurring on the left of a single function type, as in the argument $C\to\nat$ of the ``constructor'' $g$ considered above, since this yields a contravariant functor rather than a covariant one.
However, since the composite of two contravariant functors is covariant, \emph{double} function types such as $((X\to \nat)\to \nat)$ are once again covariant.
This enables us to reproduce Cantorian-style paradoxes.

For instance, consider an ``inductive type'' $D$ with the following constructor:
\begin{itemize}
\item $k:((D\to\prop)\to\prop)\to D$.
\end{itemize}
Assuming such a type exists, we define functions
\begin{align*}
  r&:D\to ((D\to\prop)\to\prop)\\
  r(k(\theta)) &\defeq \theta\\
  f&:(D\to\prop) \to D \\
  f(\delta) &\defeq k(\lam{x} (x=\delta))\\
  p&:(D\to \prop) \to ((D\to\prop)\to \prop)\\
  p(\delta) &\defeq \lam{x} \delta(f(x)).
\end{align*}
Here $r$ is defined by the recursion principle of $D$, while $f$ and $p$ are defined explicitly.
Then for any $\delta:D\to\prop$, we have $r(f(\delta)) = \lam{x}(x=\delta)$.

In particular, therefore, if $f(\delta)=f(\delta')$, then we have a path $s:(\lam{x}(x=\delta)) = (\lam{x}(x=\delta'))$.
Thus, $\happly(s,\delta) : (\delta=\delta) = (\delta=\delta')$, and so in particular $\delta=\delta'$ holds.
Hence, $f$ is ``injective'' (although \emph{a priori} $D$ may not be a set).
This already sounds suspicious --- we have an ``injection'' of the ``power set'' of $D$ into $D$ --- and with a little more work we can massage it into a contradiction.

Suppose given $\theta:(D\to\prop)\to\prop$, and define $\delta:D\to\prop$ by
\begin{equation}
  \delta(d) \defeq \exis{\gamma:D\to\prop} (f(\gamma) = d) \times \theta(\gamma).\label{eq:Pinj}
\end{equation}
We claim that $p(\delta)=\theta$.
By function extensionality, it suffices to show $p(\delta)(\gamma) =_\prop \theta(\gamma)$ for any $\gamma:D\to\prop$.
And by univalence, for this it suffices to show that each implies the other.
Now by definition of $p$, we have
\begin{align*}
  p(\delta)(\gamma) &\jdeq \delta(f(\gamma))\\
  &\jdeq \exis{\gamma':D\to\prop} (f(\gamma') = f(\gamma)) \times \theta(\gamma')
\end{align*}
Clearly this holds if $\theta(\gamma)$, since we may take $\gamma'\defeq \gamma$.
On the other hand, if we have $\gamma'$ with $f(\gamma') = f(\gamma)$ and $\theta(\gamma')$, then $\gamma'=\gamma$ since $f$ is injective, hence also $\theta(\gamma)$.

This completes the proof that $p(\delta)=\theta$.
Thus, every element $\theta:(D\to\prop)\to\prop$ is the image under $p$ of some element $\delta:D\to\prop$.
However, if we define $\theta$ by a classic diagonalization:
\[ \theta(\gamma) \defeq \neg p(\gamma)(\gamma) \quad\text{for all $\gamma:D\to\prop$} \]
then from $\theta = p(\delta)$ we deduce $p(\delta)(\delta) = \neg p(\delta)(\delta)$.
This is a contradiction: no proposition can be equivalent to its negation.
(Supposing $P\leftrightarrow \neg P$, if $P$, then $\neg P$, and so $\emptyt$; hence $\neg P$, but then $P$, and so $\emptyt$.)

\begin{rmk}
  There is a question of universe size to be addressed.
  In general, an inductive type must live in a universe that already contains all the types going into its definition.
  Thus if in the definition of $D$, the ambiguous notation \prop means $\prop_{\UU}$, then we do not have $D:\UU$ but only $D:\UU'$ for some larger universe $\UU'$ with $\UU:\UU'$.
  In a predicative theory, therefore, the right-hand side of~\eqref{eq:Pinj} lives in $\prop_{\UU'}$, not $\prop_\UU$.
  So this contradiction does require the resizing rule for mere propositions mentioned in \autoref{subsec:prop-subsets}.
\end{rmk}

This counterexample suggests that we should ban an inductive type from ever appearing on the left of an arrow in the domain of its constructors, even if that appearance is nested in other arrows so as to eventually become covariant.
(Similarly, we also forbid it from appearing in the domain of a dependent function type.)
This restriction is called \textbf{strict positivity} (ordinary ``positivity'' being essentially covariance), and it turns out to suffice.

In conclusion, therefore, a valid inductive definition of a type $W$ consists of a list of \emph{constructors}.
Each constructor is assigned a type that is a function type taking some number (possibly zero) of inputs (possibly dependent on one another) and returning an element of $W$.
Finally, we allow $W$ itself to occur in the input types of its constructors, but only strictly positively.
This essentially means that each argument of a constructor is itself some iterated function type with codomain $W$.
For instance, the following is a valid constructor type:
\begin{equation}
  c:(A\to W) \to (B\to C \to W) \to D \to W \to W.\label{eq:example-constructor}
\end{equation}
All of these function types can also be dependent functions ($\Pi$-types).

Now once we have such an inductive definition, what can we do with it?
Firstly, there is a \emph{recursion principle} stating that in order to define a function $f:W\to P$, it suffices to consider the case when the input $w:W$ arises from one of the constructors, allowing ourselves to recursively call $f$ on the inputs to that constructor.
For the example constructor~\eqref{eq:example-constructor}, we would require $P$ to be equipped with a function of type
\begin{equation}\label{eq:example-rechyp}
  d:(A\to W) \to (A\to P) \to (B\to C\to W) \to (B\to C \to P) \to D \to W \to P \to P.
\end{equation}
Under these hypotheses, the recursion principle yields $f:W\to P$, which moreover ``preserves the constructor data'' in the evident way --- this is the computation rule, where we use covariance of the inputs.
For instance, in the example~\eqref{eq:example-constructor}, the computation rule says that for any $\alpha:A\to W$, $\beta:B\to C\to W$, $\delta:d$, and $\omega:W$, we have
\begin{equation}
  f(c(\alpha,\beta,\delta,\omega)) \jdeq d(\alpha,f\circ \alpha,\beta, f\circ \beta, \delta, \omega,f(\omega)).\label{eq:example-comp}
\end{equation}
As we have before in particular cases, when defining a particular function $f$, we may write these rules with $\defeq$ as a way of specifying the data $d$, and say that $f$ is defined by them.

The \emph{induction principle} for a general inductive type $W$ is only a little more complicated.
Of course, we start with a type family $P:W\to\type$, which we require to be equipped with constructor data ``lying over'' the constructor data of $W$.
That means the ``recursive call'' arguments such as $A\to P$ above must be replaced by dependent functions with types such as $\prd{a:A} P(\alpha(a))$.
In the full example of~\eqref{eq:example-constructor}, the corresponding hypothesis for the induction principle would require
\begin{multline*}
d : \dprd{\alpha:A\to W}\left(\dprd{a:A} P(\alpha(a))\right) \to
\dprd{\beta:B\to C\to W} \left(\dprd{b:B}{c:C} P(\beta(b,c))\right) \to\\
\dprd{\delta:D}
\dprd{\omega:W} P(\omega) \to
P(c(\alpha,\beta,\delta,\omega)).
\end{multline*}
The corresponding computation rule looks identical to~\eqref{eq:example-comp}.

We will not attempt to give a formal presentation of the grammar of a valid inductive definition and its resulting induction and recursion principles.
This is possible to do (indeed, it is necessary to do if implementing a computer proof assistant), but provides no additional insight.
With practice, one learns to automatically deduce the induction and recursion principles for any inductive definition, and to use them without having to think twice.

%% TODO: Double induction/recursion


\section{Inductive families}
\label{sec:inductive-families}


\section{Identity systems}
\label{sec:identity-systems}


\section*{Exercises}

\begin{ex}
  Construct two functions on natural numbers which satisfy the same recurrence $(e_z, e_s)$ but are not definitionally equal.
\end{ex}

\begin{ex}\label{ex:bool}
Show that for any dependent type $E : \two \to \type$, the space of dependent functions $\prd{b : \two} E(b)$ is equivalent to the space of recurrences $ E(\true) \times E(\false)$. 
\end{ex}

\begin{ex}
  (Continuing \autoref{ex:bool}) Formulate a corresponding statement for natural numbers and show that it fails to hold. Why?
\end{ex}

\begin{ex}
  Show that if we assume simple instead of dependent elimination for W-types, the uniqueness property (analogue of Thm.~\ref{thm:w-uniq}) fails to hold.
\end{ex}

\begin{ex}\label{ex:loop}
  Suppose that in the ``inductive definition'' of the type $C$ at the beginning of \autoref{sec:strictly-positive}, we replace the type \nat by \emptyt.
  Using only a ``recursion principle'' for such a definition with hypotheses analogous to~\eqref{eq:fake-recursor}, construct an element of \emptyt.
\end{ex}

\begin{ex}\label{ex:loop2}
  Similarly to the previous exercise, derive a contradiction from an ``inductive type'' $D$ with one constructor $(D\to D) \to D$.
\end{ex}

% Local Variables:
% TeX-master: "main"
% End:

